## python
# python==3.10

## flash attention
# pip install flash-attn # --no-build-isolation

## llama
# pip install llama-cpp-python==0.2.55 --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu122
## or
# CUDACXX=/usr/local/cuda-12/bin/nvcc CMAKE_ARGS="-DLLAMA_CUBLAS=on -DCMAKE_CUDA_ARCHITECTURES=native" FORCE_CMAKE=1 pip install llama-cpp-python==0.2.55 --no-cache-dir --force-reinstall --upgrade

## unsloth
# conda install pytorch-cuda=<12.1/11.8> pytorch cudatoolkit xformers -c pytorch -c nvidia -c xformers
# pip install "unsloth[cu121-ampere-torch230] @ git+https://github.com/unslothai/unsloth.git"

## auto installation
accelerate
bitsandbytes
black[jupyter]
datasets
deepl
flask
flask_httpauth
googletrans==3.1.0a0
gradio-client
matplotlib
ninja
nltk
notebook
openai
peft
protobuf
python-dotenv
scikit-learn
sentencepiece
torch
tqdm
transformers
trl
wandb