services:

  rag-tge_llm:
    container_name: "rag-tge_llm"
    image: ghcr.io/ggerganov/llama.cpp:full-cuda
    ports:
      - "${LLAMA_PORT}:${LLAMA_PORT}"
    volumes:
      - .:/models/

    command: --server --host 0.0.0.0 --port ${LLAMA_PORT} -m /models/${MODEL_FILE} -c ${CTX_SIZE} -ngl 0 --api-key ${LLAMA_KEY} --embedding
