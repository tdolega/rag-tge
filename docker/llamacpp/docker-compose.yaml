services:
  llamacpp-server:
    container_name: "llamacpp-server"
    image: ghcr.io/ggerganov/llama.cpp:full-cuda
    ports:
      - "${PORT}:${PORT}"
    volumes:
      - /mnt/data/tdolega/models/:/models/

    command: --server --host 0.0.0.0 --port ${PORT} -m /models/${MODEL_FILE} -np ${NP} -c ${CTX_SIZE} --embedding


  # llamacpp-convert:
  #   container_name: "llamacpp-convert"
  #   image: ghcr.io/ggerganov/llama.cpp:full-cuda
  #   volumes:
  #     - /mnt/data/tdolega/models/:/models/
  #   command: yo