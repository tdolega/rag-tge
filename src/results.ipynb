{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tsd/.conda/envs/p311/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from common.consts import RESULTS_DIR, EVAL_SIZE\n",
    "from common.utils import filename_to_obj, remove_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 41/41 [01:21<00:00,  2.00s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question_idx</th>\n",
       "      <th>question_id</th>\n",
       "      <th>llm</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>temperature</th>\n",
       "      <th>nli</th>\n",
       "      <th>ellm</th>\n",
       "      <th>sim</th>\n",
       "      <th>citations/ais_recall</th>\n",
       "      <th>citations/ais_precision</th>\n",
       "      <th>...</th>\n",
       "      <th>citations/supported</th>\n",
       "      <th>citations/citations</th>\n",
       "      <th>citations/correct_citations</th>\n",
       "      <th>citations/out_of_range</th>\n",
       "      <th>correctness/answer_overlap</th>\n",
       "      <th>correctness/answer_entail</th>\n",
       "      <th>correctness/citations_recall</th>\n",
       "      <th>correctness/citations_precision</th>\n",
       "      <th>quality/answer_relevance</th>\n",
       "      <th>quality/new_question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>5abab42e55429955dce3eed2</td>\n",
       "      <td>Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>t5_xxl_true_nli_mixture</td>\n",
       "      <td>Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>0.200</td>\n",
       "      <td>0.750000</td>\n",
       "      <td>...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 1, 0, 0]</td>\n",
       "      <td>[[4], [], [], [3, 4], [], [], [], [4], [], []]</td>\n",
       "      <td>[[True], [], [], [True], [], [], [], [True], [...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.785292</td>\n",
       "      <td>Which professional hockey player, related to P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>5abab42e55429955dce3eed2</td>\n",
       "      <td>Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>t5_xxl_true_nli_mixture</td>\n",
       "      <td>Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>0.100</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[4], [3], [], [], [], [], [], [], [], []]</td>\n",
       "      <td>[[True], [True], [], [], [], [], [], [], [], []]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.719814</td>\n",
       "      <td>Which professional hockey player related to P....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>5abab42e55429955dce3eed2</td>\n",
       "      <td>Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>t5_xxl_true_nli_mixture</td>\n",
       "      <td>Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>[[4], [], [], [3, 4], [], [], [], []]</td>\n",
       "      <td>[[True], [], [], [True], [], [], [], []]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.738571</td>\n",
       "      <td>Which professional hockey player related to P....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>5a761900554299109176e648</td>\n",
       "      <td>Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>t5_xxl_true_nli_mixture</td>\n",
       "      <td>Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[[0]]</td>\n",
       "      <td>[[True]]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.471257</td>\n",
       "      <td>What is the background of Michael Flynn, the f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>5a761900554299109176e648</td>\n",
       "      <td>Meta-Llama-3-8B-Instruct</td>\n",
       "      <td>1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>t5_xxl_true_nli_mixture</td>\n",
       "      <td>Mistral-7B-Instruct-v0.2</td>\n",
       "      <td>all-MiniLM-L6-v2</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>[0]</td>\n",
       "      <td>[[0]]</td>\n",
       "      <td>[[True]]</td>\n",
       "      <td>[0]</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.496924</td>\n",
       "      <td>What is it that Michael Flynn, the former 25th...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   question_idx               question_id                       llm prompt_id  \\\n",
       "0             0  5abab42e55429955dce3eed2  Meta-Llama-3-8B-Instruct         1   \n",
       "1             0  5abab42e55429955dce3eed2  Meta-Llama-3-8B-Instruct         1   \n",
       "2             0  5abab42e55429955dce3eed2  Meta-Llama-3-8B-Instruct         1   \n",
       "3             1  5a761900554299109176e648  Meta-Llama-3-8B-Instruct         1   \n",
       "4             1  5a761900554299109176e648  Meta-Llama-3-8B-Instruct         1   \n",
       "\n",
       "  temperature                      nli                      ellm  \\\n",
       "0         0.1  t5_xxl_true_nli_mixture  Mistral-7B-Instruct-v0.2   \n",
       "1         0.1  t5_xxl_true_nli_mixture  Mistral-7B-Instruct-v0.2   \n",
       "2         0.1  t5_xxl_true_nli_mixture  Mistral-7B-Instruct-v0.2   \n",
       "3         0.1  t5_xxl_true_nli_mixture  Mistral-7B-Instruct-v0.2   \n",
       "4         0.1  t5_xxl_true_nli_mixture  Mistral-7B-Instruct-v0.2   \n",
       "\n",
       "                sim  citations/ais_recall  citations/ais_precision  ...  \\\n",
       "0  all-MiniLM-L6-v2                 0.200                 0.750000  ...   \n",
       "1  all-MiniLM-L6-v2                 0.100                 1.000000  ...   \n",
       "2  all-MiniLM-L6-v2                 0.125                 0.666667  ...   \n",
       "3  all-MiniLM-L6-v2                 0.000                 1.000000  ...   \n",
       "4  all-MiniLM-L6-v2                 0.000                 1.000000  ...   \n",
       "\n",
       "              citations/supported  \\\n",
       "0  [1, 0, 0, 0, 0, 0, 0, 1, 0, 0]   \n",
       "1  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]   \n",
       "2        [1, 0, 0, 0, 0, 0, 0, 0]   \n",
       "3                             [0]   \n",
       "4                             [0]   \n",
       "\n",
       "                              citations/citations  \\\n",
       "0  [[4], [], [], [3, 4], [], [], [], [4], [], []]   \n",
       "1      [[4], [3], [], [], [], [], [], [], [], []]   \n",
       "2           [[4], [], [], [3, 4], [], [], [], []]   \n",
       "3                                           [[0]]   \n",
       "4                                           [[0]]   \n",
       "\n",
       "                         citations/correct_citations  \\\n",
       "0  [[True], [], [], [True], [], [], [], [True], [...   \n",
       "1   [[True], [True], [], [], [], [], [], [], [], []]   \n",
       "2           [[True], [], [], [True], [], [], [], []]   \n",
       "3                                           [[True]]   \n",
       "4                                           [[True]]   \n",
       "\n",
       "           citations/out_of_range  correctness/answer_overlap  \\\n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]                         1.0   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]                         1.0   \n",
       "2        [0, 0, 0, 0, 0, 0, 0, 0]                         1.0   \n",
       "3                             [0]                         1.0   \n",
       "4                             [0]                         1.0   \n",
       "\n",
       "  correctness/answer_entail correctness/citations_recall  \\\n",
       "0                       1.0                          1.0   \n",
       "1                       1.0                          1.0   \n",
       "2                       1.0                          1.0   \n",
       "3                       1.0                          0.5   \n",
       "4                       1.0                          0.5   \n",
       "\n",
       "  correctness/citations_precision quality/answer_relevance  \\\n",
       "0                             1.0                 0.785292   \n",
       "1                             1.0                 0.719814   \n",
       "2                             1.0                 0.738571   \n",
       "3                             1.0                 0.471257   \n",
       "4                             1.0                 0.496924   \n",
       "\n",
       "                                quality/new_question  \n",
       "0  Which professional hockey player, related to P...  \n",
       "1  Which professional hockey player related to P....  \n",
       "2  Which professional hockey player related to P....  \n",
       "3  What is the background of Michael Flynn, the f...  \n",
       "4  What is it that Michael Flynn, the former 25th...  \n",
       "\n",
       "[5 rows x 26 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def results_as_pandas(filename):\n",
    "    path = os.path.join(RESULTS_DIR, filename)\n",
    "    data = pd.read_json(path, lines=True)\n",
    "\n",
    "    if len(data) == 0:\n",
    "        print(f\"empty file: {filename}\")\n",
    "        return data\n",
    "\n",
    "    params = filename_to_obj(filename)\n",
    "    for k, v in params.items():\n",
    "        data[k] = v\n",
    "\n",
    "    data = data.explode(\"evaluations\")\n",
    "    data = data.rename_axis(\"question_idx\").reset_index()\n",
    "\n",
    "    data = pd.concat([data, data[\"evaluations\"].apply(pd.Series)], axis=1)\n",
    "    evaluation_keys = data[\"evaluations\"].apply(pd.Series).columns\n",
    "    for col in evaluation_keys:\n",
    "        data = pd.concat([data, data[col].apply(pd.Series).add_prefix(f\"{col}/\")], axis=1)\n",
    "        data = data.drop(columns=col)\n",
    "    data = data.drop(columns=[\"evaluations\"])\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "files = os.listdir(RESULTS_DIR)\n",
    "params_names = list(filename_to_obj(files[0]).keys())\n",
    "all_results = pd.concat([results_as_pandas(f) for f in tqdm(files)])\n",
    "all_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropping columns: ['citations/correct_citations', 'citations/citations', 'citations/sentences', 'citations/out_of_range', 'citations/supported', 'quality/new_question']\n"
     ]
    }
   ],
   "source": [
    "all_obj_cols = all_results.select_dtypes(include=[\"object\"]).columns\n",
    "drop_obj_cols = list(set(all_obj_cols) - set(params_names))\n",
    "drop_obj_cols.remove(\"question_id\")\n",
    "print(f\"Dropping columns: {drop_obj_cols}\")\n",
    "all_num_results = all_results.drop(columns=drop_obj_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_split = all_num_results[all_num_results[\"question_idx\"] < EVAL_SIZE]\n",
    "train_split = all_num_results[all_num_results[\"question_idx\"] >= EVAL_SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLES_AGG_FUNC = \"mean\" # or \"max\"\n",
    "\n",
    "def merge_mean_std(df):\n",
    "    ndf = pd.DataFrame()\n",
    "    for (main_column, sub_column) in df.columns[::2]:\n",
    "        if sub_column == \"\":\n",
    "            ndf[main_column] = df[main_column]\n",
    "        else:\n",
    "            main = df[main_column][SAMPLES_AGG_FUNC]\n",
    "            std = df[main_column][\"std\"]\n",
    "            if main_column in [\"citations/ais_recall\", \"citations/ais_precision\", \"correctness/answer_overlap\", \"correctness/answer_entail\", \"correctness/citations_recall\", \"correctness/citations_precision\", \"quality/answer_relevance\"]:\n",
    "                main = (main * 100).round(1).astype(str) + \"%\"\n",
    "                std = (std * 100).round(1).astype(str) + \"%\"\n",
    "            else:\n",
    "                main = main.round(1).astype(str)\n",
    "                std = std.round(1).astype(str)\n",
    "            # std = std.str.rjust(5, \" \") # doesn't work for some reason\n",
    "            ndf[main_column] = main + \" ± \" + std\n",
    "    return ndf\n",
    "\n",
    "def aggregate(split):\n",
    "    split = split.drop(columns=[\"question_idx\"])\n",
    "    results_with_std_for_each_question = split.groupby([*params_names, \"question_id\"]).agg([SAMPLES_AGG_FUNC, \"std\"])\n",
    "    results_for_each_model = results_with_std_for_each_question.groupby(params_names)\n",
    "    results = results_for_each_model.mean()\n",
    "    results[\"n_questions\"] = results_for_each_model.size()\n",
    "    results = merge_mean_std(results)\n",
    "    return results\n",
    "\n",
    "\n",
    "eval_results = aggregate(eval_split)\n",
    "train_results = aggregate(train_split)\n",
    "\n",
    "if eval_results[\"n_questions\"].nunique() != 1:\n",
    "    print(\"Warning: not all rows in evaluation have the same number of examples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Prompts comparison"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citations/ais_recall</th>\n",
       "      <th>citations/ais_precision</th>\n",
       "      <th>correctness/answer_overlap</th>\n",
       "      <th>correctness/answer_entail</th>\n",
       "      <th>correctness/citations_recall</th>\n",
       "      <th>correctness/citations_precision</th>\n",
       "      <th>quality/answer_relevance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prompt_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>57.6% ± 3.7%</td>\n",
       "      <td>87.2% ± 3.8%</td>\n",
       "      <td>87.3% ± 1.5%</td>\n",
       "      <td>88.7% ± 1.7%</td>\n",
       "      <td>75.2% ± 2.6%</td>\n",
       "      <td>72.2% ± 5.0%</td>\n",
       "      <td>72.0% ± 2.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>50.2% ± 6.6%</td>\n",
       "      <td>86.2% ± 3.5%</td>\n",
       "      <td>85.5% ± 2.8%</td>\n",
       "      <td>86.0% ± 2.3%</td>\n",
       "      <td>77.0% ± 4.3%</td>\n",
       "      <td>67.1% ± 4.7%</td>\n",
       "      <td>70.5% ± 3.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>58.0% ± 4.5%</td>\n",
       "      <td>89.0% ± 2.5%</td>\n",
       "      <td>85.6% ± 1.7%</td>\n",
       "      <td>86.3% ± 1.2%</td>\n",
       "      <td>66.5% ± 3.5%</td>\n",
       "      <td>65.8% ± 4.2%</td>\n",
       "      <td>65.9% ± 2.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>50.7% ± 5.1%</td>\n",
       "      <td>82.8% ± 4.5%</td>\n",
       "      <td>84.3% ± 2.4%</td>\n",
       "      <td>83.7% ± 1.2%</td>\n",
       "      <td>60.2% ± 3.2%</td>\n",
       "      <td>69.0% ± 3.6%</td>\n",
       "      <td>67.7% ± 2.7%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          citations/ais_recall citations/ais_precision  \\\n",
       "prompt_id                                                \n",
       "1                 57.6% ± 3.7%            87.2% ± 3.8%   \n",
       "2                 50.2% ± 6.6%            86.2% ± 3.5%   \n",
       "3                 58.0% ± 4.5%            89.0% ± 2.5%   \n",
       "4                 50.7% ± 5.1%            82.8% ± 4.5%   \n",
       "\n",
       "          correctness/answer_overlap correctness/answer_entail  \\\n",
       "prompt_id                                                        \n",
       "1                       87.3% ± 1.5%              88.7% ± 1.7%   \n",
       "2                       85.5% ± 2.8%              86.0% ± 2.3%   \n",
       "3                       85.6% ± 1.7%              86.3% ± 1.2%   \n",
       "4                       84.3% ± 2.4%              83.7% ± 1.2%   \n",
       "\n",
       "          correctness/citations_recall correctness/citations_precision  \\\n",
       "prompt_id                                                                \n",
       "1                         75.2% ± 2.6%                    72.2% ± 5.0%   \n",
       "2                         77.0% ± 4.3%                    67.1% ± 4.7%   \n",
       "3                         66.5% ± 3.5%                    65.8% ± 4.2%   \n",
       "4                         60.2% ± 3.2%                    69.0% ± 3.6%   \n",
       "\n",
       "          quality/answer_relevance  \n",
       "prompt_id                           \n",
       "1                     72.0% ± 2.8%  \n",
       "2                     70.5% ± 3.0%  \n",
       "3                     65.9% ± 2.7%  \n",
       "4                     67.7% ± 2.7%  "
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(Markdown(\"### Prompts comparison\"))\n",
    "parameter_results = eval_results[eval_results.index.get_level_values(\"llm\") == \"Mistral-7B-Instruct-v0.2\"]\n",
    "\n",
    "\n",
    "def show_cleaned_results(short_eval_display, keep_index_name=None):\n",
    "    short_eval_display = short_eval_display.copy()\n",
    "    for index_name in [\"llm\", \"temperature\", \"nli\", \"ellm\", \"sim\", \"prompt_id\"]:\n",
    "        if index_name == keep_index_name or index_name not in short_eval_display.index.names:\n",
    "            continue\n",
    "        short_eval_display = remove_index(short_eval_display, index_name)\n",
    "    important_columns = [\"citations/ais_recall\", \"citations/ais_precision\", \"correctness/answer_overlap\", \"correctness/answer_entail\", \"correctness/citations_recall\", \"correctness/citations_precision\", \"quality/answer_relevance\"]\n",
    "    short_eval_display = short_eval_display[important_columns]\n",
    "    return short_eval_display\n",
    "\n",
    "\n",
    "prompts_comparison = show_cleaned_results(parameter_results[parameter_results.index.get_level_values(\"temperature\") == \"0.1\"], keep_index_name=\"prompt_id\")\n",
    "prompts_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Temperature comparison"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>citations/ais_recall</th>\n",
       "      <th>citations/ais_precision</th>\n",
       "      <th>correctness/answer_overlap</th>\n",
       "      <th>correctness/answer_entail</th>\n",
       "      <th>correctness/citations_recall</th>\n",
       "      <th>correctness/citations_precision</th>\n",
       "      <th>quality/answer_relevance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>temperature</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.01</th>\n",
       "      <td>57.5% ± 2.7%</td>\n",
       "      <td>88.9% ± 1.7%</td>\n",
       "      <td>85.9% ± 1.3%</td>\n",
       "      <td>88.3% ± 1.7%</td>\n",
       "      <td>75.8% ± 1.4%</td>\n",
       "      <td>74.4% ± 2.3%</td>\n",
       "      <td>73.5% ± 1.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.1</th>\n",
       "      <td>57.6% ± 3.7%</td>\n",
       "      <td>87.2% ± 3.8%</td>\n",
       "      <td>87.3% ± 1.5%</td>\n",
       "      <td>88.7% ± 1.7%</td>\n",
       "      <td>75.2% ± 2.6%</td>\n",
       "      <td>72.2% ± 5.0%</td>\n",
       "      <td>72.0% ± 2.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.2</th>\n",
       "      <td>54.4% ± 8.2%</td>\n",
       "      <td>87.7% ± 5.1%</td>\n",
       "      <td>86.5% ± 3.0%</td>\n",
       "      <td>88.7% ± 0.6%</td>\n",
       "      <td>77.0% ± 5.2%</td>\n",
       "      <td>74.2% ± 5.1%</td>\n",
       "      <td>72.5% ± 3.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.3</th>\n",
       "      <td>56.7% ± 8.5%</td>\n",
       "      <td>86.4% ± 6.6%</td>\n",
       "      <td>87.3% ± 4.6%</td>\n",
       "      <td>88.3% ± 2.3%</td>\n",
       "      <td>77.7% ± 6.0%</td>\n",
       "      <td>75.0% ± 6.7%</td>\n",
       "      <td>72.3% ± 4.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.4</th>\n",
       "      <td>58.1% ± 15.0%</td>\n",
       "      <td>87.8% ± 10.9%</td>\n",
       "      <td>87.2% ± 4.5%</td>\n",
       "      <td>88.7% ± 2.9%</td>\n",
       "      <td>77.5% ± 9.7%</td>\n",
       "      <td>73.8% ± 10.0%</td>\n",
       "      <td>72.1% ± 5.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.5</th>\n",
       "      <td>57.0% ± 14.5%</td>\n",
       "      <td>88.1% ± 8.5%</td>\n",
       "      <td>85.3% ± 7.4%</td>\n",
       "      <td>88.0% ± 3.5%</td>\n",
       "      <td>76.2% ± 8.2%</td>\n",
       "      <td>73.7% ± 11.4%</td>\n",
       "      <td>70.8% ± 6.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.6</th>\n",
       "      <td>51.9% ± 16.8%</td>\n",
       "      <td>85.7% ± 11.1%</td>\n",
       "      <td>84.5% ± 4.9%</td>\n",
       "      <td>87.0% ± 4.0%</td>\n",
       "      <td>75.8% ± 12.2%</td>\n",
       "      <td>73.9% ± 12.6%</td>\n",
       "      <td>71.5% ± 6.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.7</th>\n",
       "      <td>55.6% ± 18.3%</td>\n",
       "      <td>85.8% ± 11.5%</td>\n",
       "      <td>87.1% ± 4.7%</td>\n",
       "      <td>87.3% ± 2.9%</td>\n",
       "      <td>77.0% ± 11.9%</td>\n",
       "      <td>73.6% ± 13.2%</td>\n",
       "      <td>72.7% ± 5.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.8</th>\n",
       "      <td>50.6% ± 19.5%</td>\n",
       "      <td>84.2% ± 13.2%</td>\n",
       "      <td>86.7% ± 6.3%</td>\n",
       "      <td>86.0% ± 8.7%</td>\n",
       "      <td>75.2% ± 12.0%</td>\n",
       "      <td>72.0% ± 12.4%</td>\n",
       "      <td>72.5% ± 7.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0.9</th>\n",
       "      <td>57.3% ± 20.5%</td>\n",
       "      <td>87.7% ± 12.7%</td>\n",
       "      <td>82.9% ± 7.4%</td>\n",
       "      <td>85.7% ± 5.8%</td>\n",
       "      <td>75.2% ± 14.9%</td>\n",
       "      <td>73.2% ± 15.2%</td>\n",
       "      <td>69.6% ± 7.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>55.0% ± 20.3%</td>\n",
       "      <td>85.9% ± 13.9%</td>\n",
       "      <td>82.5% ± 6.7%</td>\n",
       "      <td>83.7% ± 6.4%</td>\n",
       "      <td>70.7% ± 14.4%</td>\n",
       "      <td>70.7% ± 16.4%</td>\n",
       "      <td>68.6% ± 8.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.5</th>\n",
       "      <td>52.5% ± 22.2%</td>\n",
       "      <td>85.7% ± 15.7%</td>\n",
       "      <td>83.8% ± 7.5%</td>\n",
       "      <td>85.3% ± 6.9%</td>\n",
       "      <td>75.3% ± 17.8%</td>\n",
       "      <td>71.1% ± 16.1%</td>\n",
       "      <td>69.7% ± 7.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.0</th>\n",
       "      <td>46.3% ± 25.9%</td>\n",
       "      <td>78.5% ± 22.1%</td>\n",
       "      <td>80.9% ± 9.0%</td>\n",
       "      <td>83.0% ± 7.5%</td>\n",
       "      <td>77.0% ± 22.3%</td>\n",
       "      <td>65.9% ± 22.6%</td>\n",
       "      <td>69.6% ± 8.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2.5</th>\n",
       "      <td>26.7% ± 26.3%</td>\n",
       "      <td>71.3% ± 24.1%</td>\n",
       "      <td>72.5% ± 18.6%</td>\n",
       "      <td>75.0% ± 16.2%</td>\n",
       "      <td>66.8% ± 28.4%</td>\n",
       "      <td>55.0% ± 25.9%</td>\n",
       "      <td>70.5% ± 9.7%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            citations/ais_recall citations/ais_precision  \\\n",
       "temperature                                                \n",
       "0.01                57.5% ± 2.7%            88.9% ± 1.7%   \n",
       "0.1                 57.6% ± 3.7%            87.2% ± 3.8%   \n",
       "0.2                 54.4% ± 8.2%            87.7% ± 5.1%   \n",
       "0.3                 56.7% ± 8.5%            86.4% ± 6.6%   \n",
       "0.4                58.1% ± 15.0%           87.8% ± 10.9%   \n",
       "0.5                57.0% ± 14.5%            88.1% ± 8.5%   \n",
       "0.6                51.9% ± 16.8%           85.7% ± 11.1%   \n",
       "0.7                55.6% ± 18.3%           85.8% ± 11.5%   \n",
       "0.8                50.6% ± 19.5%           84.2% ± 13.2%   \n",
       "0.9                57.3% ± 20.5%           87.7% ± 12.7%   \n",
       "1.0                55.0% ± 20.3%           85.9% ± 13.9%   \n",
       "1.5                52.5% ± 22.2%           85.7% ± 15.7%   \n",
       "2.0                46.3% ± 25.9%           78.5% ± 22.1%   \n",
       "2.5                26.7% ± 26.3%           71.3% ± 24.1%   \n",
       "\n",
       "            correctness/answer_overlap correctness/answer_entail  \\\n",
       "temperature                                                        \n",
       "0.01                      85.9% ± 1.3%              88.3% ± 1.7%   \n",
       "0.1                       87.3% ± 1.5%              88.7% ± 1.7%   \n",
       "0.2                       86.5% ± 3.0%              88.7% ± 0.6%   \n",
       "0.3                       87.3% ± 4.6%              88.3% ± 2.3%   \n",
       "0.4                       87.2% ± 4.5%              88.7% ± 2.9%   \n",
       "0.5                       85.3% ± 7.4%              88.0% ± 3.5%   \n",
       "0.6                       84.5% ± 4.9%              87.0% ± 4.0%   \n",
       "0.7                       87.1% ± 4.7%              87.3% ± 2.9%   \n",
       "0.8                       86.7% ± 6.3%              86.0% ± 8.7%   \n",
       "0.9                       82.9% ± 7.4%              85.7% ± 5.8%   \n",
       "1.0                       82.5% ± 6.7%              83.7% ± 6.4%   \n",
       "1.5                       83.8% ± 7.5%              85.3% ± 6.9%   \n",
       "2.0                       80.9% ± 9.0%              83.0% ± 7.5%   \n",
       "2.5                      72.5% ± 18.6%             75.0% ± 16.2%   \n",
       "\n",
       "            correctness/citations_recall correctness/citations_precision  \\\n",
       "temperature                                                                \n",
       "0.01                        75.8% ± 1.4%                    74.4% ± 2.3%   \n",
       "0.1                         75.2% ± 2.6%                    72.2% ± 5.0%   \n",
       "0.2                         77.0% ± 5.2%                    74.2% ± 5.1%   \n",
       "0.3                         77.7% ± 6.0%                    75.0% ± 6.7%   \n",
       "0.4                         77.5% ± 9.7%                   73.8% ± 10.0%   \n",
       "0.5                         76.2% ± 8.2%                   73.7% ± 11.4%   \n",
       "0.6                        75.8% ± 12.2%                   73.9% ± 12.6%   \n",
       "0.7                        77.0% ± 11.9%                   73.6% ± 13.2%   \n",
       "0.8                        75.2% ± 12.0%                   72.0% ± 12.4%   \n",
       "0.9                        75.2% ± 14.9%                   73.2% ± 15.2%   \n",
       "1.0                        70.7% ± 14.4%                   70.7% ± 16.4%   \n",
       "1.5                        75.3% ± 17.8%                   71.1% ± 16.1%   \n",
       "2.0                        77.0% ± 22.3%                   65.9% ± 22.6%   \n",
       "2.5                        66.8% ± 28.4%                   55.0% ± 25.9%   \n",
       "\n",
       "            quality/answer_relevance  \n",
       "temperature                           \n",
       "0.01                    73.5% ± 1.6%  \n",
       "0.1                     72.0% ± 2.8%  \n",
       "0.2                     72.5% ± 3.3%  \n",
       "0.3                     72.3% ± 4.8%  \n",
       "0.4                     72.1% ± 5.0%  \n",
       "0.5                     70.8% ± 6.0%  \n",
       "0.6                     71.5% ± 6.5%  \n",
       "0.7                     72.7% ± 5.9%  \n",
       "0.8                     72.5% ± 7.0%  \n",
       "0.9                     69.6% ± 7.4%  \n",
       "1.0                     68.6% ± 8.1%  \n",
       "1.5                     69.7% ± 7.8%  \n",
       "2.0                     69.6% ± 8.9%  \n",
       "2.5                     70.5% ± 9.7%  "
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(Markdown(\"### Temperature comparison\"))\n",
    "temperature_comparison = show_cleaned_results(parameter_results[parameter_results.index.get_level_values(\"prompt_id\") == \"1\"], keep_index_name=\"temperature\")\n",
    "temperature_comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def separate_avg_std(x, as_num=False):\n",
    "    parts = x.split(\" ± \")\n",
    "    if as_num:\n",
    "        return float(parts[0][:-1]), float(parts[1][:-1])\n",
    "    else:\n",
    "        return parts\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(14, 6))\n",
    "for i, (column, column_nice) in enumerate([\n",
    "    (\"citations/ais_recall\", \"Średnia czułość\"),\n",
    "    (\"citations/ais_precision\", \"Średnia precyzja\")\n",
    "]):\n",
    "    val = temperature_comparison[column]\n",
    "    avg, std = zip(*[separate_avg_std(x, as_num=True) for x in val])\n",
    "    axs[i].bar(temperature_comparison.index.get_level_values(\"temperature\"), avg, yerr=std, capsize=5)\n",
    "    axs[i].set_ylim(0, 100)\n",
    "    axs[i].yaxis.set_major_formatter(plt.FuncFormatter(lambda x, _: f\"{x:.0f}%\"))\n",
    "    axs[i].set_ylabel(column_nice)\n",
    "    axs[i].set_xlabel(\"Temperatura\")\n",
    "    axs[i].set_title(column)\n",
    "axs[0].set_title(\"Czułość wstawionych cytowań\")\n",
    "axs[1].set_title(\"Precyzja wstawionych cytowań\")\n",
    "fig.suptitle(\"Porównanie wyników ewaluacji dla różnych temperatur generacji\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Evaluation results"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>citations/ais_recall</th>\n",
       "      <th>citations/ais_precision</th>\n",
       "      <th>citations/n_sentences</th>\n",
       "      <th>citations/n_total_citations</th>\n",
       "      <th>citations/n_correct_citations</th>\n",
       "      <th>citations/n_correctly_multicited_sentences</th>\n",
       "      <th>citations/n_overcitations</th>\n",
       "      <th>correctness/answer_overlap</th>\n",
       "      <th>correctness/answer_entail</th>\n",
       "      <th>correctness/citations_recall</th>\n",
       "      <th>correctness/citations_precision</th>\n",
       "      <th>quality/answer_relevance</th>\n",
       "      <th>n_questions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llm</th>\n",
       "      <th>temperature</th>\n",
       "      <th>nli</th>\n",
       "      <th>ellm</th>\n",
       "      <th>sim</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rag-tge_Llama-3-8B_v1</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>64.0% ± 10.0%</td>\n",
       "      <td>95.4% ± 3.0%</td>\n",
       "      <td>8.2 ± 1.2</td>\n",
       "      <td>8.5 ± 1.3</td>\n",
       "      <td>8.0 ± 1.2</td>\n",
       "      <td>1.5 ± 0.5</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>94.4% ± 1.4%</td>\n",
       "      <td>92.3% ± 2.3%</td>\n",
       "      <td>98.8% ± 0.9%</td>\n",
       "      <td>77.6% ± 9.9%</td>\n",
       "      <td>73.5% ± 7.1%</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rag-tge_Llama-3-8B_v2</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>0</td>\n",
       "      <td>65.4% ± 12.0%</td>\n",
       "      <td>94.4% ± 5.0%</td>\n",
       "      <td>7.5 ± 1.0</td>\n",
       "      <td>8.4 ± 1.3</td>\n",
       "      <td>7.8 ± 1.1</td>\n",
       "      <td>1.7 ± 0.6</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>93.4% ± 1.5%</td>\n",
       "      <td>93.3% ± 4.0%</td>\n",
       "      <td>98.2% ± 0.6%</td>\n",
       "      <td>76.7% ± 13.6%</td>\n",
       "      <td>74.6% ± 6.6%</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rag-tge_Mistral_v2-4480</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>73.3% ± 13.5%</td>\n",
       "      <td>95.4% ± 3.1%</td>\n",
       "      <td>4.7 ± 1.2</td>\n",
       "      <td>5.5 ± 1.1</td>\n",
       "      <td>5.2 ± 1.1</td>\n",
       "      <td>1.1 ± 0.3</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>91.5% ± 1.8%</td>\n",
       "      <td>89.7% ± 2.3%</td>\n",
       "      <td>97.2% ± 2.6%</td>\n",
       "      <td>87.7% ± 7.0%</td>\n",
       "      <td>72.9% ± 5.5%</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rag-tge_Mistral_v2-3360</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>68.3% ± 14.6%</td>\n",
       "      <td>95.8% ± 3.3%</td>\n",
       "      <td>5.6 ± 1.4</td>\n",
       "      <td>6.2 ± 1.4</td>\n",
       "      <td>6.0 ± 1.3</td>\n",
       "      <td>1.1 ± 0.4</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>93.6% ± 3.1%</td>\n",
       "      <td>91.0% ± 1.2%</td>\n",
       "      <td>96.8% ± 2.6%</td>\n",
       "      <td>84.2% ± 8.4%</td>\n",
       "      <td>72.6% ± 6.3%</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rag-tge_Mistral_v6</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>4</td>\n",
       "      <td>78.3% ± 8.2%</td>\n",
       "      <td>94.7% ± 2.4%</td>\n",
       "      <td>1.6 ± 0.1</td>\n",
       "      <td>2.2 ± 0.1</td>\n",
       "      <td>2.1 ± 0.1</td>\n",
       "      <td>0.7 ± 0.1</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>90.6% ± 3.2%</td>\n",
       "      <td>93.0% ± 2.9%</td>\n",
       "      <td>93.7% ± 2.3%</td>\n",
       "      <td>96.4% ± 1.8%</td>\n",
       "      <td>76.1% ± 4.0%</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rag-tge_Llama-3-8B_v3</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>4</td>\n",
       "      <td>81.3% ± 7.5%</td>\n",
       "      <td>95.6% ± 2.9%</td>\n",
       "      <td>1.5 ± 0.2</td>\n",
       "      <td>2.4 ± 0.2</td>\n",
       "      <td>2.3 ± 0.2</td>\n",
       "      <td>0.9 ± 0.1</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>85.1% ± 4.5%</td>\n",
       "      <td>90.0% ± 2.3%</td>\n",
       "      <td>93.3% ± 2.5%</td>\n",
       "      <td>95.0% ± 0.9%</td>\n",
       "      <td>76.1% ± 3.3%</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rag-tge_Mistral.Q8</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>81.2% ± 6.1%</td>\n",
       "      <td>95.8% ± 2.1%</td>\n",
       "      <td>2.2 ± 0.3</td>\n",
       "      <td>3.0 ± 0.3</td>\n",
       "      <td>2.8 ± 0.3</td>\n",
       "      <td>0.8 ± 0.1</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>86.1% ± 1.3%</td>\n",
       "      <td>86.7% ± 0.6%</td>\n",
       "      <td>91.5% ± 2.3%</td>\n",
       "      <td>90.7% ± 4.1%</td>\n",
       "      <td>74.2% ± 4.0%</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo-0125</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>73.8% ± 7.3%</td>\n",
       "      <td>99.6% ± 0.3%</td>\n",
       "      <td>2.0 ± 0.2</td>\n",
       "      <td>2.1 ± 0.2</td>\n",
       "      <td>2.1 ± 0.2</td>\n",
       "      <td>0.5 ± 0.1</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>88.4% ± 1.3%</td>\n",
       "      <td>91.0% ± 1.2%</td>\n",
       "      <td>85.3% ± 2.3%</td>\n",
       "      <td>94.2% ± 2.4%</td>\n",
       "      <td>75.2% ± 4.1%</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mixtral-8x7B-Instruct-v0.1</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>65.4% ± 6.1%</td>\n",
       "      <td>92.9% ± 3.0%</td>\n",
       "      <td>2.1 ± 0.1</td>\n",
       "      <td>2.3 ± 0.2</td>\n",
       "      <td>2.1 ± 0.2</td>\n",
       "      <td>0.5 ± 0.1</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>91.0% ± 0.1%</td>\n",
       "      <td>89.7% ± 1.2%</td>\n",
       "      <td>82.5% ± 3.5%</td>\n",
       "      <td>87.6% ± 3.8%</td>\n",
       "      <td>72.6% ± 3.1%</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-turbo</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>73.3% ± 4.5%</td>\n",
       "      <td>98.5% ± 0.6%</td>\n",
       "      <td>1.6 ± 0.2</td>\n",
       "      <td>1.8 ± 0.1</td>\n",
       "      <td>1.8 ± 0.1</td>\n",
       "      <td>0.6 ± 0.1</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>87.7% ± 0.8%</td>\n",
       "      <td>90.0% ± 1.2%</td>\n",
       "      <td>79.5% ± 2.9%</td>\n",
       "      <td>93.9% ± 1.1%</td>\n",
       "      <td>74.7% ± 3.4%</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Meta-Llama-3-70B-Instruct</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>69.2% ± 5.0%</td>\n",
       "      <td>95.6% ± 1.8%</td>\n",
       "      <td>1.5 ± 0.1</td>\n",
       "      <td>1.9 ± 0.1</td>\n",
       "      <td>1.8 ± 0.1</td>\n",
       "      <td>0.6 ± 0.0</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>86.6% ± 2.1%</td>\n",
       "      <td>87.3% ± 1.7%</td>\n",
       "      <td>78.5% ± 1.7%</td>\n",
       "      <td>90.8% ± 1.0%</td>\n",
       "      <td>75.3% ± 2.8%</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zephyr-orpo-141b-A35b-v0.1</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>41.8% ± 13.4%</td>\n",
       "      <td>79.6% ± 10.1%</td>\n",
       "      <td>2.0 ± 0.4</td>\n",
       "      <td>4.9 ± 1.3</td>\n",
       "      <td>3.6 ± 1.1</td>\n",
       "      <td>0.6 ± 0.2</td>\n",
       "      <td>0.1 ± 0.0</td>\n",
       "      <td>80.7% ± 6.0%</td>\n",
       "      <td>86.0% ± 4.0%</td>\n",
       "      <td>77.5% ± 9.2%</td>\n",
       "      <td>54.6% ± 10.8%</td>\n",
       "      <td>69.6% ± 5.9%</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Meta-Llama-3-8B-Instruct</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>49.3% ± 6.1%</td>\n",
       "      <td>95.1% ± 2.2%</td>\n",
       "      <td>2.2 ± 0.2</td>\n",
       "      <td>2.0 ± 0.2</td>\n",
       "      <td>1.9 ± 0.2</td>\n",
       "      <td>0.4 ± 0.1</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>91.8% ± 0.0%</td>\n",
       "      <td>90.3% ± 0.6%</td>\n",
       "      <td>75.3% ± 2.3%</td>\n",
       "      <td>90.8% ± 1.8%</td>\n",
       "      <td>72.0% ± 3.2%</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>57.6% ± 3.7%</td>\n",
       "      <td>87.2% ± 3.8%</td>\n",
       "      <td>2.6 ± 0.2</td>\n",
       "      <td>3.0 ± 0.4</td>\n",
       "      <td>2.7 ± 0.3</td>\n",
       "      <td>0.9 ± 0.1</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>87.3% ± 1.5%</td>\n",
       "      <td>88.7% ± 1.7%</td>\n",
       "      <td>75.2% ± 2.6%</td>\n",
       "      <td>72.2% ± 5.0%</td>\n",
       "      <td>72.0% ± 2.8%</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen1_5-110b-chat</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>52.8% ± 9.5%</td>\n",
       "      <td>97.6% ± 2.7%</td>\n",
       "      <td>2.0 ± 0.2</td>\n",
       "      <td>1.7 ± 0.1</td>\n",
       "      <td>1.6 ± 0.1</td>\n",
       "      <td>0.5 ± 0.1</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>86.7% ± 2.0%</td>\n",
       "      <td>89.3% ± 1.7%</td>\n",
       "      <td>73.5% ± 5.2%</td>\n",
       "      <td>93.0% ± 2.3%</td>\n",
       "      <td>73.7% ± 5.5%</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen1_5-14b-chat-q8_0</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>63.3% ± 4.2%</td>\n",
       "      <td>97.5% ± 0.9%</td>\n",
       "      <td>1.4 ± 0.1</td>\n",
       "      <td>1.6 ± 0.1</td>\n",
       "      <td>1.5 ± 0.1</td>\n",
       "      <td>0.3 ± 0.0</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>87.6% ± 0.7%</td>\n",
       "      <td>86.3% ± 0.6%</td>\n",
       "      <td>66.0% ± 3.1%</td>\n",
       "      <td>92.0% ± 1.6%</td>\n",
       "      <td>71.9% ± 3.9%</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen1_5-32b-chat-q8_0</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>51.2% ± 3.2%</td>\n",
       "      <td>99.3% ± 0.3%</td>\n",
       "      <td>1.6 ± 0.1</td>\n",
       "      <td>1.4 ± 0.0</td>\n",
       "      <td>1.4 ± 0.0</td>\n",
       "      <td>0.3 ± 0.0</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>87.4% ± 0.6%</td>\n",
       "      <td>89.7% ± 0.6%</td>\n",
       "      <td>65.5% ± 0.9%</td>\n",
       "      <td>96.2% ± 0.0%</td>\n",
       "      <td>71.0% ± 2.7%</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zephyr-7b-beta</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>40.7% ± 9.1%</td>\n",
       "      <td>91.1% ± 6.9%</td>\n",
       "      <td>3.3 ± 0.4</td>\n",
       "      <td>2.6 ± 0.7</td>\n",
       "      <td>2.3 ± 0.6</td>\n",
       "      <td>0.5 ± 0.2</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>85.4% ± 2.9%</td>\n",
       "      <td>86.7% ± 4.6%</td>\n",
       "      <td>62.7% ± 8.0%</td>\n",
       "      <td>72.8% ± 8.8%</td>\n",
       "      <td>71.5% ± 4.2%</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>c4ai-command-r-plus</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>40.9% ± 6.0%</td>\n",
       "      <td>71.5% ± 6.1%</td>\n",
       "      <td>2.1 ± 0.1</td>\n",
       "      <td>1.8 ± 0.3</td>\n",
       "      <td>1.6 ± 0.3</td>\n",
       "      <td>0.4 ± 0.1</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>73.6% ± 2.3%</td>\n",
       "      <td>88.0% ± 0.0%</td>\n",
       "      <td>57.8% ± 4.4%</td>\n",
       "      <td>64.4% ± 5.5%</td>\n",
       "      <td>63.5% ± 4.7%</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen1_5-7b-chat-q8_0</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>23.3% ± 6.4%</td>\n",
       "      <td>86.2% ± 5.7%</td>\n",
       "      <td>2.4 ± 0.2</td>\n",
       "      <td>1.3 ± 0.2</td>\n",
       "      <td>1.3 ± 0.2</td>\n",
       "      <td>0.1 ± 0.1</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>86.5% ± 2.4%</td>\n",
       "      <td>84.0% ± 2.3%</td>\n",
       "      <td>54.8% ± 7.4%</td>\n",
       "      <td>80.0% ± 7.8%</td>\n",
       "      <td>76.8% ± 3.4%</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-1.1-7b-it</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>36.8% ± 12.3%</td>\n",
       "      <td>76.7% ± 8.3%</td>\n",
       "      <td>1.3 ± 0.2</td>\n",
       "      <td>1.4 ± 0.2</td>\n",
       "      <td>1.2 ± 0.2</td>\n",
       "      <td>0.4 ± 0.1</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>72.3% ± 7.6%</td>\n",
       "      <td>72.7% ± 7.5%</td>\n",
       "      <td>47.2% ± 8.1%</td>\n",
       "      <td>66.0% ± 10.1%</td>\n",
       "      <td>73.9% ± 4.3%</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Phi-3-mini-4k-instruct</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>25.3% ± 3.1%</td>\n",
       "      <td>57.8% ± 7.0%</td>\n",
       "      <td>8.4 ± 2.7</td>\n",
       "      <td>1.6 ± 0.3</td>\n",
       "      <td>1.5 ± 0.3</td>\n",
       "      <td>0.5 ± 0.1</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>71.1% ± 1.2%</td>\n",
       "      <td>78.0% ± 2.3%</td>\n",
       "      <td>45.2% ± 5.1%</td>\n",
       "      <td>45.8% ± 6.4%</td>\n",
       "      <td>66.4% ± 3.7%</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemma-1.1-2b-it</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.7% ± 0.6%</td>\n",
       "      <td>4.6% ± 1.0%</td>\n",
       "      <td>1.4 ± 0.0</td>\n",
       "      <td>0.1 ± 0.0</td>\n",
       "      <td>0.1 ± 0.0</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>63.1% ± 3.8%</td>\n",
       "      <td>67.0% ± 4.0%</td>\n",
       "      <td>3.3% ± 0.6%</td>\n",
       "      <td>4.3% ± 0.7%</td>\n",
       "      <td>70.9% ± 2.0%</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mistral-7B-Instruct-v0.1</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>0.5% ± 0.0%</td>\n",
       "      <td>1.0% ± 0.0%</td>\n",
       "      <td>1.4 ± 0.1</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>79.0% ± 1.4%</td>\n",
       "      <td>82.7% ± 1.7%</td>\n",
       "      <td>1.0% ± 0.0%</td>\n",
       "      <td>0.7% ± 0.0%</td>\n",
       "      <td>73.8% ± 2.7%</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                         prompt_id  \\\n",
       "llm                        temperature nli                     ellm                     sim                          \n",
       "rag-tge_Llama-3-8B_v1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "rag-tge_Llama-3-8B_v2      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         0   \n",
       "rag-tge_Mistral_v2-4480    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "rag-tge_Mistral_v2-3360    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "rag-tge_Mistral_v6         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         4   \n",
       "rag-tge_Llama-3-8B_v3      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         4   \n",
       "rag-tge_Mistral.Q8         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "gpt-3.5-turbo-0125         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "Mixtral-8x7B-Instruct-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "gpt-4-turbo                0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "Meta-Llama-3-70B-Instruct  0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "zephyr-orpo-141b-A35b-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "Meta-Llama-3-8B-Instruct   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "Mistral-7B-Instruct-v0.2   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "qwen1_5-110b-chat          0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "qwen1_5-14b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "qwen1_5-32b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "zephyr-7b-beta             0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "c4ai-command-r-plus        0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "qwen1_5-7b-chat-q8_0       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "gemma-1.1-7b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "Phi-3-mini-4k-instruct     0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "gemma-1.1-2b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "Mistral-7B-Instruct-v0.1   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "\n",
       "                                                                                                         citations/ais_recall  \\\n",
       "llm                        temperature nli                     ellm                     sim                                     \n",
       "rag-tge_Llama-3-8B_v1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2        64.0% ± 10.0%   \n",
       "rag-tge_Llama-3-8B_v2      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2        65.4% ± 12.0%   \n",
       "rag-tge_Mistral_v2-4480    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2        73.3% ± 13.5%   \n",
       "rag-tge_Mistral_v2-3360    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2        68.3% ± 14.6%   \n",
       "rag-tge_Mistral_v6         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         78.3% ± 8.2%   \n",
       "rag-tge_Llama-3-8B_v3      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         81.3% ± 7.5%   \n",
       "rag-tge_Mistral.Q8         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         81.2% ± 6.1%   \n",
       "gpt-3.5-turbo-0125         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         73.8% ± 7.3%   \n",
       "Mixtral-8x7B-Instruct-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         65.4% ± 6.1%   \n",
       "gpt-4-turbo                0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         73.3% ± 4.5%   \n",
       "Meta-Llama-3-70B-Instruct  0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         69.2% ± 5.0%   \n",
       "zephyr-orpo-141b-A35b-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2        41.8% ± 13.4%   \n",
       "Meta-Llama-3-8B-Instruct   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         49.3% ± 6.1%   \n",
       "Mistral-7B-Instruct-v0.2   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         57.6% ± 3.7%   \n",
       "qwen1_5-110b-chat          0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         52.8% ± 9.5%   \n",
       "qwen1_5-14b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         63.3% ± 4.2%   \n",
       "qwen1_5-32b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         51.2% ± 3.2%   \n",
       "zephyr-7b-beta             0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         40.7% ± 9.1%   \n",
       "c4ai-command-r-plus        0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         40.9% ± 6.0%   \n",
       "qwen1_5-7b-chat-q8_0       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         23.3% ± 6.4%   \n",
       "gemma-1.1-7b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2        36.8% ± 12.3%   \n",
       "Phi-3-mini-4k-instruct     0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         25.3% ± 3.1%   \n",
       "gemma-1.1-2b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          0.7% ± 0.6%   \n",
       "Mistral-7B-Instruct-v0.1   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          0.5% ± 0.0%   \n",
       "\n",
       "                                                                                                         citations/ais_precision  \\\n",
       "llm                        temperature nli                     ellm                     sim                                        \n",
       "rag-tge_Llama-3-8B_v1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2            95.4% ± 3.0%   \n",
       "rag-tge_Llama-3-8B_v2      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2            94.4% ± 5.0%   \n",
       "rag-tge_Mistral_v2-4480    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2            95.4% ± 3.1%   \n",
       "rag-tge_Mistral_v2-3360    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2            95.8% ± 3.3%   \n",
       "rag-tge_Mistral_v6         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2            94.7% ± 2.4%   \n",
       "rag-tge_Llama-3-8B_v3      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2            95.6% ± 2.9%   \n",
       "rag-tge_Mistral.Q8         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2            95.8% ± 2.1%   \n",
       "gpt-3.5-turbo-0125         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2            99.6% ± 0.3%   \n",
       "Mixtral-8x7B-Instruct-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2            92.9% ± 3.0%   \n",
       "gpt-4-turbo                0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2            98.5% ± 0.6%   \n",
       "Meta-Llama-3-70B-Instruct  0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2            95.6% ± 1.8%   \n",
       "zephyr-orpo-141b-A35b-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2           79.6% ± 10.1%   \n",
       "Meta-Llama-3-8B-Instruct   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2            95.1% ± 2.2%   \n",
       "Mistral-7B-Instruct-v0.2   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2            87.2% ± 3.8%   \n",
       "qwen1_5-110b-chat          0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2            97.6% ± 2.7%   \n",
       "qwen1_5-14b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2            97.5% ± 0.9%   \n",
       "qwen1_5-32b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2            99.3% ± 0.3%   \n",
       "zephyr-7b-beta             0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2            91.1% ± 6.9%   \n",
       "c4ai-command-r-plus        0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2            71.5% ± 6.1%   \n",
       "qwen1_5-7b-chat-q8_0       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2            86.2% ± 5.7%   \n",
       "gemma-1.1-7b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2            76.7% ± 8.3%   \n",
       "Phi-3-mini-4k-instruct     0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2            57.8% ± 7.0%   \n",
       "gemma-1.1-2b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             4.6% ± 1.0%   \n",
       "Mistral-7B-Instruct-v0.1   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             1.0% ± 0.0%   \n",
       "\n",
       "                                                                                                         citations/n_sentences  \\\n",
       "llm                        temperature nli                     ellm                     sim                                      \n",
       "rag-tge_Llama-3-8B_v1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             8.2 ± 1.2   \n",
       "rag-tge_Llama-3-8B_v2      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             7.5 ± 1.0   \n",
       "rag-tge_Mistral_v2-4480    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             4.7 ± 1.2   \n",
       "rag-tge_Mistral_v2-3360    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             5.6 ± 1.4   \n",
       "rag-tge_Mistral_v6         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             1.6 ± 0.1   \n",
       "rag-tge_Llama-3-8B_v3      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             1.5 ± 0.2   \n",
       "rag-tge_Mistral.Q8         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             2.2 ± 0.3   \n",
       "gpt-3.5-turbo-0125         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             2.0 ± 0.2   \n",
       "Mixtral-8x7B-Instruct-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             2.1 ± 0.1   \n",
       "gpt-4-turbo                0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             1.6 ± 0.2   \n",
       "Meta-Llama-3-70B-Instruct  0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             1.5 ± 0.1   \n",
       "zephyr-orpo-141b-A35b-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             2.0 ± 0.4   \n",
       "Meta-Llama-3-8B-Instruct   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             2.2 ± 0.2   \n",
       "Mistral-7B-Instruct-v0.2   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             2.6 ± 0.2   \n",
       "qwen1_5-110b-chat          0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             2.0 ± 0.2   \n",
       "qwen1_5-14b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             1.4 ± 0.1   \n",
       "qwen1_5-32b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             1.6 ± 0.1   \n",
       "zephyr-7b-beta             0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             3.3 ± 0.4   \n",
       "c4ai-command-r-plus        0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             2.1 ± 0.1   \n",
       "qwen1_5-7b-chat-q8_0       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             2.4 ± 0.2   \n",
       "gemma-1.1-7b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             1.3 ± 0.2   \n",
       "Phi-3-mini-4k-instruct     0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             8.4 ± 2.7   \n",
       "gemma-1.1-2b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             1.4 ± 0.0   \n",
       "Mistral-7B-Instruct-v0.1   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             1.4 ± 0.1   \n",
       "\n",
       "                                                                                                         citations/n_total_citations  \\\n",
       "llm                        temperature nli                     ellm                     sim                                            \n",
       "rag-tge_Llama-3-8B_v1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   8.5 ± 1.3   \n",
       "rag-tge_Llama-3-8B_v2      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   8.4 ± 1.3   \n",
       "rag-tge_Mistral_v2-4480    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   5.5 ± 1.1   \n",
       "rag-tge_Mistral_v2-3360    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   6.2 ± 1.4   \n",
       "rag-tge_Mistral_v6         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   2.2 ± 0.1   \n",
       "rag-tge_Llama-3-8B_v3      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   2.4 ± 0.2   \n",
       "rag-tge_Mistral.Q8         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   3.0 ± 0.3   \n",
       "gpt-3.5-turbo-0125         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   2.1 ± 0.2   \n",
       "Mixtral-8x7B-Instruct-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   2.3 ± 0.2   \n",
       "gpt-4-turbo                0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   1.8 ± 0.1   \n",
       "Meta-Llama-3-70B-Instruct  0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   1.9 ± 0.1   \n",
       "zephyr-orpo-141b-A35b-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   4.9 ± 1.3   \n",
       "Meta-Llama-3-8B-Instruct   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   2.0 ± 0.2   \n",
       "Mistral-7B-Instruct-v0.2   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   3.0 ± 0.4   \n",
       "qwen1_5-110b-chat          0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   1.7 ± 0.1   \n",
       "qwen1_5-14b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   1.6 ± 0.1   \n",
       "qwen1_5-32b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   1.4 ± 0.0   \n",
       "zephyr-7b-beta             0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   2.6 ± 0.7   \n",
       "c4ai-command-r-plus        0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   1.8 ± 0.3   \n",
       "qwen1_5-7b-chat-q8_0       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   1.3 ± 0.2   \n",
       "gemma-1.1-7b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   1.4 ± 0.2   \n",
       "Phi-3-mini-4k-instruct     0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   1.6 ± 0.3   \n",
       "gemma-1.1-2b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   0.1 ± 0.0   \n",
       "Mistral-7B-Instruct-v0.1   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   0.0 ± 0.0   \n",
       "\n",
       "                                                                                                         citations/n_correct_citations  \\\n",
       "llm                        temperature nli                     ellm                     sim                                              \n",
       "rag-tge_Llama-3-8B_v1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     8.0 ± 1.2   \n",
       "rag-tge_Llama-3-8B_v2      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     7.8 ± 1.1   \n",
       "rag-tge_Mistral_v2-4480    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     5.2 ± 1.1   \n",
       "rag-tge_Mistral_v2-3360    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     6.0 ± 1.3   \n",
       "rag-tge_Mistral_v6         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     2.1 ± 0.1   \n",
       "rag-tge_Llama-3-8B_v3      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     2.3 ± 0.2   \n",
       "rag-tge_Mistral.Q8         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     2.8 ± 0.3   \n",
       "gpt-3.5-turbo-0125         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     2.1 ± 0.2   \n",
       "Mixtral-8x7B-Instruct-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     2.1 ± 0.2   \n",
       "gpt-4-turbo                0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     1.8 ± 0.1   \n",
       "Meta-Llama-3-70B-Instruct  0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     1.8 ± 0.1   \n",
       "zephyr-orpo-141b-A35b-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     3.6 ± 1.1   \n",
       "Meta-Llama-3-8B-Instruct   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     1.9 ± 0.2   \n",
       "Mistral-7B-Instruct-v0.2   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     2.7 ± 0.3   \n",
       "qwen1_5-110b-chat          0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     1.6 ± 0.1   \n",
       "qwen1_5-14b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     1.5 ± 0.1   \n",
       "qwen1_5-32b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     1.4 ± 0.0   \n",
       "zephyr-7b-beta             0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     2.3 ± 0.6   \n",
       "c4ai-command-r-plus        0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     1.6 ± 0.3   \n",
       "qwen1_5-7b-chat-q8_0       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     1.3 ± 0.2   \n",
       "gemma-1.1-7b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     1.2 ± 0.2   \n",
       "Phi-3-mini-4k-instruct     0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     1.5 ± 0.3   \n",
       "gemma-1.1-2b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     0.1 ± 0.0   \n",
       "Mistral-7B-Instruct-v0.1   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     0.0 ± 0.0   \n",
       "\n",
       "                                                                                                         citations/n_correctly_multicited_sentences  \\\n",
       "llm                        temperature nli                     ellm                     sim                                                           \n",
       "rag-tge_Llama-3-8B_v1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  1.5 ± 0.5   \n",
       "rag-tge_Llama-3-8B_v2      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  1.7 ± 0.6   \n",
       "rag-tge_Mistral_v2-4480    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  1.1 ± 0.3   \n",
       "rag-tge_Mistral_v2-3360    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  1.1 ± 0.4   \n",
       "rag-tge_Mistral_v6         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  0.7 ± 0.1   \n",
       "rag-tge_Llama-3-8B_v3      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  0.9 ± 0.1   \n",
       "rag-tge_Mistral.Q8         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  0.8 ± 0.1   \n",
       "gpt-3.5-turbo-0125         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  0.5 ± 0.1   \n",
       "Mixtral-8x7B-Instruct-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  0.5 ± 0.1   \n",
       "gpt-4-turbo                0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  0.6 ± 0.1   \n",
       "Meta-Llama-3-70B-Instruct  0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  0.6 ± 0.0   \n",
       "zephyr-orpo-141b-A35b-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  0.6 ± 0.2   \n",
       "Meta-Llama-3-8B-Instruct   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  0.4 ± 0.1   \n",
       "Mistral-7B-Instruct-v0.2   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  0.9 ± 0.1   \n",
       "qwen1_5-110b-chat          0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  0.5 ± 0.1   \n",
       "qwen1_5-14b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  0.3 ± 0.0   \n",
       "qwen1_5-32b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  0.3 ± 0.0   \n",
       "zephyr-7b-beta             0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  0.5 ± 0.2   \n",
       "c4ai-command-r-plus        0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  0.4 ± 0.1   \n",
       "qwen1_5-7b-chat-q8_0       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  0.1 ± 0.1   \n",
       "gemma-1.1-7b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  0.4 ± 0.1   \n",
       "Phi-3-mini-4k-instruct     0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  0.5 ± 0.1   \n",
       "gemma-1.1-2b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  0.0 ± 0.0   \n",
       "Mistral-7B-Instruct-v0.1   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  0.0 ± 0.0   \n",
       "\n",
       "                                                                                                         citations/n_overcitations  \\\n",
       "llm                        temperature nli                     ellm                     sim                                          \n",
       "rag-tge_Llama-3-8B_v1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "rag-tge_Llama-3-8B_v2      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "rag-tge_Mistral_v2-4480    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "rag-tge_Mistral_v2-3360    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "rag-tge_Mistral_v6         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "rag-tge_Llama-3-8B_v3      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "rag-tge_Mistral.Q8         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "gpt-3.5-turbo-0125         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "Mixtral-8x7B-Instruct-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "gpt-4-turbo                0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "Meta-Llama-3-70B-Instruct  0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "zephyr-orpo-141b-A35b-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.1 ± 0.0   \n",
       "Meta-Llama-3-8B-Instruct   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "Mistral-7B-Instruct-v0.2   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "qwen1_5-110b-chat          0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "qwen1_5-14b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "qwen1_5-32b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "zephyr-7b-beta             0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "c4ai-command-r-plus        0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "qwen1_5-7b-chat-q8_0       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "gemma-1.1-7b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "Phi-3-mini-4k-instruct     0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "gemma-1.1-2b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "Mistral-7B-Instruct-v0.1   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "\n",
       "                                                                                                         correctness/answer_overlap  \\\n",
       "llm                        temperature nli                     ellm                     sim                                           \n",
       "rag-tge_Llama-3-8B_v1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               94.4% ± 1.4%   \n",
       "rag-tge_Llama-3-8B_v2      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               93.4% ± 1.5%   \n",
       "rag-tge_Mistral_v2-4480    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               91.5% ± 1.8%   \n",
       "rag-tge_Mistral_v2-3360    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               93.6% ± 3.1%   \n",
       "rag-tge_Mistral_v6         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               90.6% ± 3.2%   \n",
       "rag-tge_Llama-3-8B_v3      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               85.1% ± 4.5%   \n",
       "rag-tge_Mistral.Q8         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               86.1% ± 1.3%   \n",
       "gpt-3.5-turbo-0125         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               88.4% ± 1.3%   \n",
       "Mixtral-8x7B-Instruct-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               91.0% ± 0.1%   \n",
       "gpt-4-turbo                0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               87.7% ± 0.8%   \n",
       "Meta-Llama-3-70B-Instruct  0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               86.6% ± 2.1%   \n",
       "zephyr-orpo-141b-A35b-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               80.7% ± 6.0%   \n",
       "Meta-Llama-3-8B-Instruct   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               91.8% ± 0.0%   \n",
       "Mistral-7B-Instruct-v0.2   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               87.3% ± 1.5%   \n",
       "qwen1_5-110b-chat          0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               86.7% ± 2.0%   \n",
       "qwen1_5-14b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               87.6% ± 0.7%   \n",
       "qwen1_5-32b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               87.4% ± 0.6%   \n",
       "zephyr-7b-beta             0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               85.4% ± 2.9%   \n",
       "c4ai-command-r-plus        0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               73.6% ± 2.3%   \n",
       "qwen1_5-7b-chat-q8_0       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               86.5% ± 2.4%   \n",
       "gemma-1.1-7b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               72.3% ± 7.6%   \n",
       "Phi-3-mini-4k-instruct     0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               71.1% ± 1.2%   \n",
       "gemma-1.1-2b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               63.1% ± 3.8%   \n",
       "Mistral-7B-Instruct-v0.1   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               79.0% ± 1.4%   \n",
       "\n",
       "                                                                                                         correctness/answer_entail  \\\n",
       "llm                        temperature nli                     ellm                     sim                                          \n",
       "rag-tge_Llama-3-8B_v1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              92.3% ± 2.3%   \n",
       "rag-tge_Llama-3-8B_v2      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              93.3% ± 4.0%   \n",
       "rag-tge_Mistral_v2-4480    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              89.7% ± 2.3%   \n",
       "rag-tge_Mistral_v2-3360    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              91.0% ± 1.2%   \n",
       "rag-tge_Mistral_v6         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              93.0% ± 2.9%   \n",
       "rag-tge_Llama-3-8B_v3      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              90.0% ± 2.3%   \n",
       "rag-tge_Mistral.Q8         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              86.7% ± 0.6%   \n",
       "gpt-3.5-turbo-0125         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              91.0% ± 1.2%   \n",
       "Mixtral-8x7B-Instruct-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              89.7% ± 1.2%   \n",
       "gpt-4-turbo                0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              90.0% ± 1.2%   \n",
       "Meta-Llama-3-70B-Instruct  0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              87.3% ± 1.7%   \n",
       "zephyr-orpo-141b-A35b-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              86.0% ± 4.0%   \n",
       "Meta-Llama-3-8B-Instruct   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              90.3% ± 0.6%   \n",
       "Mistral-7B-Instruct-v0.2   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              88.7% ± 1.7%   \n",
       "qwen1_5-110b-chat          0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              89.3% ± 1.7%   \n",
       "qwen1_5-14b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              86.3% ± 0.6%   \n",
       "qwen1_5-32b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              89.7% ± 0.6%   \n",
       "zephyr-7b-beta             0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              86.7% ± 4.6%   \n",
       "c4ai-command-r-plus        0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              88.0% ± 0.0%   \n",
       "qwen1_5-7b-chat-q8_0       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              84.0% ± 2.3%   \n",
       "gemma-1.1-7b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              72.7% ± 7.5%   \n",
       "Phi-3-mini-4k-instruct     0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              78.0% ± 2.3%   \n",
       "gemma-1.1-2b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              67.0% ± 4.0%   \n",
       "Mistral-7B-Instruct-v0.1   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              82.7% ± 1.7%   \n",
       "\n",
       "                                                                                                         correctness/citations_recall  \\\n",
       "llm                        temperature nli                     ellm                     sim                                             \n",
       "rag-tge_Llama-3-8B_v1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 98.8% ± 0.9%   \n",
       "rag-tge_Llama-3-8B_v2      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 98.2% ± 0.6%   \n",
       "rag-tge_Mistral_v2-4480    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 97.2% ± 2.6%   \n",
       "rag-tge_Mistral_v2-3360    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 96.8% ± 2.6%   \n",
       "rag-tge_Mistral_v6         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 93.7% ± 2.3%   \n",
       "rag-tge_Llama-3-8B_v3      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 93.3% ± 2.5%   \n",
       "rag-tge_Mistral.Q8         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 91.5% ± 2.3%   \n",
       "gpt-3.5-turbo-0125         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 85.3% ± 2.3%   \n",
       "Mixtral-8x7B-Instruct-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 82.5% ± 3.5%   \n",
       "gpt-4-turbo                0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 79.5% ± 2.9%   \n",
       "Meta-Llama-3-70B-Instruct  0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 78.5% ± 1.7%   \n",
       "zephyr-orpo-141b-A35b-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 77.5% ± 9.2%   \n",
       "Meta-Llama-3-8B-Instruct   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 75.3% ± 2.3%   \n",
       "Mistral-7B-Instruct-v0.2   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 75.2% ± 2.6%   \n",
       "qwen1_5-110b-chat          0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 73.5% ± 5.2%   \n",
       "qwen1_5-14b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 66.0% ± 3.1%   \n",
       "qwen1_5-32b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 65.5% ± 0.9%   \n",
       "zephyr-7b-beta             0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 62.7% ± 8.0%   \n",
       "c4ai-command-r-plus        0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 57.8% ± 4.4%   \n",
       "qwen1_5-7b-chat-q8_0       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 54.8% ± 7.4%   \n",
       "gemma-1.1-7b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 47.2% ± 8.1%   \n",
       "Phi-3-mini-4k-instruct     0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 45.2% ± 5.1%   \n",
       "gemma-1.1-2b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                  3.3% ± 0.6%   \n",
       "Mistral-7B-Instruct-v0.1   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                  1.0% ± 0.0%   \n",
       "\n",
       "                                                                                                         correctness/citations_precision  \\\n",
       "llm                        temperature nli                     ellm                     sim                                                \n",
       "rag-tge_Llama-3-8B_v1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                    77.6% ± 9.9%   \n",
       "rag-tge_Llama-3-8B_v2      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   76.7% ± 13.6%   \n",
       "rag-tge_Mistral_v2-4480    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                    87.7% ± 7.0%   \n",
       "rag-tge_Mistral_v2-3360    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                    84.2% ± 8.4%   \n",
       "rag-tge_Mistral_v6         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                    96.4% ± 1.8%   \n",
       "rag-tge_Llama-3-8B_v3      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                    95.0% ± 0.9%   \n",
       "rag-tge_Mistral.Q8         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                    90.7% ± 4.1%   \n",
       "gpt-3.5-turbo-0125         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                    94.2% ± 2.4%   \n",
       "Mixtral-8x7B-Instruct-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                    87.6% ± 3.8%   \n",
       "gpt-4-turbo                0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                    93.9% ± 1.1%   \n",
       "Meta-Llama-3-70B-Instruct  0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                    90.8% ± 1.0%   \n",
       "zephyr-orpo-141b-A35b-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   54.6% ± 10.8%   \n",
       "Meta-Llama-3-8B-Instruct   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                    90.8% ± 1.8%   \n",
       "Mistral-7B-Instruct-v0.2   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                    72.2% ± 5.0%   \n",
       "qwen1_5-110b-chat          0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                    93.0% ± 2.3%   \n",
       "qwen1_5-14b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                    92.0% ± 1.6%   \n",
       "qwen1_5-32b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                    96.2% ± 0.0%   \n",
       "zephyr-7b-beta             0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                    72.8% ± 8.8%   \n",
       "c4ai-command-r-plus        0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                    64.4% ± 5.5%   \n",
       "qwen1_5-7b-chat-q8_0       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                    80.0% ± 7.8%   \n",
       "gemma-1.1-7b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   66.0% ± 10.1%   \n",
       "Phi-3-mini-4k-instruct     0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                    45.8% ± 6.4%   \n",
       "gemma-1.1-2b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     4.3% ± 0.7%   \n",
       "Mistral-7B-Instruct-v0.1   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     0.7% ± 0.0%   \n",
       "\n",
       "                                                                                                         quality/answer_relevance  \\\n",
       "llm                        temperature nli                     ellm                     sim                                         \n",
       "rag-tge_Llama-3-8B_v1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             73.5% ± 7.1%   \n",
       "rag-tge_Llama-3-8B_v2      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             74.6% ± 6.6%   \n",
       "rag-tge_Mistral_v2-4480    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             72.9% ± 5.5%   \n",
       "rag-tge_Mistral_v2-3360    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             72.6% ± 6.3%   \n",
       "rag-tge_Mistral_v6         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             76.1% ± 4.0%   \n",
       "rag-tge_Llama-3-8B_v3      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             76.1% ± 3.3%   \n",
       "rag-tge_Mistral.Q8         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             74.2% ± 4.0%   \n",
       "gpt-3.5-turbo-0125         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             75.2% ± 4.1%   \n",
       "Mixtral-8x7B-Instruct-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             72.6% ± 3.1%   \n",
       "gpt-4-turbo                0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             74.7% ± 3.4%   \n",
       "Meta-Llama-3-70B-Instruct  0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             75.3% ± 2.8%   \n",
       "zephyr-orpo-141b-A35b-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             69.6% ± 5.9%   \n",
       "Meta-Llama-3-8B-Instruct   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             72.0% ± 3.2%   \n",
       "Mistral-7B-Instruct-v0.2   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             72.0% ± 2.8%   \n",
       "qwen1_5-110b-chat          0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             73.7% ± 5.5%   \n",
       "qwen1_5-14b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             71.9% ± 3.9%   \n",
       "qwen1_5-32b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             71.0% ± 2.7%   \n",
       "zephyr-7b-beta             0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             71.5% ± 4.2%   \n",
       "c4ai-command-r-plus        0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             63.5% ± 4.7%   \n",
       "qwen1_5-7b-chat-q8_0       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             76.8% ± 3.4%   \n",
       "gemma-1.1-7b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             73.9% ± 4.3%   \n",
       "Phi-3-mini-4k-instruct     0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             66.4% ± 3.7%   \n",
       "gemma-1.1-2b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             70.9% ± 2.0%   \n",
       "Mistral-7B-Instruct-v0.1   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             73.8% ± 2.7%   \n",
       "\n",
       "                                                                                                          n_questions  \n",
       "llm                        temperature nli                     ellm                     sim                            \n",
       "rag-tge_Llama-3-8B_v1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          100  \n",
       "rag-tge_Llama-3-8B_v2      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          100  \n",
       "rag-tge_Mistral_v2-4480    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          100  \n",
       "rag-tge_Mistral_v2-3360    0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          100  \n",
       "rag-tge_Mistral_v6         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          100  \n",
       "rag-tge_Llama-3-8B_v3      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          100  \n",
       "rag-tge_Mistral.Q8         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          100  \n",
       "gpt-3.5-turbo-0125         0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          100  \n",
       "Mixtral-8x7B-Instruct-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          100  \n",
       "gpt-4-turbo                0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          100  \n",
       "Meta-Llama-3-70B-Instruct  0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          100  \n",
       "zephyr-orpo-141b-A35b-v0.1 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          100  \n",
       "Meta-Llama-3-8B-Instruct   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          100  \n",
       "Mistral-7B-Instruct-v0.2   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          100  \n",
       "qwen1_5-110b-chat          0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          100  \n",
       "qwen1_5-14b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          100  \n",
       "qwen1_5-32b-chat-q8_0      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          100  \n",
       "zephyr-7b-beta             0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          100  \n",
       "c4ai-command-r-plus        0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          100  \n",
       "qwen1_5-7b-chat-q8_0       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          100  \n",
       "gemma-1.1-7b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          100  \n",
       "Phi-3-mini-4k-instruct     0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          100  \n",
       "gemma-1.1-2b-it            0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          100  \n",
       "Mistral-7B-Instruct-v0.1   0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          100  "
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(Markdown(\"### Evaluation results\"))\n",
    "eval_not_mistral = eval_results[eval_results.index.get_level_values(\"llm\") != \"Mistral-7B-Instruct-v0.2\"]\n",
    "eval_mistral = eval_results[(eval_results.index.get_level_values(\"llm\") == \"Mistral-7B-Instruct-v0.2\") & (eval_results.index.get_level_values(\"prompt_id\") == \"1\") & (eval_results.index.get_level_values(\"temperature\") == \"0.1\")]\n",
    "eval_display = pd.concat([eval_not_mistral, eval_mistral])\n",
    "eval_display = eval_display[eval_display.index.get_level_values(\"llm\") != \"Mixtral-8x7B-Instruct-v0.1.Q8_0\"]  # use this model only for training data, because we already have same model without quantization\n",
    "eval_display = remove_index(eval_display, \"prompt_id\")\n",
    "eval_display = eval_display.sort_values(by=(\"correctness/citations_recall\"), ascending=False)\n",
    "eval_display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### sorted by: citations/ais_recall"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_4c0d8_row2_col0, #T_4c0d8_row2_col1, #T_4c0d8_row2_col2, #T_4c0d8_row2_col3, #T_4c0d8_row2_col4, #T_4c0d8_row2_col5, #T_4c0d8_row2_col6 {\n",
       "  font-weight: 1000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_4c0d8\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_4c0d8_level0_col0\" class=\"col_heading level0 col0\" >citations/ais_recall</th>\n",
       "      <th id=\"T_4c0d8_level0_col1\" class=\"col_heading level0 col1\" >citations/ais_precision</th>\n",
       "      <th id=\"T_4c0d8_level0_col2\" class=\"col_heading level0 col2\" >correctness/answer_overlap</th>\n",
       "      <th id=\"T_4c0d8_level0_col3\" class=\"col_heading level0 col3\" >correctness/answer_entail</th>\n",
       "      <th id=\"T_4c0d8_level0_col4\" class=\"col_heading level0 col4\" >correctness/citations_recall</th>\n",
       "      <th id=\"T_4c0d8_level0_col5\" class=\"col_heading level0 col5\" >correctness/citations_precision</th>\n",
       "      <th id=\"T_4c0d8_level0_col6\" class=\"col_heading level0 col6\" >quality/answer_relevance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >llm</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_4c0d8_level0_row0\" class=\"row_heading level0 row0\" >rag-tge_Llama-3-8B_v3</th>\n",
       "      <td id=\"T_4c0d8_row0_col0\" class=\"data row0 col0\" >81.3% ± 7.5%</td>\n",
       "      <td id=\"T_4c0d8_row0_col1\" class=\"data row0 col1\" >95.6% ± 2.9%</td>\n",
       "      <td id=\"T_4c0d8_row0_col2\" class=\"data row0 col2\" >85.1% ± 4.5%</td>\n",
       "      <td id=\"T_4c0d8_row0_col3\" class=\"data row0 col3\" >90.0% ± 2.3%</td>\n",
       "      <td id=\"T_4c0d8_row0_col4\" class=\"data row0 col4\" >93.3% ± 2.5%</td>\n",
       "      <td id=\"T_4c0d8_row0_col5\" class=\"data row0 col5\" >95.0% ± 0.9%</td>\n",
       "      <td id=\"T_4c0d8_row0_col6\" class=\"data row0 col6\" >76.1% ± 3.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c0d8_level0_row1\" class=\"row_heading level0 row1\" >rag-tge_Mistral.Q8</th>\n",
       "      <td id=\"T_4c0d8_row1_col0\" class=\"data row1 col0\" >81.2% ± 6.1%</td>\n",
       "      <td id=\"T_4c0d8_row1_col1\" class=\"data row1 col1\" >95.8% ± 2.1%</td>\n",
       "      <td id=\"T_4c0d8_row1_col2\" class=\"data row1 col2\" >86.1% ± 1.3%</td>\n",
       "      <td id=\"T_4c0d8_row1_col3\" class=\"data row1 col3\" >86.7% ± 0.6%</td>\n",
       "      <td id=\"T_4c0d8_row1_col4\" class=\"data row1 col4\" >91.5% ± 2.3%</td>\n",
       "      <td id=\"T_4c0d8_row1_col5\" class=\"data row1 col5\" >90.7% ± 4.1%</td>\n",
       "      <td id=\"T_4c0d8_row1_col6\" class=\"data row1 col6\" >74.2% ± 4.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c0d8_level0_row2\" class=\"row_heading level0 row2\" >rag-tge_Mistral_v6</th>\n",
       "      <td id=\"T_4c0d8_row2_col0\" class=\"data row2 col0\" >78.3% ± 8.2%</td>\n",
       "      <td id=\"T_4c0d8_row2_col1\" class=\"data row2 col1\" >94.7% ± 2.4%</td>\n",
       "      <td id=\"T_4c0d8_row2_col2\" class=\"data row2 col2\" >90.6% ± 3.2%</td>\n",
       "      <td id=\"T_4c0d8_row2_col3\" class=\"data row2 col3\" >93.0% ± 2.9%</td>\n",
       "      <td id=\"T_4c0d8_row2_col4\" class=\"data row2 col4\" >93.7% ± 2.3%</td>\n",
       "      <td id=\"T_4c0d8_row2_col5\" class=\"data row2 col5\" >96.4% ± 1.8%</td>\n",
       "      <td id=\"T_4c0d8_row2_col6\" class=\"data row2 col6\" >76.1% ± 4.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c0d8_level0_row3\" class=\"row_heading level0 row3\" >gpt-3.5-turbo-0125</th>\n",
       "      <td id=\"T_4c0d8_row3_col0\" class=\"data row3 col0\" >73.8% ± 7.3%</td>\n",
       "      <td id=\"T_4c0d8_row3_col1\" class=\"data row3 col1\" >99.6% ± 0.3%</td>\n",
       "      <td id=\"T_4c0d8_row3_col2\" class=\"data row3 col2\" >88.4% ± 1.3%</td>\n",
       "      <td id=\"T_4c0d8_row3_col3\" class=\"data row3 col3\" >91.0% ± 1.2%</td>\n",
       "      <td id=\"T_4c0d8_row3_col4\" class=\"data row3 col4\" >85.3% ± 2.3%</td>\n",
       "      <td id=\"T_4c0d8_row3_col5\" class=\"data row3 col5\" >94.2% ± 2.4%</td>\n",
       "      <td id=\"T_4c0d8_row3_col6\" class=\"data row3 col6\" >75.2% ± 4.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c0d8_level0_row4\" class=\"row_heading level0 row4\" >gpt-4-turbo</th>\n",
       "      <td id=\"T_4c0d8_row4_col0\" class=\"data row4 col0\" >73.3% ± 4.5%</td>\n",
       "      <td id=\"T_4c0d8_row4_col1\" class=\"data row4 col1\" >98.5% ± 0.6%</td>\n",
       "      <td id=\"T_4c0d8_row4_col2\" class=\"data row4 col2\" >87.7% ± 0.8%</td>\n",
       "      <td id=\"T_4c0d8_row4_col3\" class=\"data row4 col3\" >90.0% ± 1.2%</td>\n",
       "      <td id=\"T_4c0d8_row4_col4\" class=\"data row4 col4\" >79.5% ± 2.9%</td>\n",
       "      <td id=\"T_4c0d8_row4_col5\" class=\"data row4 col5\" >93.9% ± 1.1%</td>\n",
       "      <td id=\"T_4c0d8_row4_col6\" class=\"data row4 col6\" >74.7% ± 3.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c0d8_level0_row5\" class=\"row_heading level0 row5\" >rag-tge_Mistral_v2-4480</th>\n",
       "      <td id=\"T_4c0d8_row5_col0\" class=\"data row5 col0\" >73.3% ± 13.5%</td>\n",
       "      <td id=\"T_4c0d8_row5_col1\" class=\"data row5 col1\" >95.4% ± 3.1%</td>\n",
       "      <td id=\"T_4c0d8_row5_col2\" class=\"data row5 col2\" >91.5% ± 1.8%</td>\n",
       "      <td id=\"T_4c0d8_row5_col3\" class=\"data row5 col3\" >89.7% ± 2.3%</td>\n",
       "      <td id=\"T_4c0d8_row5_col4\" class=\"data row5 col4\" >97.2% ± 2.6%</td>\n",
       "      <td id=\"T_4c0d8_row5_col5\" class=\"data row5 col5\" >87.7% ± 7.0%</td>\n",
       "      <td id=\"T_4c0d8_row5_col6\" class=\"data row5 col6\" >72.9% ± 5.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c0d8_level0_row6\" class=\"row_heading level0 row6\" >Meta-Llama-3-70B-Instruct</th>\n",
       "      <td id=\"T_4c0d8_row6_col0\" class=\"data row6 col0\" >69.2% ± 5.0%</td>\n",
       "      <td id=\"T_4c0d8_row6_col1\" class=\"data row6 col1\" >95.6% ± 1.8%</td>\n",
       "      <td id=\"T_4c0d8_row6_col2\" class=\"data row6 col2\" >86.6% ± 2.1%</td>\n",
       "      <td id=\"T_4c0d8_row6_col3\" class=\"data row6 col3\" >87.3% ± 1.7%</td>\n",
       "      <td id=\"T_4c0d8_row6_col4\" class=\"data row6 col4\" >78.5% ± 1.7%</td>\n",
       "      <td id=\"T_4c0d8_row6_col5\" class=\"data row6 col5\" >90.8% ± 1.0%</td>\n",
       "      <td id=\"T_4c0d8_row6_col6\" class=\"data row6 col6\" >75.3% ± 2.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c0d8_level0_row7\" class=\"row_heading level0 row7\" >rag-tge_Mistral_v2-3360</th>\n",
       "      <td id=\"T_4c0d8_row7_col0\" class=\"data row7 col0\" >68.3% ± 14.6%</td>\n",
       "      <td id=\"T_4c0d8_row7_col1\" class=\"data row7 col1\" >95.8% ± 3.3%</td>\n",
       "      <td id=\"T_4c0d8_row7_col2\" class=\"data row7 col2\" >93.6% ± 3.1%</td>\n",
       "      <td id=\"T_4c0d8_row7_col3\" class=\"data row7 col3\" >91.0% ± 1.2%</td>\n",
       "      <td id=\"T_4c0d8_row7_col4\" class=\"data row7 col4\" >96.8% ± 2.6%</td>\n",
       "      <td id=\"T_4c0d8_row7_col5\" class=\"data row7 col5\" >84.2% ± 8.4%</td>\n",
       "      <td id=\"T_4c0d8_row7_col6\" class=\"data row7 col6\" >72.6% ± 6.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c0d8_level0_row8\" class=\"row_heading level0 row8\" >Mixtral-8x7B-Instruct-v0.1</th>\n",
       "      <td id=\"T_4c0d8_row8_col0\" class=\"data row8 col0\" >65.4% ± 6.1%</td>\n",
       "      <td id=\"T_4c0d8_row8_col1\" class=\"data row8 col1\" >92.9% ± 3.0%</td>\n",
       "      <td id=\"T_4c0d8_row8_col2\" class=\"data row8 col2\" >91.0% ± 0.1%</td>\n",
       "      <td id=\"T_4c0d8_row8_col3\" class=\"data row8 col3\" >89.7% ± 1.2%</td>\n",
       "      <td id=\"T_4c0d8_row8_col4\" class=\"data row8 col4\" >82.5% ± 3.5%</td>\n",
       "      <td id=\"T_4c0d8_row8_col5\" class=\"data row8 col5\" >87.6% ± 3.8%</td>\n",
       "      <td id=\"T_4c0d8_row8_col6\" class=\"data row8 col6\" >72.6% ± 3.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c0d8_level0_row9\" class=\"row_heading level0 row9\" >rag-tge_Llama-3-8B_v2</th>\n",
       "      <td id=\"T_4c0d8_row9_col0\" class=\"data row9 col0\" >65.4% ± 12.0%</td>\n",
       "      <td id=\"T_4c0d8_row9_col1\" class=\"data row9 col1\" >94.4% ± 5.0%</td>\n",
       "      <td id=\"T_4c0d8_row9_col2\" class=\"data row9 col2\" >93.4% ± 1.5%</td>\n",
       "      <td id=\"T_4c0d8_row9_col3\" class=\"data row9 col3\" >93.3% ± 4.0%</td>\n",
       "      <td id=\"T_4c0d8_row9_col4\" class=\"data row9 col4\" >98.2% ± 0.6%</td>\n",
       "      <td id=\"T_4c0d8_row9_col5\" class=\"data row9 col5\" >76.7% ± 13.6%</td>\n",
       "      <td id=\"T_4c0d8_row9_col6\" class=\"data row9 col6\" >74.6% ± 6.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c0d8_level0_row10\" class=\"row_heading level0 row10\" >rag-tge_Llama-3-8B_v1</th>\n",
       "      <td id=\"T_4c0d8_row10_col0\" class=\"data row10 col0\" >64.0% ± 10.0%</td>\n",
       "      <td id=\"T_4c0d8_row10_col1\" class=\"data row10 col1\" >95.4% ± 3.0%</td>\n",
       "      <td id=\"T_4c0d8_row10_col2\" class=\"data row10 col2\" >94.4% ± 1.4%</td>\n",
       "      <td id=\"T_4c0d8_row10_col3\" class=\"data row10 col3\" >92.3% ± 2.3%</td>\n",
       "      <td id=\"T_4c0d8_row10_col4\" class=\"data row10 col4\" >98.8% ± 0.9%</td>\n",
       "      <td id=\"T_4c0d8_row10_col5\" class=\"data row10 col5\" >77.6% ± 9.9%</td>\n",
       "      <td id=\"T_4c0d8_row10_col6\" class=\"data row10 col6\" >73.5% ± 7.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c0d8_level0_row11\" class=\"row_heading level0 row11\" >qwen1_5-14b-chat-q8_0</th>\n",
       "      <td id=\"T_4c0d8_row11_col0\" class=\"data row11 col0\" >63.3% ± 4.2%</td>\n",
       "      <td id=\"T_4c0d8_row11_col1\" class=\"data row11 col1\" >97.5% ± 0.9%</td>\n",
       "      <td id=\"T_4c0d8_row11_col2\" class=\"data row11 col2\" >87.6% ± 0.7%</td>\n",
       "      <td id=\"T_4c0d8_row11_col3\" class=\"data row11 col3\" >86.3% ± 0.6%</td>\n",
       "      <td id=\"T_4c0d8_row11_col4\" class=\"data row11 col4\" >66.0% ± 3.1%</td>\n",
       "      <td id=\"T_4c0d8_row11_col5\" class=\"data row11 col5\" >92.0% ± 1.6%</td>\n",
       "      <td id=\"T_4c0d8_row11_col6\" class=\"data row11 col6\" >71.9% ± 3.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c0d8_level0_row12\" class=\"row_heading level0 row12\" >Mistral-7B-Instruct-v0.2</th>\n",
       "      <td id=\"T_4c0d8_row12_col0\" class=\"data row12 col0\" >57.6% ± 3.7%</td>\n",
       "      <td id=\"T_4c0d8_row12_col1\" class=\"data row12 col1\" >87.2% ± 3.8%</td>\n",
       "      <td id=\"T_4c0d8_row12_col2\" class=\"data row12 col2\" >87.3% ± 1.5%</td>\n",
       "      <td id=\"T_4c0d8_row12_col3\" class=\"data row12 col3\" >88.7% ± 1.7%</td>\n",
       "      <td id=\"T_4c0d8_row12_col4\" class=\"data row12 col4\" >75.2% ± 2.6%</td>\n",
       "      <td id=\"T_4c0d8_row12_col5\" class=\"data row12 col5\" >72.2% ± 5.0%</td>\n",
       "      <td id=\"T_4c0d8_row12_col6\" class=\"data row12 col6\" >72.0% ± 2.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c0d8_level0_row13\" class=\"row_heading level0 row13\" >qwen1_5-110b-chat</th>\n",
       "      <td id=\"T_4c0d8_row13_col0\" class=\"data row13 col0\" >52.8% ± 9.5%</td>\n",
       "      <td id=\"T_4c0d8_row13_col1\" class=\"data row13 col1\" >97.6% ± 2.7%</td>\n",
       "      <td id=\"T_4c0d8_row13_col2\" class=\"data row13 col2\" >86.7% ± 2.0%</td>\n",
       "      <td id=\"T_4c0d8_row13_col3\" class=\"data row13 col3\" >89.3% ± 1.7%</td>\n",
       "      <td id=\"T_4c0d8_row13_col4\" class=\"data row13 col4\" >73.5% ± 5.2%</td>\n",
       "      <td id=\"T_4c0d8_row13_col5\" class=\"data row13 col5\" >93.0% ± 2.3%</td>\n",
       "      <td id=\"T_4c0d8_row13_col6\" class=\"data row13 col6\" >73.7% ± 5.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c0d8_level0_row14\" class=\"row_heading level0 row14\" >qwen1_5-32b-chat-q8_0</th>\n",
       "      <td id=\"T_4c0d8_row14_col0\" class=\"data row14 col0\" >51.2% ± 3.2%</td>\n",
       "      <td id=\"T_4c0d8_row14_col1\" class=\"data row14 col1\" >99.3% ± 0.3%</td>\n",
       "      <td id=\"T_4c0d8_row14_col2\" class=\"data row14 col2\" >87.4% ± 0.6%</td>\n",
       "      <td id=\"T_4c0d8_row14_col3\" class=\"data row14 col3\" >89.7% ± 0.6%</td>\n",
       "      <td id=\"T_4c0d8_row14_col4\" class=\"data row14 col4\" >65.5% ± 0.9%</td>\n",
       "      <td id=\"T_4c0d8_row14_col5\" class=\"data row14 col5\" >96.2% ± 0.0%</td>\n",
       "      <td id=\"T_4c0d8_row14_col6\" class=\"data row14 col6\" >71.0% ± 2.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c0d8_level0_row15\" class=\"row_heading level0 row15\" >Meta-Llama-3-8B-Instruct</th>\n",
       "      <td id=\"T_4c0d8_row15_col0\" class=\"data row15 col0\" >49.3% ± 6.1%</td>\n",
       "      <td id=\"T_4c0d8_row15_col1\" class=\"data row15 col1\" >95.1% ± 2.2%</td>\n",
       "      <td id=\"T_4c0d8_row15_col2\" class=\"data row15 col2\" >91.8% ± 0.0%</td>\n",
       "      <td id=\"T_4c0d8_row15_col3\" class=\"data row15 col3\" >90.3% ± 0.6%</td>\n",
       "      <td id=\"T_4c0d8_row15_col4\" class=\"data row15 col4\" >75.3% ± 2.3%</td>\n",
       "      <td id=\"T_4c0d8_row15_col5\" class=\"data row15 col5\" >90.8% ± 1.8%</td>\n",
       "      <td id=\"T_4c0d8_row15_col6\" class=\"data row15 col6\" >72.0% ± 3.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c0d8_level0_row16\" class=\"row_heading level0 row16\" >zephyr-orpo-141b-A35b-v0.1</th>\n",
       "      <td id=\"T_4c0d8_row16_col0\" class=\"data row16 col0\" >41.8% ± 13.4%</td>\n",
       "      <td id=\"T_4c0d8_row16_col1\" class=\"data row16 col1\" >79.6% ± 10.1%</td>\n",
       "      <td id=\"T_4c0d8_row16_col2\" class=\"data row16 col2\" >80.7% ± 6.0%</td>\n",
       "      <td id=\"T_4c0d8_row16_col3\" class=\"data row16 col3\" >86.0% ± 4.0%</td>\n",
       "      <td id=\"T_4c0d8_row16_col4\" class=\"data row16 col4\" >77.5% ± 9.2%</td>\n",
       "      <td id=\"T_4c0d8_row16_col5\" class=\"data row16 col5\" >54.6% ± 10.8%</td>\n",
       "      <td id=\"T_4c0d8_row16_col6\" class=\"data row16 col6\" >69.6% ± 5.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c0d8_level0_row17\" class=\"row_heading level0 row17\" >c4ai-command-r-plus</th>\n",
       "      <td id=\"T_4c0d8_row17_col0\" class=\"data row17 col0\" >40.9% ± 6.0%</td>\n",
       "      <td id=\"T_4c0d8_row17_col1\" class=\"data row17 col1\" >71.5% ± 6.1%</td>\n",
       "      <td id=\"T_4c0d8_row17_col2\" class=\"data row17 col2\" >73.6% ± 2.3%</td>\n",
       "      <td id=\"T_4c0d8_row17_col3\" class=\"data row17 col3\" >88.0% ± 0.0%</td>\n",
       "      <td id=\"T_4c0d8_row17_col4\" class=\"data row17 col4\" >57.8% ± 4.4%</td>\n",
       "      <td id=\"T_4c0d8_row17_col5\" class=\"data row17 col5\" >64.4% ± 5.5%</td>\n",
       "      <td id=\"T_4c0d8_row17_col6\" class=\"data row17 col6\" >63.5% ± 4.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c0d8_level0_row18\" class=\"row_heading level0 row18\" >zephyr-7b-beta</th>\n",
       "      <td id=\"T_4c0d8_row18_col0\" class=\"data row18 col0\" >40.7% ± 9.1%</td>\n",
       "      <td id=\"T_4c0d8_row18_col1\" class=\"data row18 col1\" >91.1% ± 6.9%</td>\n",
       "      <td id=\"T_4c0d8_row18_col2\" class=\"data row18 col2\" >85.4% ± 2.9%</td>\n",
       "      <td id=\"T_4c0d8_row18_col3\" class=\"data row18 col3\" >86.7% ± 4.6%</td>\n",
       "      <td id=\"T_4c0d8_row18_col4\" class=\"data row18 col4\" >62.7% ± 8.0%</td>\n",
       "      <td id=\"T_4c0d8_row18_col5\" class=\"data row18 col5\" >72.8% ± 8.8%</td>\n",
       "      <td id=\"T_4c0d8_row18_col6\" class=\"data row18 col6\" >71.5% ± 4.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c0d8_level0_row19\" class=\"row_heading level0 row19\" >gemma-1.1-7b-it</th>\n",
       "      <td id=\"T_4c0d8_row19_col0\" class=\"data row19 col0\" >36.8% ± 12.3%</td>\n",
       "      <td id=\"T_4c0d8_row19_col1\" class=\"data row19 col1\" >76.7% ± 8.3%</td>\n",
       "      <td id=\"T_4c0d8_row19_col2\" class=\"data row19 col2\" >72.3% ± 7.6%</td>\n",
       "      <td id=\"T_4c0d8_row19_col3\" class=\"data row19 col3\" >72.7% ± 7.5%</td>\n",
       "      <td id=\"T_4c0d8_row19_col4\" class=\"data row19 col4\" >47.2% ± 8.1%</td>\n",
       "      <td id=\"T_4c0d8_row19_col5\" class=\"data row19 col5\" >66.0% ± 10.1%</td>\n",
       "      <td id=\"T_4c0d8_row19_col6\" class=\"data row19 col6\" >73.9% ± 4.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c0d8_level0_row20\" class=\"row_heading level0 row20\" >Phi-3-mini-4k-instruct</th>\n",
       "      <td id=\"T_4c0d8_row20_col0\" class=\"data row20 col0\" >25.3% ± 3.1%</td>\n",
       "      <td id=\"T_4c0d8_row20_col1\" class=\"data row20 col1\" >57.8% ± 7.0%</td>\n",
       "      <td id=\"T_4c0d8_row20_col2\" class=\"data row20 col2\" >71.1% ± 1.2%</td>\n",
       "      <td id=\"T_4c0d8_row20_col3\" class=\"data row20 col3\" >78.0% ± 2.3%</td>\n",
       "      <td id=\"T_4c0d8_row20_col4\" class=\"data row20 col4\" >45.2% ± 5.1%</td>\n",
       "      <td id=\"T_4c0d8_row20_col5\" class=\"data row20 col5\" >45.8% ± 6.4%</td>\n",
       "      <td id=\"T_4c0d8_row20_col6\" class=\"data row20 col6\" >66.4% ± 3.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c0d8_level0_row21\" class=\"row_heading level0 row21\" >qwen1_5-7b-chat-q8_0</th>\n",
       "      <td id=\"T_4c0d8_row21_col0\" class=\"data row21 col0\" >23.3% ± 6.4%</td>\n",
       "      <td id=\"T_4c0d8_row21_col1\" class=\"data row21 col1\" >86.2% ± 5.7%</td>\n",
       "      <td id=\"T_4c0d8_row21_col2\" class=\"data row21 col2\" >86.5% ± 2.4%</td>\n",
       "      <td id=\"T_4c0d8_row21_col3\" class=\"data row21 col3\" >84.0% ± 2.3%</td>\n",
       "      <td id=\"T_4c0d8_row21_col4\" class=\"data row21 col4\" >54.8% ± 7.4%</td>\n",
       "      <td id=\"T_4c0d8_row21_col5\" class=\"data row21 col5\" >80.0% ± 7.8%</td>\n",
       "      <td id=\"T_4c0d8_row21_col6\" class=\"data row21 col6\" >76.8% ± 3.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c0d8_level0_row22\" class=\"row_heading level0 row22\" >gemma-1.1-2b-it</th>\n",
       "      <td id=\"T_4c0d8_row22_col0\" class=\"data row22 col0\" >0.7% ± 0.6%</td>\n",
       "      <td id=\"T_4c0d8_row22_col1\" class=\"data row22 col1\" >4.6% ± 1.0%</td>\n",
       "      <td id=\"T_4c0d8_row22_col2\" class=\"data row22 col2\" >63.1% ± 3.8%</td>\n",
       "      <td id=\"T_4c0d8_row22_col3\" class=\"data row22 col3\" >67.0% ± 4.0%</td>\n",
       "      <td id=\"T_4c0d8_row22_col4\" class=\"data row22 col4\" >3.3% ± 0.6%</td>\n",
       "      <td id=\"T_4c0d8_row22_col5\" class=\"data row22 col5\" >4.3% ± 0.7%</td>\n",
       "      <td id=\"T_4c0d8_row22_col6\" class=\"data row22 col6\" >70.9% ± 2.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_4c0d8_level0_row23\" class=\"row_heading level0 row23\" >Mistral-7B-Instruct-v0.1</th>\n",
       "      <td id=\"T_4c0d8_row23_col0\" class=\"data row23 col0\" >0.5% ± 0.0%</td>\n",
       "      <td id=\"T_4c0d8_row23_col1\" class=\"data row23 col1\" >1.0% ± 0.0%</td>\n",
       "      <td id=\"T_4c0d8_row23_col2\" class=\"data row23 col2\" >79.0% ± 1.4%</td>\n",
       "      <td id=\"T_4c0d8_row23_col3\" class=\"data row23 col3\" >82.7% ± 1.7%</td>\n",
       "      <td id=\"T_4c0d8_row23_col4\" class=\"data row23 col4\" >1.0% ± 0.0%</td>\n",
       "      <td id=\"T_4c0d8_row23_col5\" class=\"data row23 col5\" >0.7% ± 0.0%</td>\n",
       "      <td id=\"T_4c0d8_row23_col6\" class=\"data row23 col6\" >73.8% ± 2.7%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fe801a14e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### sorted by: citations/ais_precision"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_484a1_row12_col0, #T_484a1_row12_col1, #T_484a1_row12_col2, #T_484a1_row12_col3, #T_484a1_row12_col4, #T_484a1_row12_col5, #T_484a1_row12_col6 {\n",
       "  font-weight: 1000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_484a1\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_484a1_level0_col0\" class=\"col_heading level0 col0\" >citations/ais_recall</th>\n",
       "      <th id=\"T_484a1_level0_col1\" class=\"col_heading level0 col1\" >citations/ais_precision</th>\n",
       "      <th id=\"T_484a1_level0_col2\" class=\"col_heading level0 col2\" >correctness/answer_overlap</th>\n",
       "      <th id=\"T_484a1_level0_col3\" class=\"col_heading level0 col3\" >correctness/answer_entail</th>\n",
       "      <th id=\"T_484a1_level0_col4\" class=\"col_heading level0 col4\" >correctness/citations_recall</th>\n",
       "      <th id=\"T_484a1_level0_col5\" class=\"col_heading level0 col5\" >correctness/citations_precision</th>\n",
       "      <th id=\"T_484a1_level0_col6\" class=\"col_heading level0 col6\" >quality/answer_relevance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >llm</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_484a1_level0_row0\" class=\"row_heading level0 row0\" >gpt-3.5-turbo-0125</th>\n",
       "      <td id=\"T_484a1_row0_col0\" class=\"data row0 col0\" >73.8% ± 7.3%</td>\n",
       "      <td id=\"T_484a1_row0_col1\" class=\"data row0 col1\" >99.6% ± 0.3%</td>\n",
       "      <td id=\"T_484a1_row0_col2\" class=\"data row0 col2\" >88.4% ± 1.3%</td>\n",
       "      <td id=\"T_484a1_row0_col3\" class=\"data row0 col3\" >91.0% ± 1.2%</td>\n",
       "      <td id=\"T_484a1_row0_col4\" class=\"data row0 col4\" >85.3% ± 2.3%</td>\n",
       "      <td id=\"T_484a1_row0_col5\" class=\"data row0 col5\" >94.2% ± 2.4%</td>\n",
       "      <td id=\"T_484a1_row0_col6\" class=\"data row0 col6\" >75.2% ± 4.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_484a1_level0_row1\" class=\"row_heading level0 row1\" >qwen1_5-32b-chat-q8_0</th>\n",
       "      <td id=\"T_484a1_row1_col0\" class=\"data row1 col0\" >51.2% ± 3.2%</td>\n",
       "      <td id=\"T_484a1_row1_col1\" class=\"data row1 col1\" >99.3% ± 0.3%</td>\n",
       "      <td id=\"T_484a1_row1_col2\" class=\"data row1 col2\" >87.4% ± 0.6%</td>\n",
       "      <td id=\"T_484a1_row1_col3\" class=\"data row1 col3\" >89.7% ± 0.6%</td>\n",
       "      <td id=\"T_484a1_row1_col4\" class=\"data row1 col4\" >65.5% ± 0.9%</td>\n",
       "      <td id=\"T_484a1_row1_col5\" class=\"data row1 col5\" >96.2% ± 0.0%</td>\n",
       "      <td id=\"T_484a1_row1_col6\" class=\"data row1 col6\" >71.0% ± 2.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_484a1_level0_row2\" class=\"row_heading level0 row2\" >gpt-4-turbo</th>\n",
       "      <td id=\"T_484a1_row2_col0\" class=\"data row2 col0\" >73.3% ± 4.5%</td>\n",
       "      <td id=\"T_484a1_row2_col1\" class=\"data row2 col1\" >98.5% ± 0.6%</td>\n",
       "      <td id=\"T_484a1_row2_col2\" class=\"data row2 col2\" >87.7% ± 0.8%</td>\n",
       "      <td id=\"T_484a1_row2_col3\" class=\"data row2 col3\" >90.0% ± 1.2%</td>\n",
       "      <td id=\"T_484a1_row2_col4\" class=\"data row2 col4\" >79.5% ± 2.9%</td>\n",
       "      <td id=\"T_484a1_row2_col5\" class=\"data row2 col5\" >93.9% ± 1.1%</td>\n",
       "      <td id=\"T_484a1_row2_col6\" class=\"data row2 col6\" >74.7% ± 3.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_484a1_level0_row3\" class=\"row_heading level0 row3\" >qwen1_5-110b-chat</th>\n",
       "      <td id=\"T_484a1_row3_col0\" class=\"data row3 col0\" >52.8% ± 9.5%</td>\n",
       "      <td id=\"T_484a1_row3_col1\" class=\"data row3 col1\" >97.6% ± 2.7%</td>\n",
       "      <td id=\"T_484a1_row3_col2\" class=\"data row3 col2\" >86.7% ± 2.0%</td>\n",
       "      <td id=\"T_484a1_row3_col3\" class=\"data row3 col3\" >89.3% ± 1.7%</td>\n",
       "      <td id=\"T_484a1_row3_col4\" class=\"data row3 col4\" >73.5% ± 5.2%</td>\n",
       "      <td id=\"T_484a1_row3_col5\" class=\"data row3 col5\" >93.0% ± 2.3%</td>\n",
       "      <td id=\"T_484a1_row3_col6\" class=\"data row3 col6\" >73.7% ± 5.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_484a1_level0_row4\" class=\"row_heading level0 row4\" >qwen1_5-14b-chat-q8_0</th>\n",
       "      <td id=\"T_484a1_row4_col0\" class=\"data row4 col0\" >63.3% ± 4.2%</td>\n",
       "      <td id=\"T_484a1_row4_col1\" class=\"data row4 col1\" >97.5% ± 0.9%</td>\n",
       "      <td id=\"T_484a1_row4_col2\" class=\"data row4 col2\" >87.6% ± 0.7%</td>\n",
       "      <td id=\"T_484a1_row4_col3\" class=\"data row4 col3\" >86.3% ± 0.6%</td>\n",
       "      <td id=\"T_484a1_row4_col4\" class=\"data row4 col4\" >66.0% ± 3.1%</td>\n",
       "      <td id=\"T_484a1_row4_col5\" class=\"data row4 col5\" >92.0% ± 1.6%</td>\n",
       "      <td id=\"T_484a1_row4_col6\" class=\"data row4 col6\" >71.9% ± 3.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_484a1_level0_row5\" class=\"row_heading level0 row5\" >rag-tge_Mistral_v2-3360</th>\n",
       "      <td id=\"T_484a1_row5_col0\" class=\"data row5 col0\" >68.3% ± 14.6%</td>\n",
       "      <td id=\"T_484a1_row5_col1\" class=\"data row5 col1\" >95.8% ± 3.3%</td>\n",
       "      <td id=\"T_484a1_row5_col2\" class=\"data row5 col2\" >93.6% ± 3.1%</td>\n",
       "      <td id=\"T_484a1_row5_col3\" class=\"data row5 col3\" >91.0% ± 1.2%</td>\n",
       "      <td id=\"T_484a1_row5_col4\" class=\"data row5 col4\" >96.8% ± 2.6%</td>\n",
       "      <td id=\"T_484a1_row5_col5\" class=\"data row5 col5\" >84.2% ± 8.4%</td>\n",
       "      <td id=\"T_484a1_row5_col6\" class=\"data row5 col6\" >72.6% ± 6.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_484a1_level0_row6\" class=\"row_heading level0 row6\" >rag-tge_Mistral.Q8</th>\n",
       "      <td id=\"T_484a1_row6_col0\" class=\"data row6 col0\" >81.2% ± 6.1%</td>\n",
       "      <td id=\"T_484a1_row6_col1\" class=\"data row6 col1\" >95.8% ± 2.1%</td>\n",
       "      <td id=\"T_484a1_row6_col2\" class=\"data row6 col2\" >86.1% ± 1.3%</td>\n",
       "      <td id=\"T_484a1_row6_col3\" class=\"data row6 col3\" >86.7% ± 0.6%</td>\n",
       "      <td id=\"T_484a1_row6_col4\" class=\"data row6 col4\" >91.5% ± 2.3%</td>\n",
       "      <td id=\"T_484a1_row6_col5\" class=\"data row6 col5\" >90.7% ± 4.1%</td>\n",
       "      <td id=\"T_484a1_row6_col6\" class=\"data row6 col6\" >74.2% ± 4.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_484a1_level0_row7\" class=\"row_heading level0 row7\" >rag-tge_Llama-3-8B_v3</th>\n",
       "      <td id=\"T_484a1_row7_col0\" class=\"data row7 col0\" >81.3% ± 7.5%</td>\n",
       "      <td id=\"T_484a1_row7_col1\" class=\"data row7 col1\" >95.6% ± 2.9%</td>\n",
       "      <td id=\"T_484a1_row7_col2\" class=\"data row7 col2\" >85.1% ± 4.5%</td>\n",
       "      <td id=\"T_484a1_row7_col3\" class=\"data row7 col3\" >90.0% ± 2.3%</td>\n",
       "      <td id=\"T_484a1_row7_col4\" class=\"data row7 col4\" >93.3% ± 2.5%</td>\n",
       "      <td id=\"T_484a1_row7_col5\" class=\"data row7 col5\" >95.0% ± 0.9%</td>\n",
       "      <td id=\"T_484a1_row7_col6\" class=\"data row7 col6\" >76.1% ± 3.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_484a1_level0_row8\" class=\"row_heading level0 row8\" >Meta-Llama-3-70B-Instruct</th>\n",
       "      <td id=\"T_484a1_row8_col0\" class=\"data row8 col0\" >69.2% ± 5.0%</td>\n",
       "      <td id=\"T_484a1_row8_col1\" class=\"data row8 col1\" >95.6% ± 1.8%</td>\n",
       "      <td id=\"T_484a1_row8_col2\" class=\"data row8 col2\" >86.6% ± 2.1%</td>\n",
       "      <td id=\"T_484a1_row8_col3\" class=\"data row8 col3\" >87.3% ± 1.7%</td>\n",
       "      <td id=\"T_484a1_row8_col4\" class=\"data row8 col4\" >78.5% ± 1.7%</td>\n",
       "      <td id=\"T_484a1_row8_col5\" class=\"data row8 col5\" >90.8% ± 1.0%</td>\n",
       "      <td id=\"T_484a1_row8_col6\" class=\"data row8 col6\" >75.3% ± 2.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_484a1_level0_row9\" class=\"row_heading level0 row9\" >rag-tge_Mistral_v2-4480</th>\n",
       "      <td id=\"T_484a1_row9_col0\" class=\"data row9 col0\" >73.3% ± 13.5%</td>\n",
       "      <td id=\"T_484a1_row9_col1\" class=\"data row9 col1\" >95.4% ± 3.1%</td>\n",
       "      <td id=\"T_484a1_row9_col2\" class=\"data row9 col2\" >91.5% ± 1.8%</td>\n",
       "      <td id=\"T_484a1_row9_col3\" class=\"data row9 col3\" >89.7% ± 2.3%</td>\n",
       "      <td id=\"T_484a1_row9_col4\" class=\"data row9 col4\" >97.2% ± 2.6%</td>\n",
       "      <td id=\"T_484a1_row9_col5\" class=\"data row9 col5\" >87.7% ± 7.0%</td>\n",
       "      <td id=\"T_484a1_row9_col6\" class=\"data row9 col6\" >72.9% ± 5.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_484a1_level0_row10\" class=\"row_heading level0 row10\" >rag-tge_Llama-3-8B_v1</th>\n",
       "      <td id=\"T_484a1_row10_col0\" class=\"data row10 col0\" >64.0% ± 10.0%</td>\n",
       "      <td id=\"T_484a1_row10_col1\" class=\"data row10 col1\" >95.4% ± 3.0%</td>\n",
       "      <td id=\"T_484a1_row10_col2\" class=\"data row10 col2\" >94.4% ± 1.4%</td>\n",
       "      <td id=\"T_484a1_row10_col3\" class=\"data row10 col3\" >92.3% ± 2.3%</td>\n",
       "      <td id=\"T_484a1_row10_col4\" class=\"data row10 col4\" >98.8% ± 0.9%</td>\n",
       "      <td id=\"T_484a1_row10_col5\" class=\"data row10 col5\" >77.6% ± 9.9%</td>\n",
       "      <td id=\"T_484a1_row10_col6\" class=\"data row10 col6\" >73.5% ± 7.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_484a1_level0_row11\" class=\"row_heading level0 row11\" >Meta-Llama-3-8B-Instruct</th>\n",
       "      <td id=\"T_484a1_row11_col0\" class=\"data row11 col0\" >49.3% ± 6.1%</td>\n",
       "      <td id=\"T_484a1_row11_col1\" class=\"data row11 col1\" >95.1% ± 2.2%</td>\n",
       "      <td id=\"T_484a1_row11_col2\" class=\"data row11 col2\" >91.8% ± 0.0%</td>\n",
       "      <td id=\"T_484a1_row11_col3\" class=\"data row11 col3\" >90.3% ± 0.6%</td>\n",
       "      <td id=\"T_484a1_row11_col4\" class=\"data row11 col4\" >75.3% ± 2.3%</td>\n",
       "      <td id=\"T_484a1_row11_col5\" class=\"data row11 col5\" >90.8% ± 1.8%</td>\n",
       "      <td id=\"T_484a1_row11_col6\" class=\"data row11 col6\" >72.0% ± 3.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_484a1_level0_row12\" class=\"row_heading level0 row12\" >rag-tge_Mistral_v6</th>\n",
       "      <td id=\"T_484a1_row12_col0\" class=\"data row12 col0\" >78.3% ± 8.2%</td>\n",
       "      <td id=\"T_484a1_row12_col1\" class=\"data row12 col1\" >94.7% ± 2.4%</td>\n",
       "      <td id=\"T_484a1_row12_col2\" class=\"data row12 col2\" >90.6% ± 3.2%</td>\n",
       "      <td id=\"T_484a1_row12_col3\" class=\"data row12 col3\" >93.0% ± 2.9%</td>\n",
       "      <td id=\"T_484a1_row12_col4\" class=\"data row12 col4\" >93.7% ± 2.3%</td>\n",
       "      <td id=\"T_484a1_row12_col5\" class=\"data row12 col5\" >96.4% ± 1.8%</td>\n",
       "      <td id=\"T_484a1_row12_col6\" class=\"data row12 col6\" >76.1% ± 4.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_484a1_level0_row13\" class=\"row_heading level0 row13\" >rag-tge_Llama-3-8B_v2</th>\n",
       "      <td id=\"T_484a1_row13_col0\" class=\"data row13 col0\" >65.4% ± 12.0%</td>\n",
       "      <td id=\"T_484a1_row13_col1\" class=\"data row13 col1\" >94.4% ± 5.0%</td>\n",
       "      <td id=\"T_484a1_row13_col2\" class=\"data row13 col2\" >93.4% ± 1.5%</td>\n",
       "      <td id=\"T_484a1_row13_col3\" class=\"data row13 col3\" >93.3% ± 4.0%</td>\n",
       "      <td id=\"T_484a1_row13_col4\" class=\"data row13 col4\" >98.2% ± 0.6%</td>\n",
       "      <td id=\"T_484a1_row13_col5\" class=\"data row13 col5\" >76.7% ± 13.6%</td>\n",
       "      <td id=\"T_484a1_row13_col6\" class=\"data row13 col6\" >74.6% ± 6.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_484a1_level0_row14\" class=\"row_heading level0 row14\" >Mixtral-8x7B-Instruct-v0.1</th>\n",
       "      <td id=\"T_484a1_row14_col0\" class=\"data row14 col0\" >65.4% ± 6.1%</td>\n",
       "      <td id=\"T_484a1_row14_col1\" class=\"data row14 col1\" >92.9% ± 3.0%</td>\n",
       "      <td id=\"T_484a1_row14_col2\" class=\"data row14 col2\" >91.0% ± 0.1%</td>\n",
       "      <td id=\"T_484a1_row14_col3\" class=\"data row14 col3\" >89.7% ± 1.2%</td>\n",
       "      <td id=\"T_484a1_row14_col4\" class=\"data row14 col4\" >82.5% ± 3.5%</td>\n",
       "      <td id=\"T_484a1_row14_col5\" class=\"data row14 col5\" >87.6% ± 3.8%</td>\n",
       "      <td id=\"T_484a1_row14_col6\" class=\"data row14 col6\" >72.6% ± 3.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_484a1_level0_row15\" class=\"row_heading level0 row15\" >zephyr-7b-beta</th>\n",
       "      <td id=\"T_484a1_row15_col0\" class=\"data row15 col0\" >40.7% ± 9.1%</td>\n",
       "      <td id=\"T_484a1_row15_col1\" class=\"data row15 col1\" >91.1% ± 6.9%</td>\n",
       "      <td id=\"T_484a1_row15_col2\" class=\"data row15 col2\" >85.4% ± 2.9%</td>\n",
       "      <td id=\"T_484a1_row15_col3\" class=\"data row15 col3\" >86.7% ± 4.6%</td>\n",
       "      <td id=\"T_484a1_row15_col4\" class=\"data row15 col4\" >62.7% ± 8.0%</td>\n",
       "      <td id=\"T_484a1_row15_col5\" class=\"data row15 col5\" >72.8% ± 8.8%</td>\n",
       "      <td id=\"T_484a1_row15_col6\" class=\"data row15 col6\" >71.5% ± 4.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_484a1_level0_row16\" class=\"row_heading level0 row16\" >Mistral-7B-Instruct-v0.2</th>\n",
       "      <td id=\"T_484a1_row16_col0\" class=\"data row16 col0\" >57.6% ± 3.7%</td>\n",
       "      <td id=\"T_484a1_row16_col1\" class=\"data row16 col1\" >87.2% ± 3.8%</td>\n",
       "      <td id=\"T_484a1_row16_col2\" class=\"data row16 col2\" >87.3% ± 1.5%</td>\n",
       "      <td id=\"T_484a1_row16_col3\" class=\"data row16 col3\" >88.7% ± 1.7%</td>\n",
       "      <td id=\"T_484a1_row16_col4\" class=\"data row16 col4\" >75.2% ± 2.6%</td>\n",
       "      <td id=\"T_484a1_row16_col5\" class=\"data row16 col5\" >72.2% ± 5.0%</td>\n",
       "      <td id=\"T_484a1_row16_col6\" class=\"data row16 col6\" >72.0% ± 2.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_484a1_level0_row17\" class=\"row_heading level0 row17\" >qwen1_5-7b-chat-q8_0</th>\n",
       "      <td id=\"T_484a1_row17_col0\" class=\"data row17 col0\" >23.3% ± 6.4%</td>\n",
       "      <td id=\"T_484a1_row17_col1\" class=\"data row17 col1\" >86.2% ± 5.7%</td>\n",
       "      <td id=\"T_484a1_row17_col2\" class=\"data row17 col2\" >86.5% ± 2.4%</td>\n",
       "      <td id=\"T_484a1_row17_col3\" class=\"data row17 col3\" >84.0% ± 2.3%</td>\n",
       "      <td id=\"T_484a1_row17_col4\" class=\"data row17 col4\" >54.8% ± 7.4%</td>\n",
       "      <td id=\"T_484a1_row17_col5\" class=\"data row17 col5\" >80.0% ± 7.8%</td>\n",
       "      <td id=\"T_484a1_row17_col6\" class=\"data row17 col6\" >76.8% ± 3.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_484a1_level0_row18\" class=\"row_heading level0 row18\" >zephyr-orpo-141b-A35b-v0.1</th>\n",
       "      <td id=\"T_484a1_row18_col0\" class=\"data row18 col0\" >41.8% ± 13.4%</td>\n",
       "      <td id=\"T_484a1_row18_col1\" class=\"data row18 col1\" >79.6% ± 10.1%</td>\n",
       "      <td id=\"T_484a1_row18_col2\" class=\"data row18 col2\" >80.7% ± 6.0%</td>\n",
       "      <td id=\"T_484a1_row18_col3\" class=\"data row18 col3\" >86.0% ± 4.0%</td>\n",
       "      <td id=\"T_484a1_row18_col4\" class=\"data row18 col4\" >77.5% ± 9.2%</td>\n",
       "      <td id=\"T_484a1_row18_col5\" class=\"data row18 col5\" >54.6% ± 10.8%</td>\n",
       "      <td id=\"T_484a1_row18_col6\" class=\"data row18 col6\" >69.6% ± 5.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_484a1_level0_row19\" class=\"row_heading level0 row19\" >gemma-1.1-7b-it</th>\n",
       "      <td id=\"T_484a1_row19_col0\" class=\"data row19 col0\" >36.8% ± 12.3%</td>\n",
       "      <td id=\"T_484a1_row19_col1\" class=\"data row19 col1\" >76.7% ± 8.3%</td>\n",
       "      <td id=\"T_484a1_row19_col2\" class=\"data row19 col2\" >72.3% ± 7.6%</td>\n",
       "      <td id=\"T_484a1_row19_col3\" class=\"data row19 col3\" >72.7% ± 7.5%</td>\n",
       "      <td id=\"T_484a1_row19_col4\" class=\"data row19 col4\" >47.2% ± 8.1%</td>\n",
       "      <td id=\"T_484a1_row19_col5\" class=\"data row19 col5\" >66.0% ± 10.1%</td>\n",
       "      <td id=\"T_484a1_row19_col6\" class=\"data row19 col6\" >73.9% ± 4.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_484a1_level0_row20\" class=\"row_heading level0 row20\" >c4ai-command-r-plus</th>\n",
       "      <td id=\"T_484a1_row20_col0\" class=\"data row20 col0\" >40.9% ± 6.0%</td>\n",
       "      <td id=\"T_484a1_row20_col1\" class=\"data row20 col1\" >71.5% ± 6.1%</td>\n",
       "      <td id=\"T_484a1_row20_col2\" class=\"data row20 col2\" >73.6% ± 2.3%</td>\n",
       "      <td id=\"T_484a1_row20_col3\" class=\"data row20 col3\" >88.0% ± 0.0%</td>\n",
       "      <td id=\"T_484a1_row20_col4\" class=\"data row20 col4\" >57.8% ± 4.4%</td>\n",
       "      <td id=\"T_484a1_row20_col5\" class=\"data row20 col5\" >64.4% ± 5.5%</td>\n",
       "      <td id=\"T_484a1_row20_col6\" class=\"data row20 col6\" >63.5% ± 4.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_484a1_level0_row21\" class=\"row_heading level0 row21\" >Phi-3-mini-4k-instruct</th>\n",
       "      <td id=\"T_484a1_row21_col0\" class=\"data row21 col0\" >25.3% ± 3.1%</td>\n",
       "      <td id=\"T_484a1_row21_col1\" class=\"data row21 col1\" >57.8% ± 7.0%</td>\n",
       "      <td id=\"T_484a1_row21_col2\" class=\"data row21 col2\" >71.1% ± 1.2%</td>\n",
       "      <td id=\"T_484a1_row21_col3\" class=\"data row21 col3\" >78.0% ± 2.3%</td>\n",
       "      <td id=\"T_484a1_row21_col4\" class=\"data row21 col4\" >45.2% ± 5.1%</td>\n",
       "      <td id=\"T_484a1_row21_col5\" class=\"data row21 col5\" >45.8% ± 6.4%</td>\n",
       "      <td id=\"T_484a1_row21_col6\" class=\"data row21 col6\" >66.4% ± 3.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_484a1_level0_row22\" class=\"row_heading level0 row22\" >gemma-1.1-2b-it</th>\n",
       "      <td id=\"T_484a1_row22_col0\" class=\"data row22 col0\" >0.7% ± 0.6%</td>\n",
       "      <td id=\"T_484a1_row22_col1\" class=\"data row22 col1\" >4.6% ± 1.0%</td>\n",
       "      <td id=\"T_484a1_row22_col2\" class=\"data row22 col2\" >63.1% ± 3.8%</td>\n",
       "      <td id=\"T_484a1_row22_col3\" class=\"data row22 col3\" >67.0% ± 4.0%</td>\n",
       "      <td id=\"T_484a1_row22_col4\" class=\"data row22 col4\" >3.3% ± 0.6%</td>\n",
       "      <td id=\"T_484a1_row22_col5\" class=\"data row22 col5\" >4.3% ± 0.7%</td>\n",
       "      <td id=\"T_484a1_row22_col6\" class=\"data row22 col6\" >70.9% ± 2.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_484a1_level0_row23\" class=\"row_heading level0 row23\" >Mistral-7B-Instruct-v0.1</th>\n",
       "      <td id=\"T_484a1_row23_col0\" class=\"data row23 col0\" >0.5% ± 0.0%</td>\n",
       "      <td id=\"T_484a1_row23_col1\" class=\"data row23 col1\" >1.0% ± 0.0%</td>\n",
       "      <td id=\"T_484a1_row23_col2\" class=\"data row23 col2\" >79.0% ± 1.4%</td>\n",
       "      <td id=\"T_484a1_row23_col3\" class=\"data row23 col3\" >82.7% ± 1.7%</td>\n",
       "      <td id=\"T_484a1_row23_col4\" class=\"data row23 col4\" >1.0% ± 0.0%</td>\n",
       "      <td id=\"T_484a1_row23_col5\" class=\"data row23 col5\" >0.7% ± 0.0%</td>\n",
       "      <td id=\"T_484a1_row23_col6\" class=\"data row23 col6\" >73.8% ± 2.7%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fe801a14e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### sorted by: correctness/answer_overlap"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_25f73_row6_col0, #T_25f73_row6_col1, #T_25f73_row6_col2, #T_25f73_row6_col3, #T_25f73_row6_col4, #T_25f73_row6_col5, #T_25f73_row6_col6 {\n",
       "  font-weight: 1000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_25f73\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_25f73_level0_col0\" class=\"col_heading level0 col0\" >citations/ais_recall</th>\n",
       "      <th id=\"T_25f73_level0_col1\" class=\"col_heading level0 col1\" >citations/ais_precision</th>\n",
       "      <th id=\"T_25f73_level0_col2\" class=\"col_heading level0 col2\" >correctness/answer_overlap</th>\n",
       "      <th id=\"T_25f73_level0_col3\" class=\"col_heading level0 col3\" >correctness/answer_entail</th>\n",
       "      <th id=\"T_25f73_level0_col4\" class=\"col_heading level0 col4\" >correctness/citations_recall</th>\n",
       "      <th id=\"T_25f73_level0_col5\" class=\"col_heading level0 col5\" >correctness/citations_precision</th>\n",
       "      <th id=\"T_25f73_level0_col6\" class=\"col_heading level0 col6\" >quality/answer_relevance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >llm</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_25f73_level0_row0\" class=\"row_heading level0 row0\" >rag-tge_Llama-3-8B_v1</th>\n",
       "      <td id=\"T_25f73_row0_col0\" class=\"data row0 col0\" >64.0% ± 10.0%</td>\n",
       "      <td id=\"T_25f73_row0_col1\" class=\"data row0 col1\" >95.4% ± 3.0%</td>\n",
       "      <td id=\"T_25f73_row0_col2\" class=\"data row0 col2\" >94.4% ± 1.4%</td>\n",
       "      <td id=\"T_25f73_row0_col3\" class=\"data row0 col3\" >92.3% ± 2.3%</td>\n",
       "      <td id=\"T_25f73_row0_col4\" class=\"data row0 col4\" >98.8% ± 0.9%</td>\n",
       "      <td id=\"T_25f73_row0_col5\" class=\"data row0 col5\" >77.6% ± 9.9%</td>\n",
       "      <td id=\"T_25f73_row0_col6\" class=\"data row0 col6\" >73.5% ± 7.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25f73_level0_row1\" class=\"row_heading level0 row1\" >rag-tge_Mistral_v2-3360</th>\n",
       "      <td id=\"T_25f73_row1_col0\" class=\"data row1 col0\" >68.3% ± 14.6%</td>\n",
       "      <td id=\"T_25f73_row1_col1\" class=\"data row1 col1\" >95.8% ± 3.3%</td>\n",
       "      <td id=\"T_25f73_row1_col2\" class=\"data row1 col2\" >93.6% ± 3.1%</td>\n",
       "      <td id=\"T_25f73_row1_col3\" class=\"data row1 col3\" >91.0% ± 1.2%</td>\n",
       "      <td id=\"T_25f73_row1_col4\" class=\"data row1 col4\" >96.8% ± 2.6%</td>\n",
       "      <td id=\"T_25f73_row1_col5\" class=\"data row1 col5\" >84.2% ± 8.4%</td>\n",
       "      <td id=\"T_25f73_row1_col6\" class=\"data row1 col6\" >72.6% ± 6.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25f73_level0_row2\" class=\"row_heading level0 row2\" >rag-tge_Llama-3-8B_v2</th>\n",
       "      <td id=\"T_25f73_row2_col0\" class=\"data row2 col0\" >65.4% ± 12.0%</td>\n",
       "      <td id=\"T_25f73_row2_col1\" class=\"data row2 col1\" >94.4% ± 5.0%</td>\n",
       "      <td id=\"T_25f73_row2_col2\" class=\"data row2 col2\" >93.4% ± 1.5%</td>\n",
       "      <td id=\"T_25f73_row2_col3\" class=\"data row2 col3\" >93.3% ± 4.0%</td>\n",
       "      <td id=\"T_25f73_row2_col4\" class=\"data row2 col4\" >98.2% ± 0.6%</td>\n",
       "      <td id=\"T_25f73_row2_col5\" class=\"data row2 col5\" >76.7% ± 13.6%</td>\n",
       "      <td id=\"T_25f73_row2_col6\" class=\"data row2 col6\" >74.6% ± 6.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25f73_level0_row3\" class=\"row_heading level0 row3\" >Meta-Llama-3-8B-Instruct</th>\n",
       "      <td id=\"T_25f73_row3_col0\" class=\"data row3 col0\" >49.3% ± 6.1%</td>\n",
       "      <td id=\"T_25f73_row3_col1\" class=\"data row3 col1\" >95.1% ± 2.2%</td>\n",
       "      <td id=\"T_25f73_row3_col2\" class=\"data row3 col2\" >91.8% ± 0.0%</td>\n",
       "      <td id=\"T_25f73_row3_col3\" class=\"data row3 col3\" >90.3% ± 0.6%</td>\n",
       "      <td id=\"T_25f73_row3_col4\" class=\"data row3 col4\" >75.3% ± 2.3%</td>\n",
       "      <td id=\"T_25f73_row3_col5\" class=\"data row3 col5\" >90.8% ± 1.8%</td>\n",
       "      <td id=\"T_25f73_row3_col6\" class=\"data row3 col6\" >72.0% ± 3.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25f73_level0_row4\" class=\"row_heading level0 row4\" >rag-tge_Mistral_v2-4480</th>\n",
       "      <td id=\"T_25f73_row4_col0\" class=\"data row4 col0\" >73.3% ± 13.5%</td>\n",
       "      <td id=\"T_25f73_row4_col1\" class=\"data row4 col1\" >95.4% ± 3.1%</td>\n",
       "      <td id=\"T_25f73_row4_col2\" class=\"data row4 col2\" >91.5% ± 1.8%</td>\n",
       "      <td id=\"T_25f73_row4_col3\" class=\"data row4 col3\" >89.7% ± 2.3%</td>\n",
       "      <td id=\"T_25f73_row4_col4\" class=\"data row4 col4\" >97.2% ± 2.6%</td>\n",
       "      <td id=\"T_25f73_row4_col5\" class=\"data row4 col5\" >87.7% ± 7.0%</td>\n",
       "      <td id=\"T_25f73_row4_col6\" class=\"data row4 col6\" >72.9% ± 5.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25f73_level0_row5\" class=\"row_heading level0 row5\" >Mixtral-8x7B-Instruct-v0.1</th>\n",
       "      <td id=\"T_25f73_row5_col0\" class=\"data row5 col0\" >65.4% ± 6.1%</td>\n",
       "      <td id=\"T_25f73_row5_col1\" class=\"data row5 col1\" >92.9% ± 3.0%</td>\n",
       "      <td id=\"T_25f73_row5_col2\" class=\"data row5 col2\" >91.0% ± 0.1%</td>\n",
       "      <td id=\"T_25f73_row5_col3\" class=\"data row5 col3\" >89.7% ± 1.2%</td>\n",
       "      <td id=\"T_25f73_row5_col4\" class=\"data row5 col4\" >82.5% ± 3.5%</td>\n",
       "      <td id=\"T_25f73_row5_col5\" class=\"data row5 col5\" >87.6% ± 3.8%</td>\n",
       "      <td id=\"T_25f73_row5_col6\" class=\"data row5 col6\" >72.6% ± 3.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25f73_level0_row6\" class=\"row_heading level0 row6\" >rag-tge_Mistral_v6</th>\n",
       "      <td id=\"T_25f73_row6_col0\" class=\"data row6 col0\" >78.3% ± 8.2%</td>\n",
       "      <td id=\"T_25f73_row6_col1\" class=\"data row6 col1\" >94.7% ± 2.4%</td>\n",
       "      <td id=\"T_25f73_row6_col2\" class=\"data row6 col2\" >90.6% ± 3.2%</td>\n",
       "      <td id=\"T_25f73_row6_col3\" class=\"data row6 col3\" >93.0% ± 2.9%</td>\n",
       "      <td id=\"T_25f73_row6_col4\" class=\"data row6 col4\" >93.7% ± 2.3%</td>\n",
       "      <td id=\"T_25f73_row6_col5\" class=\"data row6 col5\" >96.4% ± 1.8%</td>\n",
       "      <td id=\"T_25f73_row6_col6\" class=\"data row6 col6\" >76.1% ± 4.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25f73_level0_row7\" class=\"row_heading level0 row7\" >gpt-3.5-turbo-0125</th>\n",
       "      <td id=\"T_25f73_row7_col0\" class=\"data row7 col0\" >73.8% ± 7.3%</td>\n",
       "      <td id=\"T_25f73_row7_col1\" class=\"data row7 col1\" >99.6% ± 0.3%</td>\n",
       "      <td id=\"T_25f73_row7_col2\" class=\"data row7 col2\" >88.4% ± 1.3%</td>\n",
       "      <td id=\"T_25f73_row7_col3\" class=\"data row7 col3\" >91.0% ± 1.2%</td>\n",
       "      <td id=\"T_25f73_row7_col4\" class=\"data row7 col4\" >85.3% ± 2.3%</td>\n",
       "      <td id=\"T_25f73_row7_col5\" class=\"data row7 col5\" >94.2% ± 2.4%</td>\n",
       "      <td id=\"T_25f73_row7_col6\" class=\"data row7 col6\" >75.2% ± 4.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25f73_level0_row8\" class=\"row_heading level0 row8\" >gpt-4-turbo</th>\n",
       "      <td id=\"T_25f73_row8_col0\" class=\"data row8 col0\" >73.3% ± 4.5%</td>\n",
       "      <td id=\"T_25f73_row8_col1\" class=\"data row8 col1\" >98.5% ± 0.6%</td>\n",
       "      <td id=\"T_25f73_row8_col2\" class=\"data row8 col2\" >87.7% ± 0.8%</td>\n",
       "      <td id=\"T_25f73_row8_col3\" class=\"data row8 col3\" >90.0% ± 1.2%</td>\n",
       "      <td id=\"T_25f73_row8_col4\" class=\"data row8 col4\" >79.5% ± 2.9%</td>\n",
       "      <td id=\"T_25f73_row8_col5\" class=\"data row8 col5\" >93.9% ± 1.1%</td>\n",
       "      <td id=\"T_25f73_row8_col6\" class=\"data row8 col6\" >74.7% ± 3.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25f73_level0_row9\" class=\"row_heading level0 row9\" >qwen1_5-14b-chat-q8_0</th>\n",
       "      <td id=\"T_25f73_row9_col0\" class=\"data row9 col0\" >63.3% ± 4.2%</td>\n",
       "      <td id=\"T_25f73_row9_col1\" class=\"data row9 col1\" >97.5% ± 0.9%</td>\n",
       "      <td id=\"T_25f73_row9_col2\" class=\"data row9 col2\" >87.6% ± 0.7%</td>\n",
       "      <td id=\"T_25f73_row9_col3\" class=\"data row9 col3\" >86.3% ± 0.6%</td>\n",
       "      <td id=\"T_25f73_row9_col4\" class=\"data row9 col4\" >66.0% ± 3.1%</td>\n",
       "      <td id=\"T_25f73_row9_col5\" class=\"data row9 col5\" >92.0% ± 1.6%</td>\n",
       "      <td id=\"T_25f73_row9_col6\" class=\"data row9 col6\" >71.9% ± 3.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25f73_level0_row10\" class=\"row_heading level0 row10\" >qwen1_5-32b-chat-q8_0</th>\n",
       "      <td id=\"T_25f73_row10_col0\" class=\"data row10 col0\" >51.2% ± 3.2%</td>\n",
       "      <td id=\"T_25f73_row10_col1\" class=\"data row10 col1\" >99.3% ± 0.3%</td>\n",
       "      <td id=\"T_25f73_row10_col2\" class=\"data row10 col2\" >87.4% ± 0.6%</td>\n",
       "      <td id=\"T_25f73_row10_col3\" class=\"data row10 col3\" >89.7% ± 0.6%</td>\n",
       "      <td id=\"T_25f73_row10_col4\" class=\"data row10 col4\" >65.5% ± 0.9%</td>\n",
       "      <td id=\"T_25f73_row10_col5\" class=\"data row10 col5\" >96.2% ± 0.0%</td>\n",
       "      <td id=\"T_25f73_row10_col6\" class=\"data row10 col6\" >71.0% ± 2.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25f73_level0_row11\" class=\"row_heading level0 row11\" >Mistral-7B-Instruct-v0.2</th>\n",
       "      <td id=\"T_25f73_row11_col0\" class=\"data row11 col0\" >57.6% ± 3.7%</td>\n",
       "      <td id=\"T_25f73_row11_col1\" class=\"data row11 col1\" >87.2% ± 3.8%</td>\n",
       "      <td id=\"T_25f73_row11_col2\" class=\"data row11 col2\" >87.3% ± 1.5%</td>\n",
       "      <td id=\"T_25f73_row11_col3\" class=\"data row11 col3\" >88.7% ± 1.7%</td>\n",
       "      <td id=\"T_25f73_row11_col4\" class=\"data row11 col4\" >75.2% ± 2.6%</td>\n",
       "      <td id=\"T_25f73_row11_col5\" class=\"data row11 col5\" >72.2% ± 5.0%</td>\n",
       "      <td id=\"T_25f73_row11_col6\" class=\"data row11 col6\" >72.0% ± 2.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25f73_level0_row12\" class=\"row_heading level0 row12\" >qwen1_5-110b-chat</th>\n",
       "      <td id=\"T_25f73_row12_col0\" class=\"data row12 col0\" >52.8% ± 9.5%</td>\n",
       "      <td id=\"T_25f73_row12_col1\" class=\"data row12 col1\" >97.6% ± 2.7%</td>\n",
       "      <td id=\"T_25f73_row12_col2\" class=\"data row12 col2\" >86.7% ± 2.0%</td>\n",
       "      <td id=\"T_25f73_row12_col3\" class=\"data row12 col3\" >89.3% ± 1.7%</td>\n",
       "      <td id=\"T_25f73_row12_col4\" class=\"data row12 col4\" >73.5% ± 5.2%</td>\n",
       "      <td id=\"T_25f73_row12_col5\" class=\"data row12 col5\" >93.0% ± 2.3%</td>\n",
       "      <td id=\"T_25f73_row12_col6\" class=\"data row12 col6\" >73.7% ± 5.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25f73_level0_row13\" class=\"row_heading level0 row13\" >Meta-Llama-3-70B-Instruct</th>\n",
       "      <td id=\"T_25f73_row13_col0\" class=\"data row13 col0\" >69.2% ± 5.0%</td>\n",
       "      <td id=\"T_25f73_row13_col1\" class=\"data row13 col1\" >95.6% ± 1.8%</td>\n",
       "      <td id=\"T_25f73_row13_col2\" class=\"data row13 col2\" >86.6% ± 2.1%</td>\n",
       "      <td id=\"T_25f73_row13_col3\" class=\"data row13 col3\" >87.3% ± 1.7%</td>\n",
       "      <td id=\"T_25f73_row13_col4\" class=\"data row13 col4\" >78.5% ± 1.7%</td>\n",
       "      <td id=\"T_25f73_row13_col5\" class=\"data row13 col5\" >90.8% ± 1.0%</td>\n",
       "      <td id=\"T_25f73_row13_col6\" class=\"data row13 col6\" >75.3% ± 2.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25f73_level0_row14\" class=\"row_heading level0 row14\" >qwen1_5-7b-chat-q8_0</th>\n",
       "      <td id=\"T_25f73_row14_col0\" class=\"data row14 col0\" >23.3% ± 6.4%</td>\n",
       "      <td id=\"T_25f73_row14_col1\" class=\"data row14 col1\" >86.2% ± 5.7%</td>\n",
       "      <td id=\"T_25f73_row14_col2\" class=\"data row14 col2\" >86.5% ± 2.4%</td>\n",
       "      <td id=\"T_25f73_row14_col3\" class=\"data row14 col3\" >84.0% ± 2.3%</td>\n",
       "      <td id=\"T_25f73_row14_col4\" class=\"data row14 col4\" >54.8% ± 7.4%</td>\n",
       "      <td id=\"T_25f73_row14_col5\" class=\"data row14 col5\" >80.0% ± 7.8%</td>\n",
       "      <td id=\"T_25f73_row14_col6\" class=\"data row14 col6\" >76.8% ± 3.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25f73_level0_row15\" class=\"row_heading level0 row15\" >rag-tge_Mistral.Q8</th>\n",
       "      <td id=\"T_25f73_row15_col0\" class=\"data row15 col0\" >81.2% ± 6.1%</td>\n",
       "      <td id=\"T_25f73_row15_col1\" class=\"data row15 col1\" >95.8% ± 2.1%</td>\n",
       "      <td id=\"T_25f73_row15_col2\" class=\"data row15 col2\" >86.1% ± 1.3%</td>\n",
       "      <td id=\"T_25f73_row15_col3\" class=\"data row15 col3\" >86.7% ± 0.6%</td>\n",
       "      <td id=\"T_25f73_row15_col4\" class=\"data row15 col4\" >91.5% ± 2.3%</td>\n",
       "      <td id=\"T_25f73_row15_col5\" class=\"data row15 col5\" >90.7% ± 4.1%</td>\n",
       "      <td id=\"T_25f73_row15_col6\" class=\"data row15 col6\" >74.2% ± 4.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25f73_level0_row16\" class=\"row_heading level0 row16\" >zephyr-7b-beta</th>\n",
       "      <td id=\"T_25f73_row16_col0\" class=\"data row16 col0\" >40.7% ± 9.1%</td>\n",
       "      <td id=\"T_25f73_row16_col1\" class=\"data row16 col1\" >91.1% ± 6.9%</td>\n",
       "      <td id=\"T_25f73_row16_col2\" class=\"data row16 col2\" >85.4% ± 2.9%</td>\n",
       "      <td id=\"T_25f73_row16_col3\" class=\"data row16 col3\" >86.7% ± 4.6%</td>\n",
       "      <td id=\"T_25f73_row16_col4\" class=\"data row16 col4\" >62.7% ± 8.0%</td>\n",
       "      <td id=\"T_25f73_row16_col5\" class=\"data row16 col5\" >72.8% ± 8.8%</td>\n",
       "      <td id=\"T_25f73_row16_col6\" class=\"data row16 col6\" >71.5% ± 4.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25f73_level0_row17\" class=\"row_heading level0 row17\" >rag-tge_Llama-3-8B_v3</th>\n",
       "      <td id=\"T_25f73_row17_col0\" class=\"data row17 col0\" >81.3% ± 7.5%</td>\n",
       "      <td id=\"T_25f73_row17_col1\" class=\"data row17 col1\" >95.6% ± 2.9%</td>\n",
       "      <td id=\"T_25f73_row17_col2\" class=\"data row17 col2\" >85.1% ± 4.5%</td>\n",
       "      <td id=\"T_25f73_row17_col3\" class=\"data row17 col3\" >90.0% ± 2.3%</td>\n",
       "      <td id=\"T_25f73_row17_col4\" class=\"data row17 col4\" >93.3% ± 2.5%</td>\n",
       "      <td id=\"T_25f73_row17_col5\" class=\"data row17 col5\" >95.0% ± 0.9%</td>\n",
       "      <td id=\"T_25f73_row17_col6\" class=\"data row17 col6\" >76.1% ± 3.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25f73_level0_row18\" class=\"row_heading level0 row18\" >zephyr-orpo-141b-A35b-v0.1</th>\n",
       "      <td id=\"T_25f73_row18_col0\" class=\"data row18 col0\" >41.8% ± 13.4%</td>\n",
       "      <td id=\"T_25f73_row18_col1\" class=\"data row18 col1\" >79.6% ± 10.1%</td>\n",
       "      <td id=\"T_25f73_row18_col2\" class=\"data row18 col2\" >80.7% ± 6.0%</td>\n",
       "      <td id=\"T_25f73_row18_col3\" class=\"data row18 col3\" >86.0% ± 4.0%</td>\n",
       "      <td id=\"T_25f73_row18_col4\" class=\"data row18 col4\" >77.5% ± 9.2%</td>\n",
       "      <td id=\"T_25f73_row18_col5\" class=\"data row18 col5\" >54.6% ± 10.8%</td>\n",
       "      <td id=\"T_25f73_row18_col6\" class=\"data row18 col6\" >69.6% ± 5.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25f73_level0_row19\" class=\"row_heading level0 row19\" >Mistral-7B-Instruct-v0.1</th>\n",
       "      <td id=\"T_25f73_row19_col0\" class=\"data row19 col0\" >0.5% ± 0.0%</td>\n",
       "      <td id=\"T_25f73_row19_col1\" class=\"data row19 col1\" >1.0% ± 0.0%</td>\n",
       "      <td id=\"T_25f73_row19_col2\" class=\"data row19 col2\" >79.0% ± 1.4%</td>\n",
       "      <td id=\"T_25f73_row19_col3\" class=\"data row19 col3\" >82.7% ± 1.7%</td>\n",
       "      <td id=\"T_25f73_row19_col4\" class=\"data row19 col4\" >1.0% ± 0.0%</td>\n",
       "      <td id=\"T_25f73_row19_col5\" class=\"data row19 col5\" >0.7% ± 0.0%</td>\n",
       "      <td id=\"T_25f73_row19_col6\" class=\"data row19 col6\" >73.8% ± 2.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25f73_level0_row20\" class=\"row_heading level0 row20\" >c4ai-command-r-plus</th>\n",
       "      <td id=\"T_25f73_row20_col0\" class=\"data row20 col0\" >40.9% ± 6.0%</td>\n",
       "      <td id=\"T_25f73_row20_col1\" class=\"data row20 col1\" >71.5% ± 6.1%</td>\n",
       "      <td id=\"T_25f73_row20_col2\" class=\"data row20 col2\" >73.6% ± 2.3%</td>\n",
       "      <td id=\"T_25f73_row20_col3\" class=\"data row20 col3\" >88.0% ± 0.0%</td>\n",
       "      <td id=\"T_25f73_row20_col4\" class=\"data row20 col4\" >57.8% ± 4.4%</td>\n",
       "      <td id=\"T_25f73_row20_col5\" class=\"data row20 col5\" >64.4% ± 5.5%</td>\n",
       "      <td id=\"T_25f73_row20_col6\" class=\"data row20 col6\" >63.5% ± 4.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25f73_level0_row21\" class=\"row_heading level0 row21\" >gemma-1.1-7b-it</th>\n",
       "      <td id=\"T_25f73_row21_col0\" class=\"data row21 col0\" >36.8% ± 12.3%</td>\n",
       "      <td id=\"T_25f73_row21_col1\" class=\"data row21 col1\" >76.7% ± 8.3%</td>\n",
       "      <td id=\"T_25f73_row21_col2\" class=\"data row21 col2\" >72.3% ± 7.6%</td>\n",
       "      <td id=\"T_25f73_row21_col3\" class=\"data row21 col3\" >72.7% ± 7.5%</td>\n",
       "      <td id=\"T_25f73_row21_col4\" class=\"data row21 col4\" >47.2% ± 8.1%</td>\n",
       "      <td id=\"T_25f73_row21_col5\" class=\"data row21 col5\" >66.0% ± 10.1%</td>\n",
       "      <td id=\"T_25f73_row21_col6\" class=\"data row21 col6\" >73.9% ± 4.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25f73_level0_row22\" class=\"row_heading level0 row22\" >Phi-3-mini-4k-instruct</th>\n",
       "      <td id=\"T_25f73_row22_col0\" class=\"data row22 col0\" >25.3% ± 3.1%</td>\n",
       "      <td id=\"T_25f73_row22_col1\" class=\"data row22 col1\" >57.8% ± 7.0%</td>\n",
       "      <td id=\"T_25f73_row22_col2\" class=\"data row22 col2\" >71.1% ± 1.2%</td>\n",
       "      <td id=\"T_25f73_row22_col3\" class=\"data row22 col3\" >78.0% ± 2.3%</td>\n",
       "      <td id=\"T_25f73_row22_col4\" class=\"data row22 col4\" >45.2% ± 5.1%</td>\n",
       "      <td id=\"T_25f73_row22_col5\" class=\"data row22 col5\" >45.8% ± 6.4%</td>\n",
       "      <td id=\"T_25f73_row22_col6\" class=\"data row22 col6\" >66.4% ± 3.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_25f73_level0_row23\" class=\"row_heading level0 row23\" >gemma-1.1-2b-it</th>\n",
       "      <td id=\"T_25f73_row23_col0\" class=\"data row23 col0\" >0.7% ± 0.6%</td>\n",
       "      <td id=\"T_25f73_row23_col1\" class=\"data row23 col1\" >4.6% ± 1.0%</td>\n",
       "      <td id=\"T_25f73_row23_col2\" class=\"data row23 col2\" >63.1% ± 3.8%</td>\n",
       "      <td id=\"T_25f73_row23_col3\" class=\"data row23 col3\" >67.0% ± 4.0%</td>\n",
       "      <td id=\"T_25f73_row23_col4\" class=\"data row23 col4\" >3.3% ± 0.6%</td>\n",
       "      <td id=\"T_25f73_row23_col5\" class=\"data row23 col5\" >4.3% ± 0.7%</td>\n",
       "      <td id=\"T_25f73_row23_col6\" class=\"data row23 col6\" >70.9% ± 2.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fe801a14e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### sorted by: correctness/answer_entail"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_9763f_row1_col0, #T_9763f_row1_col1, #T_9763f_row1_col2, #T_9763f_row1_col3, #T_9763f_row1_col4, #T_9763f_row1_col5, #T_9763f_row1_col6 {\n",
       "  font-weight: 1000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_9763f\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_9763f_level0_col0\" class=\"col_heading level0 col0\" >citations/ais_recall</th>\n",
       "      <th id=\"T_9763f_level0_col1\" class=\"col_heading level0 col1\" >citations/ais_precision</th>\n",
       "      <th id=\"T_9763f_level0_col2\" class=\"col_heading level0 col2\" >correctness/answer_overlap</th>\n",
       "      <th id=\"T_9763f_level0_col3\" class=\"col_heading level0 col3\" >correctness/answer_entail</th>\n",
       "      <th id=\"T_9763f_level0_col4\" class=\"col_heading level0 col4\" >correctness/citations_recall</th>\n",
       "      <th id=\"T_9763f_level0_col5\" class=\"col_heading level0 col5\" >correctness/citations_precision</th>\n",
       "      <th id=\"T_9763f_level0_col6\" class=\"col_heading level0 col6\" >quality/answer_relevance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >llm</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_9763f_level0_row0\" class=\"row_heading level0 row0\" >rag-tge_Llama-3-8B_v2</th>\n",
       "      <td id=\"T_9763f_row0_col0\" class=\"data row0 col0\" >65.4% ± 12.0%</td>\n",
       "      <td id=\"T_9763f_row0_col1\" class=\"data row0 col1\" >94.4% ± 5.0%</td>\n",
       "      <td id=\"T_9763f_row0_col2\" class=\"data row0 col2\" >93.4% ± 1.5%</td>\n",
       "      <td id=\"T_9763f_row0_col3\" class=\"data row0 col3\" >93.3% ± 4.0%</td>\n",
       "      <td id=\"T_9763f_row0_col4\" class=\"data row0 col4\" >98.2% ± 0.6%</td>\n",
       "      <td id=\"T_9763f_row0_col5\" class=\"data row0 col5\" >76.7% ± 13.6%</td>\n",
       "      <td id=\"T_9763f_row0_col6\" class=\"data row0 col6\" >74.6% ± 6.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9763f_level0_row1\" class=\"row_heading level0 row1\" >rag-tge_Mistral_v6</th>\n",
       "      <td id=\"T_9763f_row1_col0\" class=\"data row1 col0\" >78.3% ± 8.2%</td>\n",
       "      <td id=\"T_9763f_row1_col1\" class=\"data row1 col1\" >94.7% ± 2.4%</td>\n",
       "      <td id=\"T_9763f_row1_col2\" class=\"data row1 col2\" >90.6% ± 3.2%</td>\n",
       "      <td id=\"T_9763f_row1_col3\" class=\"data row1 col3\" >93.0% ± 2.9%</td>\n",
       "      <td id=\"T_9763f_row1_col4\" class=\"data row1 col4\" >93.7% ± 2.3%</td>\n",
       "      <td id=\"T_9763f_row1_col5\" class=\"data row1 col5\" >96.4% ± 1.8%</td>\n",
       "      <td id=\"T_9763f_row1_col6\" class=\"data row1 col6\" >76.1% ± 4.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9763f_level0_row2\" class=\"row_heading level0 row2\" >rag-tge_Llama-3-8B_v1</th>\n",
       "      <td id=\"T_9763f_row2_col0\" class=\"data row2 col0\" >64.0% ± 10.0%</td>\n",
       "      <td id=\"T_9763f_row2_col1\" class=\"data row2 col1\" >95.4% ± 3.0%</td>\n",
       "      <td id=\"T_9763f_row2_col2\" class=\"data row2 col2\" >94.4% ± 1.4%</td>\n",
       "      <td id=\"T_9763f_row2_col3\" class=\"data row2 col3\" >92.3% ± 2.3%</td>\n",
       "      <td id=\"T_9763f_row2_col4\" class=\"data row2 col4\" >98.8% ± 0.9%</td>\n",
       "      <td id=\"T_9763f_row2_col5\" class=\"data row2 col5\" >77.6% ± 9.9%</td>\n",
       "      <td id=\"T_9763f_row2_col6\" class=\"data row2 col6\" >73.5% ± 7.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9763f_level0_row3\" class=\"row_heading level0 row3\" >gpt-3.5-turbo-0125</th>\n",
       "      <td id=\"T_9763f_row3_col0\" class=\"data row3 col0\" >73.8% ± 7.3%</td>\n",
       "      <td id=\"T_9763f_row3_col1\" class=\"data row3 col1\" >99.6% ± 0.3%</td>\n",
       "      <td id=\"T_9763f_row3_col2\" class=\"data row3 col2\" >88.4% ± 1.3%</td>\n",
       "      <td id=\"T_9763f_row3_col3\" class=\"data row3 col3\" >91.0% ± 1.2%</td>\n",
       "      <td id=\"T_9763f_row3_col4\" class=\"data row3 col4\" >85.3% ± 2.3%</td>\n",
       "      <td id=\"T_9763f_row3_col5\" class=\"data row3 col5\" >94.2% ± 2.4%</td>\n",
       "      <td id=\"T_9763f_row3_col6\" class=\"data row3 col6\" >75.2% ± 4.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9763f_level0_row4\" class=\"row_heading level0 row4\" >rag-tge_Mistral_v2-3360</th>\n",
       "      <td id=\"T_9763f_row4_col0\" class=\"data row4 col0\" >68.3% ± 14.6%</td>\n",
       "      <td id=\"T_9763f_row4_col1\" class=\"data row4 col1\" >95.8% ± 3.3%</td>\n",
       "      <td id=\"T_9763f_row4_col2\" class=\"data row4 col2\" >93.6% ± 3.1%</td>\n",
       "      <td id=\"T_9763f_row4_col3\" class=\"data row4 col3\" >91.0% ± 1.2%</td>\n",
       "      <td id=\"T_9763f_row4_col4\" class=\"data row4 col4\" >96.8% ± 2.6%</td>\n",
       "      <td id=\"T_9763f_row4_col5\" class=\"data row4 col5\" >84.2% ± 8.4%</td>\n",
       "      <td id=\"T_9763f_row4_col6\" class=\"data row4 col6\" >72.6% ± 6.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9763f_level0_row5\" class=\"row_heading level0 row5\" >Meta-Llama-3-8B-Instruct</th>\n",
       "      <td id=\"T_9763f_row5_col0\" class=\"data row5 col0\" >49.3% ± 6.1%</td>\n",
       "      <td id=\"T_9763f_row5_col1\" class=\"data row5 col1\" >95.1% ± 2.2%</td>\n",
       "      <td id=\"T_9763f_row5_col2\" class=\"data row5 col2\" >91.8% ± 0.0%</td>\n",
       "      <td id=\"T_9763f_row5_col3\" class=\"data row5 col3\" >90.3% ± 0.6%</td>\n",
       "      <td id=\"T_9763f_row5_col4\" class=\"data row5 col4\" >75.3% ± 2.3%</td>\n",
       "      <td id=\"T_9763f_row5_col5\" class=\"data row5 col5\" >90.8% ± 1.8%</td>\n",
       "      <td id=\"T_9763f_row5_col6\" class=\"data row5 col6\" >72.0% ± 3.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9763f_level0_row6\" class=\"row_heading level0 row6\" >rag-tge_Llama-3-8B_v3</th>\n",
       "      <td id=\"T_9763f_row6_col0\" class=\"data row6 col0\" >81.3% ± 7.5%</td>\n",
       "      <td id=\"T_9763f_row6_col1\" class=\"data row6 col1\" >95.6% ± 2.9%</td>\n",
       "      <td id=\"T_9763f_row6_col2\" class=\"data row6 col2\" >85.1% ± 4.5%</td>\n",
       "      <td id=\"T_9763f_row6_col3\" class=\"data row6 col3\" >90.0% ± 2.3%</td>\n",
       "      <td id=\"T_9763f_row6_col4\" class=\"data row6 col4\" >93.3% ± 2.5%</td>\n",
       "      <td id=\"T_9763f_row6_col5\" class=\"data row6 col5\" >95.0% ± 0.9%</td>\n",
       "      <td id=\"T_9763f_row6_col6\" class=\"data row6 col6\" >76.1% ± 3.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9763f_level0_row7\" class=\"row_heading level0 row7\" >gpt-4-turbo</th>\n",
       "      <td id=\"T_9763f_row7_col0\" class=\"data row7 col0\" >73.3% ± 4.5%</td>\n",
       "      <td id=\"T_9763f_row7_col1\" class=\"data row7 col1\" >98.5% ± 0.6%</td>\n",
       "      <td id=\"T_9763f_row7_col2\" class=\"data row7 col2\" >87.7% ± 0.8%</td>\n",
       "      <td id=\"T_9763f_row7_col3\" class=\"data row7 col3\" >90.0% ± 1.2%</td>\n",
       "      <td id=\"T_9763f_row7_col4\" class=\"data row7 col4\" >79.5% ± 2.9%</td>\n",
       "      <td id=\"T_9763f_row7_col5\" class=\"data row7 col5\" >93.9% ± 1.1%</td>\n",
       "      <td id=\"T_9763f_row7_col6\" class=\"data row7 col6\" >74.7% ± 3.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9763f_level0_row8\" class=\"row_heading level0 row8\" >rag-tge_Mistral_v2-4480</th>\n",
       "      <td id=\"T_9763f_row8_col0\" class=\"data row8 col0\" >73.3% ± 13.5%</td>\n",
       "      <td id=\"T_9763f_row8_col1\" class=\"data row8 col1\" >95.4% ± 3.1%</td>\n",
       "      <td id=\"T_9763f_row8_col2\" class=\"data row8 col2\" >91.5% ± 1.8%</td>\n",
       "      <td id=\"T_9763f_row8_col3\" class=\"data row8 col3\" >89.7% ± 2.3%</td>\n",
       "      <td id=\"T_9763f_row8_col4\" class=\"data row8 col4\" >97.2% ± 2.6%</td>\n",
       "      <td id=\"T_9763f_row8_col5\" class=\"data row8 col5\" >87.7% ± 7.0%</td>\n",
       "      <td id=\"T_9763f_row8_col6\" class=\"data row8 col6\" >72.9% ± 5.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9763f_level0_row9\" class=\"row_heading level0 row9\" >Mixtral-8x7B-Instruct-v0.1</th>\n",
       "      <td id=\"T_9763f_row9_col0\" class=\"data row9 col0\" >65.4% ± 6.1%</td>\n",
       "      <td id=\"T_9763f_row9_col1\" class=\"data row9 col1\" >92.9% ± 3.0%</td>\n",
       "      <td id=\"T_9763f_row9_col2\" class=\"data row9 col2\" >91.0% ± 0.1%</td>\n",
       "      <td id=\"T_9763f_row9_col3\" class=\"data row9 col3\" >89.7% ± 1.2%</td>\n",
       "      <td id=\"T_9763f_row9_col4\" class=\"data row9 col4\" >82.5% ± 3.5%</td>\n",
       "      <td id=\"T_9763f_row9_col5\" class=\"data row9 col5\" >87.6% ± 3.8%</td>\n",
       "      <td id=\"T_9763f_row9_col6\" class=\"data row9 col6\" >72.6% ± 3.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9763f_level0_row10\" class=\"row_heading level0 row10\" >qwen1_5-32b-chat-q8_0</th>\n",
       "      <td id=\"T_9763f_row10_col0\" class=\"data row10 col0\" >51.2% ± 3.2%</td>\n",
       "      <td id=\"T_9763f_row10_col1\" class=\"data row10 col1\" >99.3% ± 0.3%</td>\n",
       "      <td id=\"T_9763f_row10_col2\" class=\"data row10 col2\" >87.4% ± 0.6%</td>\n",
       "      <td id=\"T_9763f_row10_col3\" class=\"data row10 col3\" >89.7% ± 0.6%</td>\n",
       "      <td id=\"T_9763f_row10_col4\" class=\"data row10 col4\" >65.5% ± 0.9%</td>\n",
       "      <td id=\"T_9763f_row10_col5\" class=\"data row10 col5\" >96.2% ± 0.0%</td>\n",
       "      <td id=\"T_9763f_row10_col6\" class=\"data row10 col6\" >71.0% ± 2.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9763f_level0_row11\" class=\"row_heading level0 row11\" >qwen1_5-110b-chat</th>\n",
       "      <td id=\"T_9763f_row11_col0\" class=\"data row11 col0\" >52.8% ± 9.5%</td>\n",
       "      <td id=\"T_9763f_row11_col1\" class=\"data row11 col1\" >97.6% ± 2.7%</td>\n",
       "      <td id=\"T_9763f_row11_col2\" class=\"data row11 col2\" >86.7% ± 2.0%</td>\n",
       "      <td id=\"T_9763f_row11_col3\" class=\"data row11 col3\" >89.3% ± 1.7%</td>\n",
       "      <td id=\"T_9763f_row11_col4\" class=\"data row11 col4\" >73.5% ± 5.2%</td>\n",
       "      <td id=\"T_9763f_row11_col5\" class=\"data row11 col5\" >93.0% ± 2.3%</td>\n",
       "      <td id=\"T_9763f_row11_col6\" class=\"data row11 col6\" >73.7% ± 5.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9763f_level0_row12\" class=\"row_heading level0 row12\" >Mistral-7B-Instruct-v0.2</th>\n",
       "      <td id=\"T_9763f_row12_col0\" class=\"data row12 col0\" >57.6% ± 3.7%</td>\n",
       "      <td id=\"T_9763f_row12_col1\" class=\"data row12 col1\" >87.2% ± 3.8%</td>\n",
       "      <td id=\"T_9763f_row12_col2\" class=\"data row12 col2\" >87.3% ± 1.5%</td>\n",
       "      <td id=\"T_9763f_row12_col3\" class=\"data row12 col3\" >88.7% ± 1.7%</td>\n",
       "      <td id=\"T_9763f_row12_col4\" class=\"data row12 col4\" >75.2% ± 2.6%</td>\n",
       "      <td id=\"T_9763f_row12_col5\" class=\"data row12 col5\" >72.2% ± 5.0%</td>\n",
       "      <td id=\"T_9763f_row12_col6\" class=\"data row12 col6\" >72.0% ± 2.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9763f_level0_row13\" class=\"row_heading level0 row13\" >c4ai-command-r-plus</th>\n",
       "      <td id=\"T_9763f_row13_col0\" class=\"data row13 col0\" >40.9% ± 6.0%</td>\n",
       "      <td id=\"T_9763f_row13_col1\" class=\"data row13 col1\" >71.5% ± 6.1%</td>\n",
       "      <td id=\"T_9763f_row13_col2\" class=\"data row13 col2\" >73.6% ± 2.3%</td>\n",
       "      <td id=\"T_9763f_row13_col3\" class=\"data row13 col3\" >88.0% ± 0.0%</td>\n",
       "      <td id=\"T_9763f_row13_col4\" class=\"data row13 col4\" >57.8% ± 4.4%</td>\n",
       "      <td id=\"T_9763f_row13_col5\" class=\"data row13 col5\" >64.4% ± 5.5%</td>\n",
       "      <td id=\"T_9763f_row13_col6\" class=\"data row13 col6\" >63.5% ± 4.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9763f_level0_row14\" class=\"row_heading level0 row14\" >Meta-Llama-3-70B-Instruct</th>\n",
       "      <td id=\"T_9763f_row14_col0\" class=\"data row14 col0\" >69.2% ± 5.0%</td>\n",
       "      <td id=\"T_9763f_row14_col1\" class=\"data row14 col1\" >95.6% ± 1.8%</td>\n",
       "      <td id=\"T_9763f_row14_col2\" class=\"data row14 col2\" >86.6% ± 2.1%</td>\n",
       "      <td id=\"T_9763f_row14_col3\" class=\"data row14 col3\" >87.3% ± 1.7%</td>\n",
       "      <td id=\"T_9763f_row14_col4\" class=\"data row14 col4\" >78.5% ± 1.7%</td>\n",
       "      <td id=\"T_9763f_row14_col5\" class=\"data row14 col5\" >90.8% ± 1.0%</td>\n",
       "      <td id=\"T_9763f_row14_col6\" class=\"data row14 col6\" >75.3% ± 2.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9763f_level0_row15\" class=\"row_heading level0 row15\" >zephyr-7b-beta</th>\n",
       "      <td id=\"T_9763f_row15_col0\" class=\"data row15 col0\" >40.7% ± 9.1%</td>\n",
       "      <td id=\"T_9763f_row15_col1\" class=\"data row15 col1\" >91.1% ± 6.9%</td>\n",
       "      <td id=\"T_9763f_row15_col2\" class=\"data row15 col2\" >85.4% ± 2.9%</td>\n",
       "      <td id=\"T_9763f_row15_col3\" class=\"data row15 col3\" >86.7% ± 4.6%</td>\n",
       "      <td id=\"T_9763f_row15_col4\" class=\"data row15 col4\" >62.7% ± 8.0%</td>\n",
       "      <td id=\"T_9763f_row15_col5\" class=\"data row15 col5\" >72.8% ± 8.8%</td>\n",
       "      <td id=\"T_9763f_row15_col6\" class=\"data row15 col6\" >71.5% ± 4.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9763f_level0_row16\" class=\"row_heading level0 row16\" >rag-tge_Mistral.Q8</th>\n",
       "      <td id=\"T_9763f_row16_col0\" class=\"data row16 col0\" >81.2% ± 6.1%</td>\n",
       "      <td id=\"T_9763f_row16_col1\" class=\"data row16 col1\" >95.8% ± 2.1%</td>\n",
       "      <td id=\"T_9763f_row16_col2\" class=\"data row16 col2\" >86.1% ± 1.3%</td>\n",
       "      <td id=\"T_9763f_row16_col3\" class=\"data row16 col3\" >86.7% ± 0.6%</td>\n",
       "      <td id=\"T_9763f_row16_col4\" class=\"data row16 col4\" >91.5% ± 2.3%</td>\n",
       "      <td id=\"T_9763f_row16_col5\" class=\"data row16 col5\" >90.7% ± 4.1%</td>\n",
       "      <td id=\"T_9763f_row16_col6\" class=\"data row16 col6\" >74.2% ± 4.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9763f_level0_row17\" class=\"row_heading level0 row17\" >qwen1_5-14b-chat-q8_0</th>\n",
       "      <td id=\"T_9763f_row17_col0\" class=\"data row17 col0\" >63.3% ± 4.2%</td>\n",
       "      <td id=\"T_9763f_row17_col1\" class=\"data row17 col1\" >97.5% ± 0.9%</td>\n",
       "      <td id=\"T_9763f_row17_col2\" class=\"data row17 col2\" >87.6% ± 0.7%</td>\n",
       "      <td id=\"T_9763f_row17_col3\" class=\"data row17 col3\" >86.3% ± 0.6%</td>\n",
       "      <td id=\"T_9763f_row17_col4\" class=\"data row17 col4\" >66.0% ± 3.1%</td>\n",
       "      <td id=\"T_9763f_row17_col5\" class=\"data row17 col5\" >92.0% ± 1.6%</td>\n",
       "      <td id=\"T_9763f_row17_col6\" class=\"data row17 col6\" >71.9% ± 3.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9763f_level0_row18\" class=\"row_heading level0 row18\" >zephyr-orpo-141b-A35b-v0.1</th>\n",
       "      <td id=\"T_9763f_row18_col0\" class=\"data row18 col0\" >41.8% ± 13.4%</td>\n",
       "      <td id=\"T_9763f_row18_col1\" class=\"data row18 col1\" >79.6% ± 10.1%</td>\n",
       "      <td id=\"T_9763f_row18_col2\" class=\"data row18 col2\" >80.7% ± 6.0%</td>\n",
       "      <td id=\"T_9763f_row18_col3\" class=\"data row18 col3\" >86.0% ± 4.0%</td>\n",
       "      <td id=\"T_9763f_row18_col4\" class=\"data row18 col4\" >77.5% ± 9.2%</td>\n",
       "      <td id=\"T_9763f_row18_col5\" class=\"data row18 col5\" >54.6% ± 10.8%</td>\n",
       "      <td id=\"T_9763f_row18_col6\" class=\"data row18 col6\" >69.6% ± 5.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9763f_level0_row19\" class=\"row_heading level0 row19\" >qwen1_5-7b-chat-q8_0</th>\n",
       "      <td id=\"T_9763f_row19_col0\" class=\"data row19 col0\" >23.3% ± 6.4%</td>\n",
       "      <td id=\"T_9763f_row19_col1\" class=\"data row19 col1\" >86.2% ± 5.7%</td>\n",
       "      <td id=\"T_9763f_row19_col2\" class=\"data row19 col2\" >86.5% ± 2.4%</td>\n",
       "      <td id=\"T_9763f_row19_col3\" class=\"data row19 col3\" >84.0% ± 2.3%</td>\n",
       "      <td id=\"T_9763f_row19_col4\" class=\"data row19 col4\" >54.8% ± 7.4%</td>\n",
       "      <td id=\"T_9763f_row19_col5\" class=\"data row19 col5\" >80.0% ± 7.8%</td>\n",
       "      <td id=\"T_9763f_row19_col6\" class=\"data row19 col6\" >76.8% ± 3.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9763f_level0_row20\" class=\"row_heading level0 row20\" >Mistral-7B-Instruct-v0.1</th>\n",
       "      <td id=\"T_9763f_row20_col0\" class=\"data row20 col0\" >0.5% ± 0.0%</td>\n",
       "      <td id=\"T_9763f_row20_col1\" class=\"data row20 col1\" >1.0% ± 0.0%</td>\n",
       "      <td id=\"T_9763f_row20_col2\" class=\"data row20 col2\" >79.0% ± 1.4%</td>\n",
       "      <td id=\"T_9763f_row20_col3\" class=\"data row20 col3\" >82.7% ± 1.7%</td>\n",
       "      <td id=\"T_9763f_row20_col4\" class=\"data row20 col4\" >1.0% ± 0.0%</td>\n",
       "      <td id=\"T_9763f_row20_col5\" class=\"data row20 col5\" >0.7% ± 0.0%</td>\n",
       "      <td id=\"T_9763f_row20_col6\" class=\"data row20 col6\" >73.8% ± 2.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9763f_level0_row21\" class=\"row_heading level0 row21\" >Phi-3-mini-4k-instruct</th>\n",
       "      <td id=\"T_9763f_row21_col0\" class=\"data row21 col0\" >25.3% ± 3.1%</td>\n",
       "      <td id=\"T_9763f_row21_col1\" class=\"data row21 col1\" >57.8% ± 7.0%</td>\n",
       "      <td id=\"T_9763f_row21_col2\" class=\"data row21 col2\" >71.1% ± 1.2%</td>\n",
       "      <td id=\"T_9763f_row21_col3\" class=\"data row21 col3\" >78.0% ± 2.3%</td>\n",
       "      <td id=\"T_9763f_row21_col4\" class=\"data row21 col4\" >45.2% ± 5.1%</td>\n",
       "      <td id=\"T_9763f_row21_col5\" class=\"data row21 col5\" >45.8% ± 6.4%</td>\n",
       "      <td id=\"T_9763f_row21_col6\" class=\"data row21 col6\" >66.4% ± 3.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9763f_level0_row22\" class=\"row_heading level0 row22\" >gemma-1.1-7b-it</th>\n",
       "      <td id=\"T_9763f_row22_col0\" class=\"data row22 col0\" >36.8% ± 12.3%</td>\n",
       "      <td id=\"T_9763f_row22_col1\" class=\"data row22 col1\" >76.7% ± 8.3%</td>\n",
       "      <td id=\"T_9763f_row22_col2\" class=\"data row22 col2\" >72.3% ± 7.6%</td>\n",
       "      <td id=\"T_9763f_row22_col3\" class=\"data row22 col3\" >72.7% ± 7.5%</td>\n",
       "      <td id=\"T_9763f_row22_col4\" class=\"data row22 col4\" >47.2% ± 8.1%</td>\n",
       "      <td id=\"T_9763f_row22_col5\" class=\"data row22 col5\" >66.0% ± 10.1%</td>\n",
       "      <td id=\"T_9763f_row22_col6\" class=\"data row22 col6\" >73.9% ± 4.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_9763f_level0_row23\" class=\"row_heading level0 row23\" >gemma-1.1-2b-it</th>\n",
       "      <td id=\"T_9763f_row23_col0\" class=\"data row23 col0\" >0.7% ± 0.6%</td>\n",
       "      <td id=\"T_9763f_row23_col1\" class=\"data row23 col1\" >4.6% ± 1.0%</td>\n",
       "      <td id=\"T_9763f_row23_col2\" class=\"data row23 col2\" >63.1% ± 3.8%</td>\n",
       "      <td id=\"T_9763f_row23_col3\" class=\"data row23 col3\" >67.0% ± 4.0%</td>\n",
       "      <td id=\"T_9763f_row23_col4\" class=\"data row23 col4\" >3.3% ± 0.6%</td>\n",
       "      <td id=\"T_9763f_row23_col5\" class=\"data row23 col5\" >4.3% ± 0.7%</td>\n",
       "      <td id=\"T_9763f_row23_col6\" class=\"data row23 col6\" >70.9% ± 2.0%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fe801a14e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### sorted by: correctness/citations_recall"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_d5a15_row4_col0, #T_d5a15_row4_col1, #T_d5a15_row4_col2, #T_d5a15_row4_col3, #T_d5a15_row4_col4, #T_d5a15_row4_col5, #T_d5a15_row4_col6 {\n",
       "  font-weight: 1000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_d5a15\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_d5a15_level0_col0\" class=\"col_heading level0 col0\" >citations/ais_recall</th>\n",
       "      <th id=\"T_d5a15_level0_col1\" class=\"col_heading level0 col1\" >citations/ais_precision</th>\n",
       "      <th id=\"T_d5a15_level0_col2\" class=\"col_heading level0 col2\" >correctness/answer_overlap</th>\n",
       "      <th id=\"T_d5a15_level0_col3\" class=\"col_heading level0 col3\" >correctness/answer_entail</th>\n",
       "      <th id=\"T_d5a15_level0_col4\" class=\"col_heading level0 col4\" >correctness/citations_recall</th>\n",
       "      <th id=\"T_d5a15_level0_col5\" class=\"col_heading level0 col5\" >correctness/citations_precision</th>\n",
       "      <th id=\"T_d5a15_level0_col6\" class=\"col_heading level0 col6\" >quality/answer_relevance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >llm</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_d5a15_level0_row0\" class=\"row_heading level0 row0\" >rag-tge_Llama-3-8B_v1</th>\n",
       "      <td id=\"T_d5a15_row0_col0\" class=\"data row0 col0\" >64.0% ± 10.0%</td>\n",
       "      <td id=\"T_d5a15_row0_col1\" class=\"data row0 col1\" >95.4% ± 3.0%</td>\n",
       "      <td id=\"T_d5a15_row0_col2\" class=\"data row0 col2\" >94.4% ± 1.4%</td>\n",
       "      <td id=\"T_d5a15_row0_col3\" class=\"data row0 col3\" >92.3% ± 2.3%</td>\n",
       "      <td id=\"T_d5a15_row0_col4\" class=\"data row0 col4\" >98.8% ± 0.9%</td>\n",
       "      <td id=\"T_d5a15_row0_col5\" class=\"data row0 col5\" >77.6% ± 9.9%</td>\n",
       "      <td id=\"T_d5a15_row0_col6\" class=\"data row0 col6\" >73.5% ± 7.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d5a15_level0_row1\" class=\"row_heading level0 row1\" >rag-tge_Llama-3-8B_v2</th>\n",
       "      <td id=\"T_d5a15_row1_col0\" class=\"data row1 col0\" >65.4% ± 12.0%</td>\n",
       "      <td id=\"T_d5a15_row1_col1\" class=\"data row1 col1\" >94.4% ± 5.0%</td>\n",
       "      <td id=\"T_d5a15_row1_col2\" class=\"data row1 col2\" >93.4% ± 1.5%</td>\n",
       "      <td id=\"T_d5a15_row1_col3\" class=\"data row1 col3\" >93.3% ± 4.0%</td>\n",
       "      <td id=\"T_d5a15_row1_col4\" class=\"data row1 col4\" >98.2% ± 0.6%</td>\n",
       "      <td id=\"T_d5a15_row1_col5\" class=\"data row1 col5\" >76.7% ± 13.6%</td>\n",
       "      <td id=\"T_d5a15_row1_col6\" class=\"data row1 col6\" >74.6% ± 6.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d5a15_level0_row2\" class=\"row_heading level0 row2\" >rag-tge_Mistral_v2-4480</th>\n",
       "      <td id=\"T_d5a15_row2_col0\" class=\"data row2 col0\" >73.3% ± 13.5%</td>\n",
       "      <td id=\"T_d5a15_row2_col1\" class=\"data row2 col1\" >95.4% ± 3.1%</td>\n",
       "      <td id=\"T_d5a15_row2_col2\" class=\"data row2 col2\" >91.5% ± 1.8%</td>\n",
       "      <td id=\"T_d5a15_row2_col3\" class=\"data row2 col3\" >89.7% ± 2.3%</td>\n",
       "      <td id=\"T_d5a15_row2_col4\" class=\"data row2 col4\" >97.2% ± 2.6%</td>\n",
       "      <td id=\"T_d5a15_row2_col5\" class=\"data row2 col5\" >87.7% ± 7.0%</td>\n",
       "      <td id=\"T_d5a15_row2_col6\" class=\"data row2 col6\" >72.9% ± 5.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d5a15_level0_row3\" class=\"row_heading level0 row3\" >rag-tge_Mistral_v2-3360</th>\n",
       "      <td id=\"T_d5a15_row3_col0\" class=\"data row3 col0\" >68.3% ± 14.6%</td>\n",
       "      <td id=\"T_d5a15_row3_col1\" class=\"data row3 col1\" >95.8% ± 3.3%</td>\n",
       "      <td id=\"T_d5a15_row3_col2\" class=\"data row3 col2\" >93.6% ± 3.1%</td>\n",
       "      <td id=\"T_d5a15_row3_col3\" class=\"data row3 col3\" >91.0% ± 1.2%</td>\n",
       "      <td id=\"T_d5a15_row3_col4\" class=\"data row3 col4\" >96.8% ± 2.6%</td>\n",
       "      <td id=\"T_d5a15_row3_col5\" class=\"data row3 col5\" >84.2% ± 8.4%</td>\n",
       "      <td id=\"T_d5a15_row3_col6\" class=\"data row3 col6\" >72.6% ± 6.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d5a15_level0_row4\" class=\"row_heading level0 row4\" >rag-tge_Mistral_v6</th>\n",
       "      <td id=\"T_d5a15_row4_col0\" class=\"data row4 col0\" >78.3% ± 8.2%</td>\n",
       "      <td id=\"T_d5a15_row4_col1\" class=\"data row4 col1\" >94.7% ± 2.4%</td>\n",
       "      <td id=\"T_d5a15_row4_col2\" class=\"data row4 col2\" >90.6% ± 3.2%</td>\n",
       "      <td id=\"T_d5a15_row4_col3\" class=\"data row4 col3\" >93.0% ± 2.9%</td>\n",
       "      <td id=\"T_d5a15_row4_col4\" class=\"data row4 col4\" >93.7% ± 2.3%</td>\n",
       "      <td id=\"T_d5a15_row4_col5\" class=\"data row4 col5\" >96.4% ± 1.8%</td>\n",
       "      <td id=\"T_d5a15_row4_col6\" class=\"data row4 col6\" >76.1% ± 4.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d5a15_level0_row5\" class=\"row_heading level0 row5\" >rag-tge_Llama-3-8B_v3</th>\n",
       "      <td id=\"T_d5a15_row5_col0\" class=\"data row5 col0\" >81.3% ± 7.5%</td>\n",
       "      <td id=\"T_d5a15_row5_col1\" class=\"data row5 col1\" >95.6% ± 2.9%</td>\n",
       "      <td id=\"T_d5a15_row5_col2\" class=\"data row5 col2\" >85.1% ± 4.5%</td>\n",
       "      <td id=\"T_d5a15_row5_col3\" class=\"data row5 col3\" >90.0% ± 2.3%</td>\n",
       "      <td id=\"T_d5a15_row5_col4\" class=\"data row5 col4\" >93.3% ± 2.5%</td>\n",
       "      <td id=\"T_d5a15_row5_col5\" class=\"data row5 col5\" >95.0% ± 0.9%</td>\n",
       "      <td id=\"T_d5a15_row5_col6\" class=\"data row5 col6\" >76.1% ± 3.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d5a15_level0_row6\" class=\"row_heading level0 row6\" >rag-tge_Mistral.Q8</th>\n",
       "      <td id=\"T_d5a15_row6_col0\" class=\"data row6 col0\" >81.2% ± 6.1%</td>\n",
       "      <td id=\"T_d5a15_row6_col1\" class=\"data row6 col1\" >95.8% ± 2.1%</td>\n",
       "      <td id=\"T_d5a15_row6_col2\" class=\"data row6 col2\" >86.1% ± 1.3%</td>\n",
       "      <td id=\"T_d5a15_row6_col3\" class=\"data row6 col3\" >86.7% ± 0.6%</td>\n",
       "      <td id=\"T_d5a15_row6_col4\" class=\"data row6 col4\" >91.5% ± 2.3%</td>\n",
       "      <td id=\"T_d5a15_row6_col5\" class=\"data row6 col5\" >90.7% ± 4.1%</td>\n",
       "      <td id=\"T_d5a15_row6_col6\" class=\"data row6 col6\" >74.2% ± 4.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d5a15_level0_row7\" class=\"row_heading level0 row7\" >gpt-3.5-turbo-0125</th>\n",
       "      <td id=\"T_d5a15_row7_col0\" class=\"data row7 col0\" >73.8% ± 7.3%</td>\n",
       "      <td id=\"T_d5a15_row7_col1\" class=\"data row7 col1\" >99.6% ± 0.3%</td>\n",
       "      <td id=\"T_d5a15_row7_col2\" class=\"data row7 col2\" >88.4% ± 1.3%</td>\n",
       "      <td id=\"T_d5a15_row7_col3\" class=\"data row7 col3\" >91.0% ± 1.2%</td>\n",
       "      <td id=\"T_d5a15_row7_col4\" class=\"data row7 col4\" >85.3% ± 2.3%</td>\n",
       "      <td id=\"T_d5a15_row7_col5\" class=\"data row7 col5\" >94.2% ± 2.4%</td>\n",
       "      <td id=\"T_d5a15_row7_col6\" class=\"data row7 col6\" >75.2% ± 4.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d5a15_level0_row8\" class=\"row_heading level0 row8\" >Mixtral-8x7B-Instruct-v0.1</th>\n",
       "      <td id=\"T_d5a15_row8_col0\" class=\"data row8 col0\" >65.4% ± 6.1%</td>\n",
       "      <td id=\"T_d5a15_row8_col1\" class=\"data row8 col1\" >92.9% ± 3.0%</td>\n",
       "      <td id=\"T_d5a15_row8_col2\" class=\"data row8 col2\" >91.0% ± 0.1%</td>\n",
       "      <td id=\"T_d5a15_row8_col3\" class=\"data row8 col3\" >89.7% ± 1.2%</td>\n",
       "      <td id=\"T_d5a15_row8_col4\" class=\"data row8 col4\" >82.5% ± 3.5%</td>\n",
       "      <td id=\"T_d5a15_row8_col5\" class=\"data row8 col5\" >87.6% ± 3.8%</td>\n",
       "      <td id=\"T_d5a15_row8_col6\" class=\"data row8 col6\" >72.6% ± 3.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d5a15_level0_row9\" class=\"row_heading level0 row9\" >gpt-4-turbo</th>\n",
       "      <td id=\"T_d5a15_row9_col0\" class=\"data row9 col0\" >73.3% ± 4.5%</td>\n",
       "      <td id=\"T_d5a15_row9_col1\" class=\"data row9 col1\" >98.5% ± 0.6%</td>\n",
       "      <td id=\"T_d5a15_row9_col2\" class=\"data row9 col2\" >87.7% ± 0.8%</td>\n",
       "      <td id=\"T_d5a15_row9_col3\" class=\"data row9 col3\" >90.0% ± 1.2%</td>\n",
       "      <td id=\"T_d5a15_row9_col4\" class=\"data row9 col4\" >79.5% ± 2.9%</td>\n",
       "      <td id=\"T_d5a15_row9_col5\" class=\"data row9 col5\" >93.9% ± 1.1%</td>\n",
       "      <td id=\"T_d5a15_row9_col6\" class=\"data row9 col6\" >74.7% ± 3.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d5a15_level0_row10\" class=\"row_heading level0 row10\" >Meta-Llama-3-70B-Instruct</th>\n",
       "      <td id=\"T_d5a15_row10_col0\" class=\"data row10 col0\" >69.2% ± 5.0%</td>\n",
       "      <td id=\"T_d5a15_row10_col1\" class=\"data row10 col1\" >95.6% ± 1.8%</td>\n",
       "      <td id=\"T_d5a15_row10_col2\" class=\"data row10 col2\" >86.6% ± 2.1%</td>\n",
       "      <td id=\"T_d5a15_row10_col3\" class=\"data row10 col3\" >87.3% ± 1.7%</td>\n",
       "      <td id=\"T_d5a15_row10_col4\" class=\"data row10 col4\" >78.5% ± 1.7%</td>\n",
       "      <td id=\"T_d5a15_row10_col5\" class=\"data row10 col5\" >90.8% ± 1.0%</td>\n",
       "      <td id=\"T_d5a15_row10_col6\" class=\"data row10 col6\" >75.3% ± 2.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d5a15_level0_row11\" class=\"row_heading level0 row11\" >zephyr-orpo-141b-A35b-v0.1</th>\n",
       "      <td id=\"T_d5a15_row11_col0\" class=\"data row11 col0\" >41.8% ± 13.4%</td>\n",
       "      <td id=\"T_d5a15_row11_col1\" class=\"data row11 col1\" >79.6% ± 10.1%</td>\n",
       "      <td id=\"T_d5a15_row11_col2\" class=\"data row11 col2\" >80.7% ± 6.0%</td>\n",
       "      <td id=\"T_d5a15_row11_col3\" class=\"data row11 col3\" >86.0% ± 4.0%</td>\n",
       "      <td id=\"T_d5a15_row11_col4\" class=\"data row11 col4\" >77.5% ± 9.2%</td>\n",
       "      <td id=\"T_d5a15_row11_col5\" class=\"data row11 col5\" >54.6% ± 10.8%</td>\n",
       "      <td id=\"T_d5a15_row11_col6\" class=\"data row11 col6\" >69.6% ± 5.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d5a15_level0_row12\" class=\"row_heading level0 row12\" >Meta-Llama-3-8B-Instruct</th>\n",
       "      <td id=\"T_d5a15_row12_col0\" class=\"data row12 col0\" >49.3% ± 6.1%</td>\n",
       "      <td id=\"T_d5a15_row12_col1\" class=\"data row12 col1\" >95.1% ± 2.2%</td>\n",
       "      <td id=\"T_d5a15_row12_col2\" class=\"data row12 col2\" >91.8% ± 0.0%</td>\n",
       "      <td id=\"T_d5a15_row12_col3\" class=\"data row12 col3\" >90.3% ± 0.6%</td>\n",
       "      <td id=\"T_d5a15_row12_col4\" class=\"data row12 col4\" >75.3% ± 2.3%</td>\n",
       "      <td id=\"T_d5a15_row12_col5\" class=\"data row12 col5\" >90.8% ± 1.8%</td>\n",
       "      <td id=\"T_d5a15_row12_col6\" class=\"data row12 col6\" >72.0% ± 3.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d5a15_level0_row13\" class=\"row_heading level0 row13\" >Mistral-7B-Instruct-v0.2</th>\n",
       "      <td id=\"T_d5a15_row13_col0\" class=\"data row13 col0\" >57.6% ± 3.7%</td>\n",
       "      <td id=\"T_d5a15_row13_col1\" class=\"data row13 col1\" >87.2% ± 3.8%</td>\n",
       "      <td id=\"T_d5a15_row13_col2\" class=\"data row13 col2\" >87.3% ± 1.5%</td>\n",
       "      <td id=\"T_d5a15_row13_col3\" class=\"data row13 col3\" >88.7% ± 1.7%</td>\n",
       "      <td id=\"T_d5a15_row13_col4\" class=\"data row13 col4\" >75.2% ± 2.6%</td>\n",
       "      <td id=\"T_d5a15_row13_col5\" class=\"data row13 col5\" >72.2% ± 5.0%</td>\n",
       "      <td id=\"T_d5a15_row13_col6\" class=\"data row13 col6\" >72.0% ± 2.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d5a15_level0_row14\" class=\"row_heading level0 row14\" >qwen1_5-110b-chat</th>\n",
       "      <td id=\"T_d5a15_row14_col0\" class=\"data row14 col0\" >52.8% ± 9.5%</td>\n",
       "      <td id=\"T_d5a15_row14_col1\" class=\"data row14 col1\" >97.6% ± 2.7%</td>\n",
       "      <td id=\"T_d5a15_row14_col2\" class=\"data row14 col2\" >86.7% ± 2.0%</td>\n",
       "      <td id=\"T_d5a15_row14_col3\" class=\"data row14 col3\" >89.3% ± 1.7%</td>\n",
       "      <td id=\"T_d5a15_row14_col4\" class=\"data row14 col4\" >73.5% ± 5.2%</td>\n",
       "      <td id=\"T_d5a15_row14_col5\" class=\"data row14 col5\" >93.0% ± 2.3%</td>\n",
       "      <td id=\"T_d5a15_row14_col6\" class=\"data row14 col6\" >73.7% ± 5.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d5a15_level0_row15\" class=\"row_heading level0 row15\" >qwen1_5-14b-chat-q8_0</th>\n",
       "      <td id=\"T_d5a15_row15_col0\" class=\"data row15 col0\" >63.3% ± 4.2%</td>\n",
       "      <td id=\"T_d5a15_row15_col1\" class=\"data row15 col1\" >97.5% ± 0.9%</td>\n",
       "      <td id=\"T_d5a15_row15_col2\" class=\"data row15 col2\" >87.6% ± 0.7%</td>\n",
       "      <td id=\"T_d5a15_row15_col3\" class=\"data row15 col3\" >86.3% ± 0.6%</td>\n",
       "      <td id=\"T_d5a15_row15_col4\" class=\"data row15 col4\" >66.0% ± 3.1%</td>\n",
       "      <td id=\"T_d5a15_row15_col5\" class=\"data row15 col5\" >92.0% ± 1.6%</td>\n",
       "      <td id=\"T_d5a15_row15_col6\" class=\"data row15 col6\" >71.9% ± 3.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d5a15_level0_row16\" class=\"row_heading level0 row16\" >qwen1_5-32b-chat-q8_0</th>\n",
       "      <td id=\"T_d5a15_row16_col0\" class=\"data row16 col0\" >51.2% ± 3.2%</td>\n",
       "      <td id=\"T_d5a15_row16_col1\" class=\"data row16 col1\" >99.3% ± 0.3%</td>\n",
       "      <td id=\"T_d5a15_row16_col2\" class=\"data row16 col2\" >87.4% ± 0.6%</td>\n",
       "      <td id=\"T_d5a15_row16_col3\" class=\"data row16 col3\" >89.7% ± 0.6%</td>\n",
       "      <td id=\"T_d5a15_row16_col4\" class=\"data row16 col4\" >65.5% ± 0.9%</td>\n",
       "      <td id=\"T_d5a15_row16_col5\" class=\"data row16 col5\" >96.2% ± 0.0%</td>\n",
       "      <td id=\"T_d5a15_row16_col6\" class=\"data row16 col6\" >71.0% ± 2.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d5a15_level0_row17\" class=\"row_heading level0 row17\" >zephyr-7b-beta</th>\n",
       "      <td id=\"T_d5a15_row17_col0\" class=\"data row17 col0\" >40.7% ± 9.1%</td>\n",
       "      <td id=\"T_d5a15_row17_col1\" class=\"data row17 col1\" >91.1% ± 6.9%</td>\n",
       "      <td id=\"T_d5a15_row17_col2\" class=\"data row17 col2\" >85.4% ± 2.9%</td>\n",
       "      <td id=\"T_d5a15_row17_col3\" class=\"data row17 col3\" >86.7% ± 4.6%</td>\n",
       "      <td id=\"T_d5a15_row17_col4\" class=\"data row17 col4\" >62.7% ± 8.0%</td>\n",
       "      <td id=\"T_d5a15_row17_col5\" class=\"data row17 col5\" >72.8% ± 8.8%</td>\n",
       "      <td id=\"T_d5a15_row17_col6\" class=\"data row17 col6\" >71.5% ± 4.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d5a15_level0_row18\" class=\"row_heading level0 row18\" >c4ai-command-r-plus</th>\n",
       "      <td id=\"T_d5a15_row18_col0\" class=\"data row18 col0\" >40.9% ± 6.0%</td>\n",
       "      <td id=\"T_d5a15_row18_col1\" class=\"data row18 col1\" >71.5% ± 6.1%</td>\n",
       "      <td id=\"T_d5a15_row18_col2\" class=\"data row18 col2\" >73.6% ± 2.3%</td>\n",
       "      <td id=\"T_d5a15_row18_col3\" class=\"data row18 col3\" >88.0% ± 0.0%</td>\n",
       "      <td id=\"T_d5a15_row18_col4\" class=\"data row18 col4\" >57.8% ± 4.4%</td>\n",
       "      <td id=\"T_d5a15_row18_col5\" class=\"data row18 col5\" >64.4% ± 5.5%</td>\n",
       "      <td id=\"T_d5a15_row18_col6\" class=\"data row18 col6\" >63.5% ± 4.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d5a15_level0_row19\" class=\"row_heading level0 row19\" >qwen1_5-7b-chat-q8_0</th>\n",
       "      <td id=\"T_d5a15_row19_col0\" class=\"data row19 col0\" >23.3% ± 6.4%</td>\n",
       "      <td id=\"T_d5a15_row19_col1\" class=\"data row19 col1\" >86.2% ± 5.7%</td>\n",
       "      <td id=\"T_d5a15_row19_col2\" class=\"data row19 col2\" >86.5% ± 2.4%</td>\n",
       "      <td id=\"T_d5a15_row19_col3\" class=\"data row19 col3\" >84.0% ± 2.3%</td>\n",
       "      <td id=\"T_d5a15_row19_col4\" class=\"data row19 col4\" >54.8% ± 7.4%</td>\n",
       "      <td id=\"T_d5a15_row19_col5\" class=\"data row19 col5\" >80.0% ± 7.8%</td>\n",
       "      <td id=\"T_d5a15_row19_col6\" class=\"data row19 col6\" >76.8% ± 3.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d5a15_level0_row20\" class=\"row_heading level0 row20\" >gemma-1.1-7b-it</th>\n",
       "      <td id=\"T_d5a15_row20_col0\" class=\"data row20 col0\" >36.8% ± 12.3%</td>\n",
       "      <td id=\"T_d5a15_row20_col1\" class=\"data row20 col1\" >76.7% ± 8.3%</td>\n",
       "      <td id=\"T_d5a15_row20_col2\" class=\"data row20 col2\" >72.3% ± 7.6%</td>\n",
       "      <td id=\"T_d5a15_row20_col3\" class=\"data row20 col3\" >72.7% ± 7.5%</td>\n",
       "      <td id=\"T_d5a15_row20_col4\" class=\"data row20 col4\" >47.2% ± 8.1%</td>\n",
       "      <td id=\"T_d5a15_row20_col5\" class=\"data row20 col5\" >66.0% ± 10.1%</td>\n",
       "      <td id=\"T_d5a15_row20_col6\" class=\"data row20 col6\" >73.9% ± 4.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d5a15_level0_row21\" class=\"row_heading level0 row21\" >Phi-3-mini-4k-instruct</th>\n",
       "      <td id=\"T_d5a15_row21_col0\" class=\"data row21 col0\" >25.3% ± 3.1%</td>\n",
       "      <td id=\"T_d5a15_row21_col1\" class=\"data row21 col1\" >57.8% ± 7.0%</td>\n",
       "      <td id=\"T_d5a15_row21_col2\" class=\"data row21 col2\" >71.1% ± 1.2%</td>\n",
       "      <td id=\"T_d5a15_row21_col3\" class=\"data row21 col3\" >78.0% ± 2.3%</td>\n",
       "      <td id=\"T_d5a15_row21_col4\" class=\"data row21 col4\" >45.2% ± 5.1%</td>\n",
       "      <td id=\"T_d5a15_row21_col5\" class=\"data row21 col5\" >45.8% ± 6.4%</td>\n",
       "      <td id=\"T_d5a15_row21_col6\" class=\"data row21 col6\" >66.4% ± 3.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d5a15_level0_row22\" class=\"row_heading level0 row22\" >gemma-1.1-2b-it</th>\n",
       "      <td id=\"T_d5a15_row22_col0\" class=\"data row22 col0\" >0.7% ± 0.6%</td>\n",
       "      <td id=\"T_d5a15_row22_col1\" class=\"data row22 col1\" >4.6% ± 1.0%</td>\n",
       "      <td id=\"T_d5a15_row22_col2\" class=\"data row22 col2\" >63.1% ± 3.8%</td>\n",
       "      <td id=\"T_d5a15_row22_col3\" class=\"data row22 col3\" >67.0% ± 4.0%</td>\n",
       "      <td id=\"T_d5a15_row22_col4\" class=\"data row22 col4\" >3.3% ± 0.6%</td>\n",
       "      <td id=\"T_d5a15_row22_col5\" class=\"data row22 col5\" >4.3% ± 0.7%</td>\n",
       "      <td id=\"T_d5a15_row22_col6\" class=\"data row22 col6\" >70.9% ± 2.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_d5a15_level0_row23\" class=\"row_heading level0 row23\" >Mistral-7B-Instruct-v0.1</th>\n",
       "      <td id=\"T_d5a15_row23_col0\" class=\"data row23 col0\" >0.5% ± 0.0%</td>\n",
       "      <td id=\"T_d5a15_row23_col1\" class=\"data row23 col1\" >1.0% ± 0.0%</td>\n",
       "      <td id=\"T_d5a15_row23_col2\" class=\"data row23 col2\" >79.0% ± 1.4%</td>\n",
       "      <td id=\"T_d5a15_row23_col3\" class=\"data row23 col3\" >82.7% ± 1.7%</td>\n",
       "      <td id=\"T_d5a15_row23_col4\" class=\"data row23 col4\" >1.0% ± 0.0%</td>\n",
       "      <td id=\"T_d5a15_row23_col5\" class=\"data row23 col5\" >0.7% ± 0.0%</td>\n",
       "      <td id=\"T_d5a15_row23_col6\" class=\"data row23 col6\" >73.8% ± 2.7%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fe801a14e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### sorted by: correctness/citations_precision"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_2ad00_row0_col0, #T_2ad00_row0_col1, #T_2ad00_row0_col2, #T_2ad00_row0_col3, #T_2ad00_row0_col4, #T_2ad00_row0_col5, #T_2ad00_row0_col6 {\n",
       "  font-weight: 1000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_2ad00\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_2ad00_level0_col0\" class=\"col_heading level0 col0\" >citations/ais_recall</th>\n",
       "      <th id=\"T_2ad00_level0_col1\" class=\"col_heading level0 col1\" >citations/ais_precision</th>\n",
       "      <th id=\"T_2ad00_level0_col2\" class=\"col_heading level0 col2\" >correctness/answer_overlap</th>\n",
       "      <th id=\"T_2ad00_level0_col3\" class=\"col_heading level0 col3\" >correctness/answer_entail</th>\n",
       "      <th id=\"T_2ad00_level0_col4\" class=\"col_heading level0 col4\" >correctness/citations_recall</th>\n",
       "      <th id=\"T_2ad00_level0_col5\" class=\"col_heading level0 col5\" >correctness/citations_precision</th>\n",
       "      <th id=\"T_2ad00_level0_col6\" class=\"col_heading level0 col6\" >quality/answer_relevance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >llm</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_2ad00_level0_row0\" class=\"row_heading level0 row0\" >rag-tge_Mistral_v6</th>\n",
       "      <td id=\"T_2ad00_row0_col0\" class=\"data row0 col0\" >78.3% ± 8.2%</td>\n",
       "      <td id=\"T_2ad00_row0_col1\" class=\"data row0 col1\" >94.7% ± 2.4%</td>\n",
       "      <td id=\"T_2ad00_row0_col2\" class=\"data row0 col2\" >90.6% ± 3.2%</td>\n",
       "      <td id=\"T_2ad00_row0_col3\" class=\"data row0 col3\" >93.0% ± 2.9%</td>\n",
       "      <td id=\"T_2ad00_row0_col4\" class=\"data row0 col4\" >93.7% ± 2.3%</td>\n",
       "      <td id=\"T_2ad00_row0_col5\" class=\"data row0 col5\" >96.4% ± 1.8%</td>\n",
       "      <td id=\"T_2ad00_row0_col6\" class=\"data row0 col6\" >76.1% ± 4.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2ad00_level0_row1\" class=\"row_heading level0 row1\" >qwen1_5-32b-chat-q8_0</th>\n",
       "      <td id=\"T_2ad00_row1_col0\" class=\"data row1 col0\" >51.2% ± 3.2%</td>\n",
       "      <td id=\"T_2ad00_row1_col1\" class=\"data row1 col1\" >99.3% ± 0.3%</td>\n",
       "      <td id=\"T_2ad00_row1_col2\" class=\"data row1 col2\" >87.4% ± 0.6%</td>\n",
       "      <td id=\"T_2ad00_row1_col3\" class=\"data row1 col3\" >89.7% ± 0.6%</td>\n",
       "      <td id=\"T_2ad00_row1_col4\" class=\"data row1 col4\" >65.5% ± 0.9%</td>\n",
       "      <td id=\"T_2ad00_row1_col5\" class=\"data row1 col5\" >96.2% ± 0.0%</td>\n",
       "      <td id=\"T_2ad00_row1_col6\" class=\"data row1 col6\" >71.0% ± 2.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2ad00_level0_row2\" class=\"row_heading level0 row2\" >rag-tge_Llama-3-8B_v3</th>\n",
       "      <td id=\"T_2ad00_row2_col0\" class=\"data row2 col0\" >81.3% ± 7.5%</td>\n",
       "      <td id=\"T_2ad00_row2_col1\" class=\"data row2 col1\" >95.6% ± 2.9%</td>\n",
       "      <td id=\"T_2ad00_row2_col2\" class=\"data row2 col2\" >85.1% ± 4.5%</td>\n",
       "      <td id=\"T_2ad00_row2_col3\" class=\"data row2 col3\" >90.0% ± 2.3%</td>\n",
       "      <td id=\"T_2ad00_row2_col4\" class=\"data row2 col4\" >93.3% ± 2.5%</td>\n",
       "      <td id=\"T_2ad00_row2_col5\" class=\"data row2 col5\" >95.0% ± 0.9%</td>\n",
       "      <td id=\"T_2ad00_row2_col6\" class=\"data row2 col6\" >76.1% ± 3.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2ad00_level0_row3\" class=\"row_heading level0 row3\" >gpt-3.5-turbo-0125</th>\n",
       "      <td id=\"T_2ad00_row3_col0\" class=\"data row3 col0\" >73.8% ± 7.3%</td>\n",
       "      <td id=\"T_2ad00_row3_col1\" class=\"data row3 col1\" >99.6% ± 0.3%</td>\n",
       "      <td id=\"T_2ad00_row3_col2\" class=\"data row3 col2\" >88.4% ± 1.3%</td>\n",
       "      <td id=\"T_2ad00_row3_col3\" class=\"data row3 col3\" >91.0% ± 1.2%</td>\n",
       "      <td id=\"T_2ad00_row3_col4\" class=\"data row3 col4\" >85.3% ± 2.3%</td>\n",
       "      <td id=\"T_2ad00_row3_col5\" class=\"data row3 col5\" >94.2% ± 2.4%</td>\n",
       "      <td id=\"T_2ad00_row3_col6\" class=\"data row3 col6\" >75.2% ± 4.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2ad00_level0_row4\" class=\"row_heading level0 row4\" >gpt-4-turbo</th>\n",
       "      <td id=\"T_2ad00_row4_col0\" class=\"data row4 col0\" >73.3% ± 4.5%</td>\n",
       "      <td id=\"T_2ad00_row4_col1\" class=\"data row4 col1\" >98.5% ± 0.6%</td>\n",
       "      <td id=\"T_2ad00_row4_col2\" class=\"data row4 col2\" >87.7% ± 0.8%</td>\n",
       "      <td id=\"T_2ad00_row4_col3\" class=\"data row4 col3\" >90.0% ± 1.2%</td>\n",
       "      <td id=\"T_2ad00_row4_col4\" class=\"data row4 col4\" >79.5% ± 2.9%</td>\n",
       "      <td id=\"T_2ad00_row4_col5\" class=\"data row4 col5\" >93.9% ± 1.1%</td>\n",
       "      <td id=\"T_2ad00_row4_col6\" class=\"data row4 col6\" >74.7% ± 3.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2ad00_level0_row5\" class=\"row_heading level0 row5\" >qwen1_5-110b-chat</th>\n",
       "      <td id=\"T_2ad00_row5_col0\" class=\"data row5 col0\" >52.8% ± 9.5%</td>\n",
       "      <td id=\"T_2ad00_row5_col1\" class=\"data row5 col1\" >97.6% ± 2.7%</td>\n",
       "      <td id=\"T_2ad00_row5_col2\" class=\"data row5 col2\" >86.7% ± 2.0%</td>\n",
       "      <td id=\"T_2ad00_row5_col3\" class=\"data row5 col3\" >89.3% ± 1.7%</td>\n",
       "      <td id=\"T_2ad00_row5_col4\" class=\"data row5 col4\" >73.5% ± 5.2%</td>\n",
       "      <td id=\"T_2ad00_row5_col5\" class=\"data row5 col5\" >93.0% ± 2.3%</td>\n",
       "      <td id=\"T_2ad00_row5_col6\" class=\"data row5 col6\" >73.7% ± 5.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2ad00_level0_row6\" class=\"row_heading level0 row6\" >qwen1_5-14b-chat-q8_0</th>\n",
       "      <td id=\"T_2ad00_row6_col0\" class=\"data row6 col0\" >63.3% ± 4.2%</td>\n",
       "      <td id=\"T_2ad00_row6_col1\" class=\"data row6 col1\" >97.5% ± 0.9%</td>\n",
       "      <td id=\"T_2ad00_row6_col2\" class=\"data row6 col2\" >87.6% ± 0.7%</td>\n",
       "      <td id=\"T_2ad00_row6_col3\" class=\"data row6 col3\" >86.3% ± 0.6%</td>\n",
       "      <td id=\"T_2ad00_row6_col4\" class=\"data row6 col4\" >66.0% ± 3.1%</td>\n",
       "      <td id=\"T_2ad00_row6_col5\" class=\"data row6 col5\" >92.0% ± 1.6%</td>\n",
       "      <td id=\"T_2ad00_row6_col6\" class=\"data row6 col6\" >71.9% ± 3.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2ad00_level0_row7\" class=\"row_heading level0 row7\" >Meta-Llama-3-8B-Instruct</th>\n",
       "      <td id=\"T_2ad00_row7_col0\" class=\"data row7 col0\" >49.3% ± 6.1%</td>\n",
       "      <td id=\"T_2ad00_row7_col1\" class=\"data row7 col1\" >95.1% ± 2.2%</td>\n",
       "      <td id=\"T_2ad00_row7_col2\" class=\"data row7 col2\" >91.8% ± 0.0%</td>\n",
       "      <td id=\"T_2ad00_row7_col3\" class=\"data row7 col3\" >90.3% ± 0.6%</td>\n",
       "      <td id=\"T_2ad00_row7_col4\" class=\"data row7 col4\" >75.3% ± 2.3%</td>\n",
       "      <td id=\"T_2ad00_row7_col5\" class=\"data row7 col5\" >90.8% ± 1.8%</td>\n",
       "      <td id=\"T_2ad00_row7_col6\" class=\"data row7 col6\" >72.0% ± 3.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2ad00_level0_row8\" class=\"row_heading level0 row8\" >Meta-Llama-3-70B-Instruct</th>\n",
       "      <td id=\"T_2ad00_row8_col0\" class=\"data row8 col0\" >69.2% ± 5.0%</td>\n",
       "      <td id=\"T_2ad00_row8_col1\" class=\"data row8 col1\" >95.6% ± 1.8%</td>\n",
       "      <td id=\"T_2ad00_row8_col2\" class=\"data row8 col2\" >86.6% ± 2.1%</td>\n",
       "      <td id=\"T_2ad00_row8_col3\" class=\"data row8 col3\" >87.3% ± 1.7%</td>\n",
       "      <td id=\"T_2ad00_row8_col4\" class=\"data row8 col4\" >78.5% ± 1.7%</td>\n",
       "      <td id=\"T_2ad00_row8_col5\" class=\"data row8 col5\" >90.8% ± 1.0%</td>\n",
       "      <td id=\"T_2ad00_row8_col6\" class=\"data row8 col6\" >75.3% ± 2.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2ad00_level0_row9\" class=\"row_heading level0 row9\" >rag-tge_Mistral.Q8</th>\n",
       "      <td id=\"T_2ad00_row9_col0\" class=\"data row9 col0\" >81.2% ± 6.1%</td>\n",
       "      <td id=\"T_2ad00_row9_col1\" class=\"data row9 col1\" >95.8% ± 2.1%</td>\n",
       "      <td id=\"T_2ad00_row9_col2\" class=\"data row9 col2\" >86.1% ± 1.3%</td>\n",
       "      <td id=\"T_2ad00_row9_col3\" class=\"data row9 col3\" >86.7% ± 0.6%</td>\n",
       "      <td id=\"T_2ad00_row9_col4\" class=\"data row9 col4\" >91.5% ± 2.3%</td>\n",
       "      <td id=\"T_2ad00_row9_col5\" class=\"data row9 col5\" >90.7% ± 4.1%</td>\n",
       "      <td id=\"T_2ad00_row9_col6\" class=\"data row9 col6\" >74.2% ± 4.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2ad00_level0_row10\" class=\"row_heading level0 row10\" >rag-tge_Mistral_v2-4480</th>\n",
       "      <td id=\"T_2ad00_row10_col0\" class=\"data row10 col0\" >73.3% ± 13.5%</td>\n",
       "      <td id=\"T_2ad00_row10_col1\" class=\"data row10 col1\" >95.4% ± 3.1%</td>\n",
       "      <td id=\"T_2ad00_row10_col2\" class=\"data row10 col2\" >91.5% ± 1.8%</td>\n",
       "      <td id=\"T_2ad00_row10_col3\" class=\"data row10 col3\" >89.7% ± 2.3%</td>\n",
       "      <td id=\"T_2ad00_row10_col4\" class=\"data row10 col4\" >97.2% ± 2.6%</td>\n",
       "      <td id=\"T_2ad00_row10_col5\" class=\"data row10 col5\" >87.7% ± 7.0%</td>\n",
       "      <td id=\"T_2ad00_row10_col6\" class=\"data row10 col6\" >72.9% ± 5.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2ad00_level0_row11\" class=\"row_heading level0 row11\" >Mixtral-8x7B-Instruct-v0.1</th>\n",
       "      <td id=\"T_2ad00_row11_col0\" class=\"data row11 col0\" >65.4% ± 6.1%</td>\n",
       "      <td id=\"T_2ad00_row11_col1\" class=\"data row11 col1\" >92.9% ± 3.0%</td>\n",
       "      <td id=\"T_2ad00_row11_col2\" class=\"data row11 col2\" >91.0% ± 0.1%</td>\n",
       "      <td id=\"T_2ad00_row11_col3\" class=\"data row11 col3\" >89.7% ± 1.2%</td>\n",
       "      <td id=\"T_2ad00_row11_col4\" class=\"data row11 col4\" >82.5% ± 3.5%</td>\n",
       "      <td id=\"T_2ad00_row11_col5\" class=\"data row11 col5\" >87.6% ± 3.8%</td>\n",
       "      <td id=\"T_2ad00_row11_col6\" class=\"data row11 col6\" >72.6% ± 3.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2ad00_level0_row12\" class=\"row_heading level0 row12\" >rag-tge_Mistral_v2-3360</th>\n",
       "      <td id=\"T_2ad00_row12_col0\" class=\"data row12 col0\" >68.3% ± 14.6%</td>\n",
       "      <td id=\"T_2ad00_row12_col1\" class=\"data row12 col1\" >95.8% ± 3.3%</td>\n",
       "      <td id=\"T_2ad00_row12_col2\" class=\"data row12 col2\" >93.6% ± 3.1%</td>\n",
       "      <td id=\"T_2ad00_row12_col3\" class=\"data row12 col3\" >91.0% ± 1.2%</td>\n",
       "      <td id=\"T_2ad00_row12_col4\" class=\"data row12 col4\" >96.8% ± 2.6%</td>\n",
       "      <td id=\"T_2ad00_row12_col5\" class=\"data row12 col5\" >84.2% ± 8.4%</td>\n",
       "      <td id=\"T_2ad00_row12_col6\" class=\"data row12 col6\" >72.6% ± 6.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2ad00_level0_row13\" class=\"row_heading level0 row13\" >qwen1_5-7b-chat-q8_0</th>\n",
       "      <td id=\"T_2ad00_row13_col0\" class=\"data row13 col0\" >23.3% ± 6.4%</td>\n",
       "      <td id=\"T_2ad00_row13_col1\" class=\"data row13 col1\" >86.2% ± 5.7%</td>\n",
       "      <td id=\"T_2ad00_row13_col2\" class=\"data row13 col2\" >86.5% ± 2.4%</td>\n",
       "      <td id=\"T_2ad00_row13_col3\" class=\"data row13 col3\" >84.0% ± 2.3%</td>\n",
       "      <td id=\"T_2ad00_row13_col4\" class=\"data row13 col4\" >54.8% ± 7.4%</td>\n",
       "      <td id=\"T_2ad00_row13_col5\" class=\"data row13 col5\" >80.0% ± 7.8%</td>\n",
       "      <td id=\"T_2ad00_row13_col6\" class=\"data row13 col6\" >76.8% ± 3.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2ad00_level0_row14\" class=\"row_heading level0 row14\" >rag-tge_Llama-3-8B_v1</th>\n",
       "      <td id=\"T_2ad00_row14_col0\" class=\"data row14 col0\" >64.0% ± 10.0%</td>\n",
       "      <td id=\"T_2ad00_row14_col1\" class=\"data row14 col1\" >95.4% ± 3.0%</td>\n",
       "      <td id=\"T_2ad00_row14_col2\" class=\"data row14 col2\" >94.4% ± 1.4%</td>\n",
       "      <td id=\"T_2ad00_row14_col3\" class=\"data row14 col3\" >92.3% ± 2.3%</td>\n",
       "      <td id=\"T_2ad00_row14_col4\" class=\"data row14 col4\" >98.8% ± 0.9%</td>\n",
       "      <td id=\"T_2ad00_row14_col5\" class=\"data row14 col5\" >77.6% ± 9.9%</td>\n",
       "      <td id=\"T_2ad00_row14_col6\" class=\"data row14 col6\" >73.5% ± 7.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2ad00_level0_row15\" class=\"row_heading level0 row15\" >rag-tge_Llama-3-8B_v2</th>\n",
       "      <td id=\"T_2ad00_row15_col0\" class=\"data row15 col0\" >65.4% ± 12.0%</td>\n",
       "      <td id=\"T_2ad00_row15_col1\" class=\"data row15 col1\" >94.4% ± 5.0%</td>\n",
       "      <td id=\"T_2ad00_row15_col2\" class=\"data row15 col2\" >93.4% ± 1.5%</td>\n",
       "      <td id=\"T_2ad00_row15_col3\" class=\"data row15 col3\" >93.3% ± 4.0%</td>\n",
       "      <td id=\"T_2ad00_row15_col4\" class=\"data row15 col4\" >98.2% ± 0.6%</td>\n",
       "      <td id=\"T_2ad00_row15_col5\" class=\"data row15 col5\" >76.7% ± 13.6%</td>\n",
       "      <td id=\"T_2ad00_row15_col6\" class=\"data row15 col6\" >74.6% ± 6.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2ad00_level0_row16\" class=\"row_heading level0 row16\" >zephyr-7b-beta</th>\n",
       "      <td id=\"T_2ad00_row16_col0\" class=\"data row16 col0\" >40.7% ± 9.1%</td>\n",
       "      <td id=\"T_2ad00_row16_col1\" class=\"data row16 col1\" >91.1% ± 6.9%</td>\n",
       "      <td id=\"T_2ad00_row16_col2\" class=\"data row16 col2\" >85.4% ± 2.9%</td>\n",
       "      <td id=\"T_2ad00_row16_col3\" class=\"data row16 col3\" >86.7% ± 4.6%</td>\n",
       "      <td id=\"T_2ad00_row16_col4\" class=\"data row16 col4\" >62.7% ± 8.0%</td>\n",
       "      <td id=\"T_2ad00_row16_col5\" class=\"data row16 col5\" >72.8% ± 8.8%</td>\n",
       "      <td id=\"T_2ad00_row16_col6\" class=\"data row16 col6\" >71.5% ± 4.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2ad00_level0_row17\" class=\"row_heading level0 row17\" >Mistral-7B-Instruct-v0.2</th>\n",
       "      <td id=\"T_2ad00_row17_col0\" class=\"data row17 col0\" >57.6% ± 3.7%</td>\n",
       "      <td id=\"T_2ad00_row17_col1\" class=\"data row17 col1\" >87.2% ± 3.8%</td>\n",
       "      <td id=\"T_2ad00_row17_col2\" class=\"data row17 col2\" >87.3% ± 1.5%</td>\n",
       "      <td id=\"T_2ad00_row17_col3\" class=\"data row17 col3\" >88.7% ± 1.7%</td>\n",
       "      <td id=\"T_2ad00_row17_col4\" class=\"data row17 col4\" >75.2% ± 2.6%</td>\n",
       "      <td id=\"T_2ad00_row17_col5\" class=\"data row17 col5\" >72.2% ± 5.0%</td>\n",
       "      <td id=\"T_2ad00_row17_col6\" class=\"data row17 col6\" >72.0% ± 2.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2ad00_level0_row18\" class=\"row_heading level0 row18\" >gemma-1.1-7b-it</th>\n",
       "      <td id=\"T_2ad00_row18_col0\" class=\"data row18 col0\" >36.8% ± 12.3%</td>\n",
       "      <td id=\"T_2ad00_row18_col1\" class=\"data row18 col1\" >76.7% ± 8.3%</td>\n",
       "      <td id=\"T_2ad00_row18_col2\" class=\"data row18 col2\" >72.3% ± 7.6%</td>\n",
       "      <td id=\"T_2ad00_row18_col3\" class=\"data row18 col3\" >72.7% ± 7.5%</td>\n",
       "      <td id=\"T_2ad00_row18_col4\" class=\"data row18 col4\" >47.2% ± 8.1%</td>\n",
       "      <td id=\"T_2ad00_row18_col5\" class=\"data row18 col5\" >66.0% ± 10.1%</td>\n",
       "      <td id=\"T_2ad00_row18_col6\" class=\"data row18 col6\" >73.9% ± 4.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2ad00_level0_row19\" class=\"row_heading level0 row19\" >c4ai-command-r-plus</th>\n",
       "      <td id=\"T_2ad00_row19_col0\" class=\"data row19 col0\" >40.9% ± 6.0%</td>\n",
       "      <td id=\"T_2ad00_row19_col1\" class=\"data row19 col1\" >71.5% ± 6.1%</td>\n",
       "      <td id=\"T_2ad00_row19_col2\" class=\"data row19 col2\" >73.6% ± 2.3%</td>\n",
       "      <td id=\"T_2ad00_row19_col3\" class=\"data row19 col3\" >88.0% ± 0.0%</td>\n",
       "      <td id=\"T_2ad00_row19_col4\" class=\"data row19 col4\" >57.8% ± 4.4%</td>\n",
       "      <td id=\"T_2ad00_row19_col5\" class=\"data row19 col5\" >64.4% ± 5.5%</td>\n",
       "      <td id=\"T_2ad00_row19_col6\" class=\"data row19 col6\" >63.5% ± 4.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2ad00_level0_row20\" class=\"row_heading level0 row20\" >zephyr-orpo-141b-A35b-v0.1</th>\n",
       "      <td id=\"T_2ad00_row20_col0\" class=\"data row20 col0\" >41.8% ± 13.4%</td>\n",
       "      <td id=\"T_2ad00_row20_col1\" class=\"data row20 col1\" >79.6% ± 10.1%</td>\n",
       "      <td id=\"T_2ad00_row20_col2\" class=\"data row20 col2\" >80.7% ± 6.0%</td>\n",
       "      <td id=\"T_2ad00_row20_col3\" class=\"data row20 col3\" >86.0% ± 4.0%</td>\n",
       "      <td id=\"T_2ad00_row20_col4\" class=\"data row20 col4\" >77.5% ± 9.2%</td>\n",
       "      <td id=\"T_2ad00_row20_col5\" class=\"data row20 col5\" >54.6% ± 10.8%</td>\n",
       "      <td id=\"T_2ad00_row20_col6\" class=\"data row20 col6\" >69.6% ± 5.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2ad00_level0_row21\" class=\"row_heading level0 row21\" >Phi-3-mini-4k-instruct</th>\n",
       "      <td id=\"T_2ad00_row21_col0\" class=\"data row21 col0\" >25.3% ± 3.1%</td>\n",
       "      <td id=\"T_2ad00_row21_col1\" class=\"data row21 col1\" >57.8% ± 7.0%</td>\n",
       "      <td id=\"T_2ad00_row21_col2\" class=\"data row21 col2\" >71.1% ± 1.2%</td>\n",
       "      <td id=\"T_2ad00_row21_col3\" class=\"data row21 col3\" >78.0% ± 2.3%</td>\n",
       "      <td id=\"T_2ad00_row21_col4\" class=\"data row21 col4\" >45.2% ± 5.1%</td>\n",
       "      <td id=\"T_2ad00_row21_col5\" class=\"data row21 col5\" >45.8% ± 6.4%</td>\n",
       "      <td id=\"T_2ad00_row21_col6\" class=\"data row21 col6\" >66.4% ± 3.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2ad00_level0_row22\" class=\"row_heading level0 row22\" >gemma-1.1-2b-it</th>\n",
       "      <td id=\"T_2ad00_row22_col0\" class=\"data row22 col0\" >0.7% ± 0.6%</td>\n",
       "      <td id=\"T_2ad00_row22_col1\" class=\"data row22 col1\" >4.6% ± 1.0%</td>\n",
       "      <td id=\"T_2ad00_row22_col2\" class=\"data row22 col2\" >63.1% ± 3.8%</td>\n",
       "      <td id=\"T_2ad00_row22_col3\" class=\"data row22 col3\" >67.0% ± 4.0%</td>\n",
       "      <td id=\"T_2ad00_row22_col4\" class=\"data row22 col4\" >3.3% ± 0.6%</td>\n",
       "      <td id=\"T_2ad00_row22_col5\" class=\"data row22 col5\" >4.3% ± 0.7%</td>\n",
       "      <td id=\"T_2ad00_row22_col6\" class=\"data row22 col6\" >70.9% ± 2.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_2ad00_level0_row23\" class=\"row_heading level0 row23\" >Mistral-7B-Instruct-v0.1</th>\n",
       "      <td id=\"T_2ad00_row23_col0\" class=\"data row23 col0\" >0.5% ± 0.0%</td>\n",
       "      <td id=\"T_2ad00_row23_col1\" class=\"data row23 col1\" >1.0% ± 0.0%</td>\n",
       "      <td id=\"T_2ad00_row23_col2\" class=\"data row23 col2\" >79.0% ± 1.4%</td>\n",
       "      <td id=\"T_2ad00_row23_col3\" class=\"data row23 col3\" >82.7% ± 1.7%</td>\n",
       "      <td id=\"T_2ad00_row23_col4\" class=\"data row23 col4\" >1.0% ± 0.0%</td>\n",
       "      <td id=\"T_2ad00_row23_col5\" class=\"data row23 col5\" >0.7% ± 0.0%</td>\n",
       "      <td id=\"T_2ad00_row23_col6\" class=\"data row23 col6\" >73.8% ± 2.7%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fe801a14e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### sorted by: quality/answer_relevance"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_a39b7_row1_col0, #T_a39b7_row1_col1, #T_a39b7_row1_col2, #T_a39b7_row1_col3, #T_a39b7_row1_col4, #T_a39b7_row1_col5, #T_a39b7_row1_col6 {\n",
       "  font-weight: 1000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_a39b7\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_a39b7_level0_col0\" class=\"col_heading level0 col0\" >citations/ais_recall</th>\n",
       "      <th id=\"T_a39b7_level0_col1\" class=\"col_heading level0 col1\" >citations/ais_precision</th>\n",
       "      <th id=\"T_a39b7_level0_col2\" class=\"col_heading level0 col2\" >correctness/answer_overlap</th>\n",
       "      <th id=\"T_a39b7_level0_col3\" class=\"col_heading level0 col3\" >correctness/answer_entail</th>\n",
       "      <th id=\"T_a39b7_level0_col4\" class=\"col_heading level0 col4\" >correctness/citations_recall</th>\n",
       "      <th id=\"T_a39b7_level0_col5\" class=\"col_heading level0 col5\" >correctness/citations_precision</th>\n",
       "      <th id=\"T_a39b7_level0_col6\" class=\"col_heading level0 col6\" >quality/answer_relevance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th class=\"index_name level0\" >llm</th>\n",
       "      <th class=\"blank col0\" >&nbsp;</th>\n",
       "      <th class=\"blank col1\" >&nbsp;</th>\n",
       "      <th class=\"blank col2\" >&nbsp;</th>\n",
       "      <th class=\"blank col3\" >&nbsp;</th>\n",
       "      <th class=\"blank col4\" >&nbsp;</th>\n",
       "      <th class=\"blank col5\" >&nbsp;</th>\n",
       "      <th class=\"blank col6\" >&nbsp;</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_a39b7_level0_row0\" class=\"row_heading level0 row0\" >qwen1_5-7b-chat-q8_0</th>\n",
       "      <td id=\"T_a39b7_row0_col0\" class=\"data row0 col0\" >23.3% ± 6.4%</td>\n",
       "      <td id=\"T_a39b7_row0_col1\" class=\"data row0 col1\" >86.2% ± 5.7%</td>\n",
       "      <td id=\"T_a39b7_row0_col2\" class=\"data row0 col2\" >86.5% ± 2.4%</td>\n",
       "      <td id=\"T_a39b7_row0_col3\" class=\"data row0 col3\" >84.0% ± 2.3%</td>\n",
       "      <td id=\"T_a39b7_row0_col4\" class=\"data row0 col4\" >54.8% ± 7.4%</td>\n",
       "      <td id=\"T_a39b7_row0_col5\" class=\"data row0 col5\" >80.0% ± 7.8%</td>\n",
       "      <td id=\"T_a39b7_row0_col6\" class=\"data row0 col6\" >76.8% ± 3.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a39b7_level0_row1\" class=\"row_heading level0 row1\" >rag-tge_Mistral_v6</th>\n",
       "      <td id=\"T_a39b7_row1_col0\" class=\"data row1 col0\" >78.3% ± 8.2%</td>\n",
       "      <td id=\"T_a39b7_row1_col1\" class=\"data row1 col1\" >94.7% ± 2.4%</td>\n",
       "      <td id=\"T_a39b7_row1_col2\" class=\"data row1 col2\" >90.6% ± 3.2%</td>\n",
       "      <td id=\"T_a39b7_row1_col3\" class=\"data row1 col3\" >93.0% ± 2.9%</td>\n",
       "      <td id=\"T_a39b7_row1_col4\" class=\"data row1 col4\" >93.7% ± 2.3%</td>\n",
       "      <td id=\"T_a39b7_row1_col5\" class=\"data row1 col5\" >96.4% ± 1.8%</td>\n",
       "      <td id=\"T_a39b7_row1_col6\" class=\"data row1 col6\" >76.1% ± 4.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a39b7_level0_row2\" class=\"row_heading level0 row2\" >rag-tge_Llama-3-8B_v3</th>\n",
       "      <td id=\"T_a39b7_row2_col0\" class=\"data row2 col0\" >81.3% ± 7.5%</td>\n",
       "      <td id=\"T_a39b7_row2_col1\" class=\"data row2 col1\" >95.6% ± 2.9%</td>\n",
       "      <td id=\"T_a39b7_row2_col2\" class=\"data row2 col2\" >85.1% ± 4.5%</td>\n",
       "      <td id=\"T_a39b7_row2_col3\" class=\"data row2 col3\" >90.0% ± 2.3%</td>\n",
       "      <td id=\"T_a39b7_row2_col4\" class=\"data row2 col4\" >93.3% ± 2.5%</td>\n",
       "      <td id=\"T_a39b7_row2_col5\" class=\"data row2 col5\" >95.0% ± 0.9%</td>\n",
       "      <td id=\"T_a39b7_row2_col6\" class=\"data row2 col6\" >76.1% ± 3.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a39b7_level0_row3\" class=\"row_heading level0 row3\" >Meta-Llama-3-70B-Instruct</th>\n",
       "      <td id=\"T_a39b7_row3_col0\" class=\"data row3 col0\" >69.2% ± 5.0%</td>\n",
       "      <td id=\"T_a39b7_row3_col1\" class=\"data row3 col1\" >95.6% ± 1.8%</td>\n",
       "      <td id=\"T_a39b7_row3_col2\" class=\"data row3 col2\" >86.6% ± 2.1%</td>\n",
       "      <td id=\"T_a39b7_row3_col3\" class=\"data row3 col3\" >87.3% ± 1.7%</td>\n",
       "      <td id=\"T_a39b7_row3_col4\" class=\"data row3 col4\" >78.5% ± 1.7%</td>\n",
       "      <td id=\"T_a39b7_row3_col5\" class=\"data row3 col5\" >90.8% ± 1.0%</td>\n",
       "      <td id=\"T_a39b7_row3_col6\" class=\"data row3 col6\" >75.3% ± 2.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a39b7_level0_row4\" class=\"row_heading level0 row4\" >gpt-3.5-turbo-0125</th>\n",
       "      <td id=\"T_a39b7_row4_col0\" class=\"data row4 col0\" >73.8% ± 7.3%</td>\n",
       "      <td id=\"T_a39b7_row4_col1\" class=\"data row4 col1\" >99.6% ± 0.3%</td>\n",
       "      <td id=\"T_a39b7_row4_col2\" class=\"data row4 col2\" >88.4% ± 1.3%</td>\n",
       "      <td id=\"T_a39b7_row4_col3\" class=\"data row4 col3\" >91.0% ± 1.2%</td>\n",
       "      <td id=\"T_a39b7_row4_col4\" class=\"data row4 col4\" >85.3% ± 2.3%</td>\n",
       "      <td id=\"T_a39b7_row4_col5\" class=\"data row4 col5\" >94.2% ± 2.4%</td>\n",
       "      <td id=\"T_a39b7_row4_col6\" class=\"data row4 col6\" >75.2% ± 4.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a39b7_level0_row5\" class=\"row_heading level0 row5\" >gpt-4-turbo</th>\n",
       "      <td id=\"T_a39b7_row5_col0\" class=\"data row5 col0\" >73.3% ± 4.5%</td>\n",
       "      <td id=\"T_a39b7_row5_col1\" class=\"data row5 col1\" >98.5% ± 0.6%</td>\n",
       "      <td id=\"T_a39b7_row5_col2\" class=\"data row5 col2\" >87.7% ± 0.8%</td>\n",
       "      <td id=\"T_a39b7_row5_col3\" class=\"data row5 col3\" >90.0% ± 1.2%</td>\n",
       "      <td id=\"T_a39b7_row5_col4\" class=\"data row5 col4\" >79.5% ± 2.9%</td>\n",
       "      <td id=\"T_a39b7_row5_col5\" class=\"data row5 col5\" >93.9% ± 1.1%</td>\n",
       "      <td id=\"T_a39b7_row5_col6\" class=\"data row5 col6\" >74.7% ± 3.4%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a39b7_level0_row6\" class=\"row_heading level0 row6\" >rag-tge_Llama-3-8B_v2</th>\n",
       "      <td id=\"T_a39b7_row6_col0\" class=\"data row6 col0\" >65.4% ± 12.0%</td>\n",
       "      <td id=\"T_a39b7_row6_col1\" class=\"data row6 col1\" >94.4% ± 5.0%</td>\n",
       "      <td id=\"T_a39b7_row6_col2\" class=\"data row6 col2\" >93.4% ± 1.5%</td>\n",
       "      <td id=\"T_a39b7_row6_col3\" class=\"data row6 col3\" >93.3% ± 4.0%</td>\n",
       "      <td id=\"T_a39b7_row6_col4\" class=\"data row6 col4\" >98.2% ± 0.6%</td>\n",
       "      <td id=\"T_a39b7_row6_col5\" class=\"data row6 col5\" >76.7% ± 13.6%</td>\n",
       "      <td id=\"T_a39b7_row6_col6\" class=\"data row6 col6\" >74.6% ± 6.6%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a39b7_level0_row7\" class=\"row_heading level0 row7\" >rag-tge_Mistral.Q8</th>\n",
       "      <td id=\"T_a39b7_row7_col0\" class=\"data row7 col0\" >81.2% ± 6.1%</td>\n",
       "      <td id=\"T_a39b7_row7_col1\" class=\"data row7 col1\" >95.8% ± 2.1%</td>\n",
       "      <td id=\"T_a39b7_row7_col2\" class=\"data row7 col2\" >86.1% ± 1.3%</td>\n",
       "      <td id=\"T_a39b7_row7_col3\" class=\"data row7 col3\" >86.7% ± 0.6%</td>\n",
       "      <td id=\"T_a39b7_row7_col4\" class=\"data row7 col4\" >91.5% ± 2.3%</td>\n",
       "      <td id=\"T_a39b7_row7_col5\" class=\"data row7 col5\" >90.7% ± 4.1%</td>\n",
       "      <td id=\"T_a39b7_row7_col6\" class=\"data row7 col6\" >74.2% ± 4.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a39b7_level0_row8\" class=\"row_heading level0 row8\" >gemma-1.1-7b-it</th>\n",
       "      <td id=\"T_a39b7_row8_col0\" class=\"data row8 col0\" >36.8% ± 12.3%</td>\n",
       "      <td id=\"T_a39b7_row8_col1\" class=\"data row8 col1\" >76.7% ± 8.3%</td>\n",
       "      <td id=\"T_a39b7_row8_col2\" class=\"data row8 col2\" >72.3% ± 7.6%</td>\n",
       "      <td id=\"T_a39b7_row8_col3\" class=\"data row8 col3\" >72.7% ± 7.5%</td>\n",
       "      <td id=\"T_a39b7_row8_col4\" class=\"data row8 col4\" >47.2% ± 8.1%</td>\n",
       "      <td id=\"T_a39b7_row8_col5\" class=\"data row8 col5\" >66.0% ± 10.1%</td>\n",
       "      <td id=\"T_a39b7_row8_col6\" class=\"data row8 col6\" >73.9% ± 4.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a39b7_level0_row9\" class=\"row_heading level0 row9\" >Mistral-7B-Instruct-v0.1</th>\n",
       "      <td id=\"T_a39b7_row9_col0\" class=\"data row9 col0\" >0.5% ± 0.0%</td>\n",
       "      <td id=\"T_a39b7_row9_col1\" class=\"data row9 col1\" >1.0% ± 0.0%</td>\n",
       "      <td id=\"T_a39b7_row9_col2\" class=\"data row9 col2\" >79.0% ± 1.4%</td>\n",
       "      <td id=\"T_a39b7_row9_col3\" class=\"data row9 col3\" >82.7% ± 1.7%</td>\n",
       "      <td id=\"T_a39b7_row9_col4\" class=\"data row9 col4\" >1.0% ± 0.0%</td>\n",
       "      <td id=\"T_a39b7_row9_col5\" class=\"data row9 col5\" >0.7% ± 0.0%</td>\n",
       "      <td id=\"T_a39b7_row9_col6\" class=\"data row9 col6\" >73.8% ± 2.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a39b7_level0_row10\" class=\"row_heading level0 row10\" >qwen1_5-110b-chat</th>\n",
       "      <td id=\"T_a39b7_row10_col0\" class=\"data row10 col0\" >52.8% ± 9.5%</td>\n",
       "      <td id=\"T_a39b7_row10_col1\" class=\"data row10 col1\" >97.6% ± 2.7%</td>\n",
       "      <td id=\"T_a39b7_row10_col2\" class=\"data row10 col2\" >86.7% ± 2.0%</td>\n",
       "      <td id=\"T_a39b7_row10_col3\" class=\"data row10 col3\" >89.3% ± 1.7%</td>\n",
       "      <td id=\"T_a39b7_row10_col4\" class=\"data row10 col4\" >73.5% ± 5.2%</td>\n",
       "      <td id=\"T_a39b7_row10_col5\" class=\"data row10 col5\" >93.0% ± 2.3%</td>\n",
       "      <td id=\"T_a39b7_row10_col6\" class=\"data row10 col6\" >73.7% ± 5.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a39b7_level0_row11\" class=\"row_heading level0 row11\" >rag-tge_Llama-3-8B_v1</th>\n",
       "      <td id=\"T_a39b7_row11_col0\" class=\"data row11 col0\" >64.0% ± 10.0%</td>\n",
       "      <td id=\"T_a39b7_row11_col1\" class=\"data row11 col1\" >95.4% ± 3.0%</td>\n",
       "      <td id=\"T_a39b7_row11_col2\" class=\"data row11 col2\" >94.4% ± 1.4%</td>\n",
       "      <td id=\"T_a39b7_row11_col3\" class=\"data row11 col3\" >92.3% ± 2.3%</td>\n",
       "      <td id=\"T_a39b7_row11_col4\" class=\"data row11 col4\" >98.8% ± 0.9%</td>\n",
       "      <td id=\"T_a39b7_row11_col5\" class=\"data row11 col5\" >77.6% ± 9.9%</td>\n",
       "      <td id=\"T_a39b7_row11_col6\" class=\"data row11 col6\" >73.5% ± 7.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a39b7_level0_row12\" class=\"row_heading level0 row12\" >rag-tge_Mistral_v2-4480</th>\n",
       "      <td id=\"T_a39b7_row12_col0\" class=\"data row12 col0\" >73.3% ± 13.5%</td>\n",
       "      <td id=\"T_a39b7_row12_col1\" class=\"data row12 col1\" >95.4% ± 3.1%</td>\n",
       "      <td id=\"T_a39b7_row12_col2\" class=\"data row12 col2\" >91.5% ± 1.8%</td>\n",
       "      <td id=\"T_a39b7_row12_col3\" class=\"data row12 col3\" >89.7% ± 2.3%</td>\n",
       "      <td id=\"T_a39b7_row12_col4\" class=\"data row12 col4\" >97.2% ± 2.6%</td>\n",
       "      <td id=\"T_a39b7_row12_col5\" class=\"data row12 col5\" >87.7% ± 7.0%</td>\n",
       "      <td id=\"T_a39b7_row12_col6\" class=\"data row12 col6\" >72.9% ± 5.5%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a39b7_level0_row13\" class=\"row_heading level0 row13\" >rag-tge_Mistral_v2-3360</th>\n",
       "      <td id=\"T_a39b7_row13_col0\" class=\"data row13 col0\" >68.3% ± 14.6%</td>\n",
       "      <td id=\"T_a39b7_row13_col1\" class=\"data row13 col1\" >95.8% ± 3.3%</td>\n",
       "      <td id=\"T_a39b7_row13_col2\" class=\"data row13 col2\" >93.6% ± 3.1%</td>\n",
       "      <td id=\"T_a39b7_row13_col3\" class=\"data row13 col3\" >91.0% ± 1.2%</td>\n",
       "      <td id=\"T_a39b7_row13_col4\" class=\"data row13 col4\" >96.8% ± 2.6%</td>\n",
       "      <td id=\"T_a39b7_row13_col5\" class=\"data row13 col5\" >84.2% ± 8.4%</td>\n",
       "      <td id=\"T_a39b7_row13_col6\" class=\"data row13 col6\" >72.6% ± 6.3%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a39b7_level0_row14\" class=\"row_heading level0 row14\" >Mixtral-8x7B-Instruct-v0.1</th>\n",
       "      <td id=\"T_a39b7_row14_col0\" class=\"data row14 col0\" >65.4% ± 6.1%</td>\n",
       "      <td id=\"T_a39b7_row14_col1\" class=\"data row14 col1\" >92.9% ± 3.0%</td>\n",
       "      <td id=\"T_a39b7_row14_col2\" class=\"data row14 col2\" >91.0% ± 0.1%</td>\n",
       "      <td id=\"T_a39b7_row14_col3\" class=\"data row14 col3\" >89.7% ± 1.2%</td>\n",
       "      <td id=\"T_a39b7_row14_col4\" class=\"data row14 col4\" >82.5% ± 3.5%</td>\n",
       "      <td id=\"T_a39b7_row14_col5\" class=\"data row14 col5\" >87.6% ± 3.8%</td>\n",
       "      <td id=\"T_a39b7_row14_col6\" class=\"data row14 col6\" >72.6% ± 3.1%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a39b7_level0_row15\" class=\"row_heading level0 row15\" >Meta-Llama-3-8B-Instruct</th>\n",
       "      <td id=\"T_a39b7_row15_col0\" class=\"data row15 col0\" >49.3% ± 6.1%</td>\n",
       "      <td id=\"T_a39b7_row15_col1\" class=\"data row15 col1\" >95.1% ± 2.2%</td>\n",
       "      <td id=\"T_a39b7_row15_col2\" class=\"data row15 col2\" >91.8% ± 0.0%</td>\n",
       "      <td id=\"T_a39b7_row15_col3\" class=\"data row15 col3\" >90.3% ± 0.6%</td>\n",
       "      <td id=\"T_a39b7_row15_col4\" class=\"data row15 col4\" >75.3% ± 2.3%</td>\n",
       "      <td id=\"T_a39b7_row15_col5\" class=\"data row15 col5\" >90.8% ± 1.8%</td>\n",
       "      <td id=\"T_a39b7_row15_col6\" class=\"data row15 col6\" >72.0% ± 3.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a39b7_level0_row16\" class=\"row_heading level0 row16\" >Mistral-7B-Instruct-v0.2</th>\n",
       "      <td id=\"T_a39b7_row16_col0\" class=\"data row16 col0\" >57.6% ± 3.7%</td>\n",
       "      <td id=\"T_a39b7_row16_col1\" class=\"data row16 col1\" >87.2% ± 3.8%</td>\n",
       "      <td id=\"T_a39b7_row16_col2\" class=\"data row16 col2\" >87.3% ± 1.5%</td>\n",
       "      <td id=\"T_a39b7_row16_col3\" class=\"data row16 col3\" >88.7% ± 1.7%</td>\n",
       "      <td id=\"T_a39b7_row16_col4\" class=\"data row16 col4\" >75.2% ± 2.6%</td>\n",
       "      <td id=\"T_a39b7_row16_col5\" class=\"data row16 col5\" >72.2% ± 5.0%</td>\n",
       "      <td id=\"T_a39b7_row16_col6\" class=\"data row16 col6\" >72.0% ± 2.8%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a39b7_level0_row17\" class=\"row_heading level0 row17\" >qwen1_5-14b-chat-q8_0</th>\n",
       "      <td id=\"T_a39b7_row17_col0\" class=\"data row17 col0\" >63.3% ± 4.2%</td>\n",
       "      <td id=\"T_a39b7_row17_col1\" class=\"data row17 col1\" >97.5% ± 0.9%</td>\n",
       "      <td id=\"T_a39b7_row17_col2\" class=\"data row17 col2\" >87.6% ± 0.7%</td>\n",
       "      <td id=\"T_a39b7_row17_col3\" class=\"data row17 col3\" >86.3% ± 0.6%</td>\n",
       "      <td id=\"T_a39b7_row17_col4\" class=\"data row17 col4\" >66.0% ± 3.1%</td>\n",
       "      <td id=\"T_a39b7_row17_col5\" class=\"data row17 col5\" >92.0% ± 1.6%</td>\n",
       "      <td id=\"T_a39b7_row17_col6\" class=\"data row17 col6\" >71.9% ± 3.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a39b7_level0_row18\" class=\"row_heading level0 row18\" >zephyr-7b-beta</th>\n",
       "      <td id=\"T_a39b7_row18_col0\" class=\"data row18 col0\" >40.7% ± 9.1%</td>\n",
       "      <td id=\"T_a39b7_row18_col1\" class=\"data row18 col1\" >91.1% ± 6.9%</td>\n",
       "      <td id=\"T_a39b7_row18_col2\" class=\"data row18 col2\" >85.4% ± 2.9%</td>\n",
       "      <td id=\"T_a39b7_row18_col3\" class=\"data row18 col3\" >86.7% ± 4.6%</td>\n",
       "      <td id=\"T_a39b7_row18_col4\" class=\"data row18 col4\" >62.7% ± 8.0%</td>\n",
       "      <td id=\"T_a39b7_row18_col5\" class=\"data row18 col5\" >72.8% ± 8.8%</td>\n",
       "      <td id=\"T_a39b7_row18_col6\" class=\"data row18 col6\" >71.5% ± 4.2%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a39b7_level0_row19\" class=\"row_heading level0 row19\" >qwen1_5-32b-chat-q8_0</th>\n",
       "      <td id=\"T_a39b7_row19_col0\" class=\"data row19 col0\" >51.2% ± 3.2%</td>\n",
       "      <td id=\"T_a39b7_row19_col1\" class=\"data row19 col1\" >99.3% ± 0.3%</td>\n",
       "      <td id=\"T_a39b7_row19_col2\" class=\"data row19 col2\" >87.4% ± 0.6%</td>\n",
       "      <td id=\"T_a39b7_row19_col3\" class=\"data row19 col3\" >89.7% ± 0.6%</td>\n",
       "      <td id=\"T_a39b7_row19_col4\" class=\"data row19 col4\" >65.5% ± 0.9%</td>\n",
       "      <td id=\"T_a39b7_row19_col5\" class=\"data row19 col5\" >96.2% ± 0.0%</td>\n",
       "      <td id=\"T_a39b7_row19_col6\" class=\"data row19 col6\" >71.0% ± 2.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a39b7_level0_row20\" class=\"row_heading level0 row20\" >gemma-1.1-2b-it</th>\n",
       "      <td id=\"T_a39b7_row20_col0\" class=\"data row20 col0\" >0.7% ± 0.6%</td>\n",
       "      <td id=\"T_a39b7_row20_col1\" class=\"data row20 col1\" >4.6% ± 1.0%</td>\n",
       "      <td id=\"T_a39b7_row20_col2\" class=\"data row20 col2\" >63.1% ± 3.8%</td>\n",
       "      <td id=\"T_a39b7_row20_col3\" class=\"data row20 col3\" >67.0% ± 4.0%</td>\n",
       "      <td id=\"T_a39b7_row20_col4\" class=\"data row20 col4\" >3.3% ± 0.6%</td>\n",
       "      <td id=\"T_a39b7_row20_col5\" class=\"data row20 col5\" >4.3% ± 0.7%</td>\n",
       "      <td id=\"T_a39b7_row20_col6\" class=\"data row20 col6\" >70.9% ± 2.0%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a39b7_level0_row21\" class=\"row_heading level0 row21\" >zephyr-orpo-141b-A35b-v0.1</th>\n",
       "      <td id=\"T_a39b7_row21_col0\" class=\"data row21 col0\" >41.8% ± 13.4%</td>\n",
       "      <td id=\"T_a39b7_row21_col1\" class=\"data row21 col1\" >79.6% ± 10.1%</td>\n",
       "      <td id=\"T_a39b7_row21_col2\" class=\"data row21 col2\" >80.7% ± 6.0%</td>\n",
       "      <td id=\"T_a39b7_row21_col3\" class=\"data row21 col3\" >86.0% ± 4.0%</td>\n",
       "      <td id=\"T_a39b7_row21_col4\" class=\"data row21 col4\" >77.5% ± 9.2%</td>\n",
       "      <td id=\"T_a39b7_row21_col5\" class=\"data row21 col5\" >54.6% ± 10.8%</td>\n",
       "      <td id=\"T_a39b7_row21_col6\" class=\"data row21 col6\" >69.6% ± 5.9%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a39b7_level0_row22\" class=\"row_heading level0 row22\" >Phi-3-mini-4k-instruct</th>\n",
       "      <td id=\"T_a39b7_row22_col0\" class=\"data row22 col0\" >25.3% ± 3.1%</td>\n",
       "      <td id=\"T_a39b7_row22_col1\" class=\"data row22 col1\" >57.8% ± 7.0%</td>\n",
       "      <td id=\"T_a39b7_row22_col2\" class=\"data row22 col2\" >71.1% ± 1.2%</td>\n",
       "      <td id=\"T_a39b7_row22_col3\" class=\"data row22 col3\" >78.0% ± 2.3%</td>\n",
       "      <td id=\"T_a39b7_row22_col4\" class=\"data row22 col4\" >45.2% ± 5.1%</td>\n",
       "      <td id=\"T_a39b7_row22_col5\" class=\"data row22 col5\" >45.8% ± 6.4%</td>\n",
       "      <td id=\"T_a39b7_row22_col6\" class=\"data row22 col6\" >66.4% ± 3.7%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_a39b7_level0_row23\" class=\"row_heading level0 row23\" >c4ai-command-r-plus</th>\n",
       "      <td id=\"T_a39b7_row23_col0\" class=\"data row23 col0\" >40.9% ± 6.0%</td>\n",
       "      <td id=\"T_a39b7_row23_col1\" class=\"data row23 col1\" >71.5% ± 6.1%</td>\n",
       "      <td id=\"T_a39b7_row23_col2\" class=\"data row23 col2\" >73.6% ± 2.3%</td>\n",
       "      <td id=\"T_a39b7_row23_col3\" class=\"data row23 col3\" >88.0% ± 0.0%</td>\n",
       "      <td id=\"T_a39b7_row23_col4\" class=\"data row23 col4\" >57.8% ± 4.4%</td>\n",
       "      <td id=\"T_a39b7_row23_col5\" class=\"data row23 col5\" >64.4% ± 5.5%</td>\n",
       "      <td id=\"T_a39b7_row23_col6\" class=\"data row23 col6\" >63.5% ± 4.7%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x7fe801a14e10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "HIGHLIGHT_MODEL = \"rag-tge_Mistral_v6\"\n",
    "REMOVE_REST = False\n",
    "\n",
    "def highlight(x, row_name):\n",
    "    return ['font-weight: 1000' if x.name == row_name else '' for i in x]\n",
    "\n",
    "clean_results = show_cleaned_results(eval_display, keep_index_name=\"llm\")\n",
    "clean_results.style.apply(highlight, row_name=HIGHLIGHT_MODEL, axis=1)\n",
    "if REMOVE_REST:\n",
    "    clean_results = clean_results[~clean_results.index.get_level_values(\"llm\").str.contains(\"rag-tge\") | (clean_results.index.get_level_values(\"llm\") == HIGHLIGHT_MODEL)]\n",
    "\n",
    "for sort_by in [\n",
    "    \"citations/ais_recall\",\n",
    "    \"citations/ais_precision\",\n",
    "    \"correctness/answer_overlap\",\n",
    "    \"correctness/answer_entail\",\n",
    "    \"correctness/citations_recall\",\n",
    "    \"correctness/citations_precision\",\n",
    "    \"quality/answer_relevance\",\n",
    "]:\n",
    "    display(Markdown(f\"### sorted by: {sort_by}\"))\n",
    "    results = clean_results.sort_values(by=sort_by, ascending=False)\n",
    "    results = results.style.apply(highlight, row_name=HIGHLIGHT_MODEL, axis=1)\n",
    "    display(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Training results"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>citations/ais_recall</th>\n",
       "      <th>citations/ais_precision</th>\n",
       "      <th>citations/n_sentences</th>\n",
       "      <th>citations/n_total_citations</th>\n",
       "      <th>citations/n_correct_citations</th>\n",
       "      <th>citations/n_correctly_multicited_sentences</th>\n",
       "      <th>citations/n_overcitations</th>\n",
       "      <th>correctness/answer_overlap</th>\n",
       "      <th>correctness/answer_entail</th>\n",
       "      <th>correctness/citations_recall</th>\n",
       "      <th>correctness/citations_precision</th>\n",
       "      <th>quality/answer_relevance</th>\n",
       "      <th>n_questions</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llm</th>\n",
       "      <th>temperature</th>\n",
       "      <th>nli</th>\n",
       "      <th>ellm</th>\n",
       "      <th>sim</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Meta-Llama-3-70B-Instruct</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>74.2% ± 3.1%</td>\n",
       "      <td>95.8% ± 1.1%</td>\n",
       "      <td>1.5 ± 0.1</td>\n",
       "      <td>2.0 ± 0.1</td>\n",
       "      <td>1.9 ± 0.1</td>\n",
       "      <td>0.6 ± 0.0</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>88.7% ± 1.0%</td>\n",
       "      <td>89.2% ± 1.1%</td>\n",
       "      <td>81.0% ± 1.6%</td>\n",
       "      <td>91.1% ± 1.2%</td>\n",
       "      <td>74.3% ± 2.4%</td>\n",
       "      <td>17872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mixtral-8x7B-Instruct-v0.1</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>67.0% ± 6.4%</td>\n",
       "      <td>93.2% ± 2.8%</td>\n",
       "      <td>2.1 ± 0.2</td>\n",
       "      <td>2.5 ± 0.3</td>\n",
       "      <td>2.3 ± 0.3</td>\n",
       "      <td>0.6 ± 0.1</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>88.0% ± 1.7%</td>\n",
       "      <td>88.0% ± 2.0%</td>\n",
       "      <td>82.5% ± 3.8%</td>\n",
       "      <td>84.5% ± 3.8%</td>\n",
       "      <td>73.0% ± 3.5%</td>\n",
       "      <td>7659</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Mixtral-8x7B-Instruct-v0.1.Q8_0</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>62.4% ± 7.0%</td>\n",
       "      <td>90.2% ± 3.5%</td>\n",
       "      <td>2.7 ± 0.3</td>\n",
       "      <td>3.0 ± 0.4</td>\n",
       "      <td>2.8 ± 0.3</td>\n",
       "      <td>0.7 ± 0.1</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>87.5% ± 1.8%</td>\n",
       "      <td>87.8% ± 2.0%</td>\n",
       "      <td>84.8% ± 3.6%</td>\n",
       "      <td>79.7% ± 4.6%</td>\n",
       "      <td>72.5% ± 3.7%</td>\n",
       "      <td>6000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen1_5-110b-chat</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>56.5% ± 11.7%</td>\n",
       "      <td>96.4% ± 3.0%</td>\n",
       "      <td>1.9 ± 0.3</td>\n",
       "      <td>1.8 ± 0.2</td>\n",
       "      <td>1.7 ± 0.2</td>\n",
       "      <td>0.5 ± 0.1</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>88.5% ± 2.2%</td>\n",
       "      <td>87.4% ± 2.5%</td>\n",
       "      <td>73.7% ± 5.4%</td>\n",
       "      <td>91.0% ± 3.7%</td>\n",
       "      <td>72.5% ± 4.3%</td>\n",
       "      <td>1000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen1_5-32b-chat-q8_0</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>55.0% ± 3.2%</td>\n",
       "      <td>98.9% ± 0.6%</td>\n",
       "      <td>1.6 ± 0.1</td>\n",
       "      <td>1.4 ± 0.0</td>\n",
       "      <td>1.3 ± 0.1</td>\n",
       "      <td>0.2 ± 0.0</td>\n",
       "      <td>0.0 ± 0.0</td>\n",
       "      <td>87.0% ± 1.0%</td>\n",
       "      <td>84.4% ± 0.0%</td>\n",
       "      <td>63.3% ± 2.2%</td>\n",
       "      <td>95.4% ± 0.3%</td>\n",
       "      <td>72.3% ± 1.8%</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zephyr-orpo-141b-A35b-v0.1</th>\n",
       "      <th>0.1</th>\n",
       "      <th>t5_xxl_true_nli_mixture</th>\n",
       "      <th>Mistral-7B-Instruct-v0.2</th>\n",
       "      <th>all-MiniLM-L6-v2</th>\n",
       "      <td>1</td>\n",
       "      <td>44.3% ± 12.7%</td>\n",
       "      <td>79.7% ± 10.2%</td>\n",
       "      <td>2.0 ± 0.4</td>\n",
       "      <td>4.6 ± 1.3</td>\n",
       "      <td>3.3 ± 1.0</td>\n",
       "      <td>0.6 ± 0.2</td>\n",
       "      <td>0.1 ± 0.0</td>\n",
       "      <td>83.5% ± 3.6%</td>\n",
       "      <td>84.5% ± 3.5%</td>\n",
       "      <td>73.8% ± 9.7%</td>\n",
       "      <td>56.2% ± 10.6%</td>\n",
       "      <td>68.2% ± 6.4%</td>\n",
       "      <td>763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                              prompt_id  \\\n",
       "llm                             temperature nli                     ellm                     sim                          \n",
       "Meta-Llama-3-70B-Instruct       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "Mixtral-8x7B-Instruct-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "Mixtral-8x7B-Instruct-v0.1.Q8_0 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "qwen1_5-110b-chat               0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "qwen1_5-32b-chat-q8_0           0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "zephyr-orpo-141b-A35b-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1   \n",
       "\n",
       "                                                                                                              citations/ais_recall  \\\n",
       "llm                             temperature nli                     ellm                     sim                                     \n",
       "Meta-Llama-3-70B-Instruct       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         74.2% ± 3.1%   \n",
       "Mixtral-8x7B-Instruct-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         67.0% ± 6.4%   \n",
       "Mixtral-8x7B-Instruct-v0.1.Q8_0 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         62.4% ± 7.0%   \n",
       "qwen1_5-110b-chat               0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2        56.5% ± 11.7%   \n",
       "qwen1_5-32b-chat-q8_0           0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         55.0% ± 3.2%   \n",
       "zephyr-orpo-141b-A35b-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2        44.3% ± 12.7%   \n",
       "\n",
       "                                                                                                              citations/ais_precision  \\\n",
       "llm                             temperature nli                     ellm                     sim                                        \n",
       "Meta-Llama-3-70B-Instruct       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2            95.8% ± 1.1%   \n",
       "Mixtral-8x7B-Instruct-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2            93.2% ± 2.8%   \n",
       "Mixtral-8x7B-Instruct-v0.1.Q8_0 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2            90.2% ± 3.5%   \n",
       "qwen1_5-110b-chat               0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2            96.4% ± 3.0%   \n",
       "qwen1_5-32b-chat-q8_0           0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2            98.9% ± 0.6%   \n",
       "zephyr-orpo-141b-A35b-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2           79.7% ± 10.2%   \n",
       "\n",
       "                                                                                                              citations/n_sentences  \\\n",
       "llm                             temperature nli                     ellm                     sim                                      \n",
       "Meta-Llama-3-70B-Instruct       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             1.5 ± 0.1   \n",
       "Mixtral-8x7B-Instruct-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             2.1 ± 0.2   \n",
       "Mixtral-8x7B-Instruct-v0.1.Q8_0 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             2.7 ± 0.3   \n",
       "qwen1_5-110b-chat               0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             1.9 ± 0.3   \n",
       "qwen1_5-32b-chat-q8_0           0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             1.6 ± 0.1   \n",
       "zephyr-orpo-141b-A35b-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             2.0 ± 0.4   \n",
       "\n",
       "                                                                                                              citations/n_total_citations  \\\n",
       "llm                             temperature nli                     ellm                     sim                                            \n",
       "Meta-Llama-3-70B-Instruct       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   2.0 ± 0.1   \n",
       "Mixtral-8x7B-Instruct-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   2.5 ± 0.3   \n",
       "Mixtral-8x7B-Instruct-v0.1.Q8_0 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   3.0 ± 0.4   \n",
       "qwen1_5-110b-chat               0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   1.8 ± 0.2   \n",
       "qwen1_5-32b-chat-q8_0           0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   1.4 ± 0.0   \n",
       "zephyr-orpo-141b-A35b-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   4.6 ± 1.3   \n",
       "\n",
       "                                                                                                              citations/n_correct_citations  \\\n",
       "llm                             temperature nli                     ellm                     sim                                              \n",
       "Meta-Llama-3-70B-Instruct       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     1.9 ± 0.1   \n",
       "Mixtral-8x7B-Instruct-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     2.3 ± 0.3   \n",
       "Mixtral-8x7B-Instruct-v0.1.Q8_0 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     2.8 ± 0.3   \n",
       "qwen1_5-110b-chat               0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     1.7 ± 0.2   \n",
       "qwen1_5-32b-chat-q8_0           0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     1.3 ± 0.1   \n",
       "zephyr-orpo-141b-A35b-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                     3.3 ± 1.0   \n",
       "\n",
       "                                                                                                              citations/n_correctly_multicited_sentences  \\\n",
       "llm                             temperature nli                     ellm                     sim                                                           \n",
       "Meta-Llama-3-70B-Instruct       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  0.6 ± 0.0   \n",
       "Mixtral-8x7B-Instruct-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  0.6 ± 0.1   \n",
       "Mixtral-8x7B-Instruct-v0.1.Q8_0 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  0.7 ± 0.1   \n",
       "qwen1_5-110b-chat               0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  0.5 ± 0.1   \n",
       "qwen1_5-32b-chat-q8_0           0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  0.2 ± 0.0   \n",
       "zephyr-orpo-141b-A35b-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                                  0.6 ± 0.2   \n",
       "\n",
       "                                                                                                              citations/n_overcitations  \\\n",
       "llm                             temperature nli                     ellm                     sim                                          \n",
       "Meta-Llama-3-70B-Instruct       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "Mixtral-8x7B-Instruct-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "Mixtral-8x7B-Instruct-v0.1.Q8_0 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "qwen1_5-110b-chat               0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "qwen1_5-32b-chat-q8_0           0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.0 ± 0.0   \n",
       "zephyr-orpo-141b-A35b-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 0.1 ± 0.0   \n",
       "\n",
       "                                                                                                              correctness/answer_overlap  \\\n",
       "llm                             temperature nli                     ellm                     sim                                           \n",
       "Meta-Llama-3-70B-Instruct       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               88.7% ± 1.0%   \n",
       "Mixtral-8x7B-Instruct-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               88.0% ± 1.7%   \n",
       "Mixtral-8x7B-Instruct-v0.1.Q8_0 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               87.5% ± 1.8%   \n",
       "qwen1_5-110b-chat               0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               88.5% ± 2.2%   \n",
       "qwen1_5-32b-chat-q8_0           0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               87.0% ± 1.0%   \n",
       "zephyr-orpo-141b-A35b-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2               83.5% ± 3.6%   \n",
       "\n",
       "                                                                                                              correctness/answer_entail  \\\n",
       "llm                             temperature nli                     ellm                     sim                                          \n",
       "Meta-Llama-3-70B-Instruct       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              89.2% ± 1.1%   \n",
       "Mixtral-8x7B-Instruct-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              88.0% ± 2.0%   \n",
       "Mixtral-8x7B-Instruct-v0.1.Q8_0 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              87.8% ± 2.0%   \n",
       "qwen1_5-110b-chat               0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              87.4% ± 2.5%   \n",
       "qwen1_5-32b-chat-q8_0           0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              84.4% ± 0.0%   \n",
       "zephyr-orpo-141b-A35b-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2              84.5% ± 3.5%   \n",
       "\n",
       "                                                                                                              correctness/citations_recall  \\\n",
       "llm                             temperature nli                     ellm                     sim                                             \n",
       "Meta-Llama-3-70B-Instruct       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 81.0% ± 1.6%   \n",
       "Mixtral-8x7B-Instruct-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 82.5% ± 3.8%   \n",
       "Mixtral-8x7B-Instruct-v0.1.Q8_0 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 84.8% ± 3.6%   \n",
       "qwen1_5-110b-chat               0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 73.7% ± 5.4%   \n",
       "qwen1_5-32b-chat-q8_0           0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 63.3% ± 2.2%   \n",
       "zephyr-orpo-141b-A35b-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                 73.8% ± 9.7%   \n",
       "\n",
       "                                                                                                              correctness/citations_precision  \\\n",
       "llm                             temperature nli                     ellm                     sim                                                \n",
       "Meta-Llama-3-70B-Instruct       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                    91.1% ± 1.2%   \n",
       "Mixtral-8x7B-Instruct-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                    84.5% ± 3.8%   \n",
       "Mixtral-8x7B-Instruct-v0.1.Q8_0 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                    79.7% ± 4.6%   \n",
       "qwen1_5-110b-chat               0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                    91.0% ± 3.7%   \n",
       "qwen1_5-32b-chat-q8_0           0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                    95.4% ± 0.3%   \n",
       "zephyr-orpo-141b-A35b-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2                   56.2% ± 10.6%   \n",
       "\n",
       "                                                                                                              quality/answer_relevance  \\\n",
       "llm                             temperature nli                     ellm                     sim                                         \n",
       "Meta-Llama-3-70B-Instruct       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             74.3% ± 2.4%   \n",
       "Mixtral-8x7B-Instruct-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             73.0% ± 3.5%   \n",
       "Mixtral-8x7B-Instruct-v0.1.Q8_0 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             72.5% ± 3.7%   \n",
       "qwen1_5-110b-chat               0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             72.5% ± 4.3%   \n",
       "qwen1_5-32b-chat-q8_0           0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             72.3% ± 1.8%   \n",
       "zephyr-orpo-141b-A35b-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2             68.2% ± 6.4%   \n",
       "\n",
       "                                                                                                               n_questions  \n",
       "llm                             temperature nli                     ellm                     sim                            \n",
       "Meta-Llama-3-70B-Instruct       0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2        17872  \n",
       "Mixtral-8x7B-Instruct-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         7659  \n",
       "Mixtral-8x7B-Instruct-v0.1.Q8_0 0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         6000  \n",
       "qwen1_5-110b-chat               0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2         1000  \n",
       "qwen1_5-32b-chat-q8_0           0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2           90  \n",
       "zephyr-orpo-141b-A35b-v0.1      0.1         t5_xxl_true_nli_mixture Mistral-7B-Instruct-v0.2 all-MiniLM-L6-v2          763  "
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "display(Markdown(\"### Training results\"))\n",
    "train_display = remove_index(train_results, \"prompt_id\")\n",
    "train_display"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "p311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
